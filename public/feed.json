{
    "version": "https://jsonfeed.org/version/1",
    "title": "$ root@workingtitle.pro",
    "home_page_url": "https://workingtitle.pro",
    "feed_url": "https://workingtitle.profeed.json",
    "description": "A minimal hugo theme focus on content",
    "favicon": "https://workingtitle.pro/assets/favicon.ico",
    "expired": false,
    "author": {
      "name": "Calvin Tran",
      "url": "https://canhtran.me/"
    },
    "items": [
      
      
  
      
      {
        "id": "4bd129539263ee832de2182bdb92463ee3b3dc32",
        "title": "Configure Harbor proxy cache for pulling images from Docker Hub",
        "summary": "",
        "content_text": "Due to severe sanctions restrictions, I\u0026rsquo;ve been having lots of trouble pulling images from Docker Hub. A great method of getting around this issue is to setup proxy cache on Harbor image registry. It can pull images from Docker Hub and cache them so for the next use, you\u0026rsquo;ll end up pulling from your local repo instead of Docker Hub.\nIn order to do this, I needed to upgrade Harbor to the latest version. You can do that by following the official docs.\nAfter you\u0026rsquo;ve done that, you can follow these steps:\nOn the Harbor dashboard navigate to Administration \u0026gt; Registries \u0026gt; New Endpoint\nSelect Docker Hub as the provider, give it a name and then test your connection to Docker Hub. Click on Test connection to check your connection. In my case, I had to configure an HTTP proxy in harbor.yaml file since my server does not have direct access to internet. (Side note to all my Iranian readers: Use \u0026ldquo;shecan\u0026rdquo; as your DNS)\nAfter saving, you should have a working endpoint like this: Navigate to Projects \u0026gt; New Project and create a project named proxy_cache. Make sure to enable the proxy cache option and point it to the registry you just created. As so:\nAnd we\u0026rsquo;re done!\nNow when you want to pull an image, you should do so in this format:\ndocker pull yourRepoAddress.com/proxy_cache/\u0026lt;image_name\u0026gt;:\u0026lt;image_tag\u0026gt; In my example, it looks like this:\ndocker pull reg.zcore.local/proxy_cache/nginx:latest If the image is already present, it will download it from the cache. Otherwise, it will pull it from Docker Hub. It will also always check and if there is a new version available on Docker Hub, it will try to download that.\n",
        "content_html": "\u003cp\u003eDue to severe sanctions restrictions, I\u0026rsquo;ve been having lots of trouble pulling images from Docker Hub. A great method of getting around this issue is to setup proxy cache on Harbor image registry. It can pull images from Docker Hub and cache them so for the next use, you\u0026rsquo;ll end up pulling from your local repo instead of Docker Hub.\u003c/p\u003e\n\u003cp\u003eIn order to do this, I needed to upgrade Harbor to the latest version. You can do that by following the \u003ca href=\"https://goharbor.io/docs/2.8.0/administration/upgrade/\"\u003eofficial docs\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAfter you\u0026rsquo;ve done that, you can follow these steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eOn the Harbor dashboard navigate to \u003cstrong\u003eAdministration \u0026gt; Registries \u0026gt; New Endpoint\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSelect \u003cstrong\u003eDocker Hub\u003c/strong\u003e as the provider, give it a name and then test your connection to Docker Hub.\n\u003cimg\n  src=\"https://workingtitle.pro/images/new-endpoint-harbor.png\"\n  alt=\"new-endpoint-harbor\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClick on \u003cstrong\u003eTest connection\u003c/strong\u003e to check your connection. In my case, I had to configure an HTTP proxy in \u003ccode\u003eharbor.yaml\u003c/code\u003e file since my server does not have direct access to internet. \u003cem\u003e(Side note to all my Iranian readers: Use \u0026ldquo;shecan\u0026rdquo; as your DNS)\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAfter saving, you should have a working endpoint like this:\n\u003cimg\n  src=\"https://workingtitle.pro/images/endpoint-configured-harbor.png\"\n  alt=\"endpoint-configured-harbor\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNavigate to \u003cstrong\u003eProjects \u0026gt; New Project\u003c/strong\u003e and create a project named \u003ccode\u003eproxy_cache\u003c/code\u003e. Make sure to enable the proxy cache option and point it to the registry you just created. As so:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/proxy-cache-project-harbor.png\"\n  alt=\"proxy-cache-project-harbor\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eAnd we\u0026rsquo;re done!\u003c/p\u003e\n\u003cp\u003eNow when you want to pull an image, you should do so in this format:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker pull yourRepoAddress.com/proxy_cache/\u0026lt;image_name\u0026gt;:\u0026lt;image_tag\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn my example, it looks like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker pull reg.zcore.local/proxy_cache/nginx:latest\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf the image is already present, it will download it from the cache. Otherwise, it will pull it from Docker Hub. It will also always check and if there is a new version available on Docker Hub, it will try to download that.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/configure-proxy-cache-harbor/",
        "date_published": "19086-19-09T814:1919:00+03:30",
        "date_modified": "19086-19-09T814:1919:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "29dd7cb1cd6a8e30159f7ecfaa29a1531f1b6581",
        "title": "Graylog on Docker — Part 3: Graylog! ",
        "summary": "",
        "content_text": "Now that we have a working Elasticsearch cluster and a MongoDB replica set, we can move to the final piece of the puzzle — the Graylog cluster!\nWe will deploy 3 Graylog containers, one of which be will denoted as \u0026ldquo;master\u0026rdquo;, using the is_master = true parameter in the configuration file. The other two containers will be worker nodes and will have identical configurations.\nHere\u0026rsquo;s our master configuration file:\nis_master = true node_id_file = /usr/share/graylog/node-id password_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM root_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e root_timezone = Asia/Tehran http_bind_address = 0.0.0.0:9000 elasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203 rotation_strategy = count elasticsearch_max_docs_per_index = 20000000 elasticsearch_max_number_of_indices = 20 retention_strategy = delete elasticsearch_shards = 4 elasticsearch_replicas = 3 elasticsearch_index_prefix = graylog allow_leading_wildcard_searches = false allow_highlighting = false elasticsearch_analyzer = standard output_batch_size = 500 output_flush_interval = 1 output_fault_count_threshold = 5 output_fault_penalty_seconds = 30 processbuffer_processors = 5 outputbuffer_processors = 3 processor_wait_strategy = blocking ring_size = 65536 inputbuffer_ring_size = 65536 inputbuffer_processors = 2 inputbuffer_wait_strategy = blocking message_journal_enabled = true lb_recognition_period_seconds = 3 mongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs mongodb_max_connections = 1000 mongodb_threads_allowed_to_block_multiplier = 5 proxied_requests_thread_pool_size = 32 And this will be our slave configuration file, which is identical to master with the difference that is_master is set to false.\nis_master = false node_id_file = /usr/share/graylog/node-id password_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM root_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e root_timezone = Asia/Tehran http_bind_address = 0.0.0.0:9000 elasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203 rotation_strategy = count elasticsearch_max_docs_per_index = 20000000 elasticsearch_max_number_of_indices = 20 retention_strategy = delete elasticsearch_shards = 4 elasticsearch_replicas = 3 elasticsearch_index_prefix = graylog allow_leading_wildcard_searches = false allow_highlighting = false elasticsearch_analyzer = standard output_batch_size = 500 output_flush_interval = 1 output_fault_count_threshold = 5 output_fault_penalty_seconds = 30 processbuffer_processors = 5 outputbuffer_processors = 3 processor_wait_strategy = blocking ring_size = 65536 inputbuffer_ring_size = 65536 inputbuffer_processors = 2 inputbuffer_wait_strategy = blocking message_journal_enabled = true lb_recognition_period_seconds = 3 mongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs mongodb_max_connections = 1000 mongodb_threads_allowed_to_block_multiplier = 5 proxied_requests_thread_pool_size = 32 Then we have our final docker compose file:\nversion: \u0026#34;3.8\u0026#34; services: graylog-1: container_name: graylog-1-master image: graylog:5.0.6 volumes: - ./graylog-config/master/:/usr/share/graylog/data/config/ networks: - my-overlay-2 ports: - 9000:9000 # Graylog web interface and REST API - 1514:1514 # Syslog TCP - 1514:1514/udp # Syslog UDP - 12201:12201 # GELF TCP - 12201:12201/udp # GELF UDP - 5045:5044 #Logstash port extra_hosts: - \u0026#34;es01.example.net:172.17.93.170\u0026#34; - \u0026#34;es02.example.net:172.17.93.171\u0026#34; - \u0026#34;es03.example.net:172.17.93.172\u0026#34; deploy: restart_policy: condition: on-failure placement: constraints: - node.labels.type == master replicas: 1 entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ] graylog-2: container_name: graylog-2 image: graylog:5.0.6 volumes: - /opt/graylog-config/slave/:/usr/share/graylog/data/config/ networks: - my-overlay-2 ports: - 9001:9000 # Graylog web interface and REST API - 1515:1514 # Syslog TCP - 1515:1514/udp # Syslog UDP - 12202:12201 # GELF TCP - 12202:12201/udp # GELF UDP - 5044:5044 #Logstash port extra_hosts: - \u0026#34;es01.example.net:172.17.93.170\u0026#34; - \u0026#34;es02.example.net:172.17.93.171\u0026#34; - \u0026#34;es03.example.net:172.17.93.172\u0026#34; deploy: restart_policy: condition: on-failure placement: constraints: - node.labels.type == worker-1 replicas: 1 entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ] graylog-3: container_name: graylog-3 image: graylog:5.0.6 volumes: - /opt/graylog-config/slave/:/usr/share/graylog/data/config/ networks: - my-overlay-2 ports: - 9002:9000 # Graylog web interface and REST API - 1516:1514 # Syslog TCP - 1516:1514/udp # Syslog UDP - 12203:12201 # GELF TCP - 12203:12201/udp # GELF UDP - 5046:5044 #Logstash port extra_hosts: - \u0026#34;es01.example.net:172.17.93.170\u0026#34; - \u0026#34;es02.example.net:172.17.93.171\u0026#34; - \u0026#34;es03.example.net:172.17.93.172\u0026#34; deploy: restart_policy: condition: on-failure placement: constraints: - node.labels.type == worker-2 replicas: 1 entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ] networks: my-overlay-2: external: true Here\u0026rsquo;s the breakdown:\nwe\u0026rsquo;re setting up 3 instances of Graylog, one master and two slaves. The master node gets its own configuration file via volumes and the slaves get their own. We use placement constraints to place containers on specific nodes, just as we did for the Elasticsearch cluster and MongoDB cluster. This ensures we have the highest level of availability if one of the nodes in our Docker Swarm goes down. We use extra_hosts to specify the IP address of the nodes where Graylog can access elasticsearch. The addresses specified such as es01.example.net is configured as the elasticsearch node in the Graylog configuration file mentioned before. Please note that all of our containers in the cluster (Elastic, Mongo, Graylog) use a single shared overlay network (my-overlay-2) so they can access each other. ",
        "content_html": "\u003cp\u003eNow that we have a working \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-1/\"\u003eElasticsearch cluster\u003c/a\u003e and a \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-2/\"\u003eMongoDB replica set\u003c/a\u003e, we can move to the final piece of the puzzle — the Graylog cluster!\u003c/p\u003e\n\u003cp\u003eWe will deploy 3 Graylog containers, one of which be will denoted as \u0026ldquo;master\u0026rdquo;, using the \u003ccode\u003eis_master = true\u003c/code\u003e parameter in the configuration file. The other two containers will be worker nodes and will have identical configurations.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s our master configuration file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eis_master = true\nnode_id_file = /usr/share/graylog/node-id\npassword_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM\nroot_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e\nroot_timezone = Asia/Tehran\nhttp_bind_address = 0.0.0.0:9000\nelasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203\nrotation_strategy = count\nelasticsearch_max_docs_per_index = 20000000\nelasticsearch_max_number_of_indices = 20\nretention_strategy = delete\nelasticsearch_shards = 4\nelasticsearch_replicas = 3\nelasticsearch_index_prefix = graylog\nallow_leading_wildcard_searches = false\nallow_highlighting = false\nelasticsearch_analyzer = standard\noutput_batch_size = 500\noutput_flush_interval = 1\noutput_fault_count_threshold = 5\noutput_fault_penalty_seconds = 30\nprocessbuffer_processors = 5\noutputbuffer_processors = 3\nprocessor_wait_strategy = blocking\nring_size = 65536\ninputbuffer_ring_size = 65536\ninputbuffer_processors = 2\ninputbuffer_wait_strategy = blocking\nmessage_journal_enabled = true\nlb_recognition_period_seconds = 3\nmongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs\nmongodb_max_connections = 1000\nmongodb_threads_allowed_to_block_multiplier = 5\nproxied_requests_thread_pool_size = 32\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd this will be our slave configuration file, which is identical to master with the difference that \u003ccode\u003eis_master\u003c/code\u003e is set to \u003ccode\u003efalse\u003c/code\u003e.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eis_master = false\nnode_id_file = /usr/share/graylog/node-id\npassword_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM\nroot_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e\nroot_timezone = Asia/Tehran\nhttp_bind_address = 0.0.0.0:9000\nelasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203\nrotation_strategy = count\nelasticsearch_max_docs_per_index = 20000000\nelasticsearch_max_number_of_indices = 20\nretention_strategy = delete\nelasticsearch_shards = 4\nelasticsearch_replicas = 3\nelasticsearch_index_prefix = graylog\nallow_leading_wildcard_searches = false\nallow_highlighting = false\nelasticsearch_analyzer = standard\noutput_batch_size = 500\noutput_flush_interval = 1\noutput_fault_count_threshold = 5\noutput_fault_penalty_seconds = 30\nprocessbuffer_processors = 5\noutputbuffer_processors = 3\nprocessor_wait_strategy = blocking\nring_size = 65536\ninputbuffer_ring_size = 65536\ninputbuffer_processors = 2\ninputbuffer_wait_strategy = blocking\nmessage_journal_enabled = true\nlb_recognition_period_seconds = 3\nmongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs\nmongodb_max_connections = 1000\nmongodb_threads_allowed_to_block_multiplier = 5\nproxied_requests_thread_pool_size = 32\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen we have our final docker compose file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eversion: \u0026#34;3.8\u0026#34;\n\nservices:\n  graylog-1:\n    container_name: graylog-1-master\n    image: graylog:5.0.6    \n    volumes: \n      - ./graylog-config/master/:/usr/share/graylog/data/config/\n    networks:\n      - my-overlay-2\n    ports:\n      - 9000:9000 # Graylog web interface and REST API\n      - 1514:1514 # Syslog TCP\n      - 1514:1514/udp # Syslog UDP\n      - 12201:12201 # GELF TCP\n      - 12201:12201/udp # GELF UDP\n      - 5045:5044 #Logstash port\n    extra_hosts:\n      - \u0026#34;es01.example.net:172.17.93.170\u0026#34;\n      - \u0026#34;es02.example.net:172.17.93.171\u0026#34;\n      - \u0026#34;es03.example.net:172.17.93.172\u0026#34;\n    deploy:\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.labels.type == master\n      replicas: 1\n    entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ]\n\n  graylog-2:\n    container_name: graylog-2\n    image: graylog:5.0.6\n    volumes:\n      - /opt/graylog-config/slave/:/usr/share/graylog/data/config/\n    networks:\n      - my-overlay-2\n    ports:\n      - 9001:9000 # Graylog web interface and REST API\n      - 1515:1514 # Syslog TCP\n      - 1515:1514/udp # Syslog UDP\n      - 12202:12201 # GELF TCP\n      - 12202:12201/udp # GELF UDP\n      - 5044:5044 #Logstash port\n    extra_hosts: \n      - \u0026#34;es01.example.net:172.17.93.170\u0026#34; \n      - \u0026#34;es02.example.net:172.17.93.171\u0026#34;\n      - \u0026#34;es03.example.net:172.17.93.172\u0026#34;\n    deploy:\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.labels.type == worker-1\n      replicas: 1\n    entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ]\n\n  graylog-3:\n    container_name: graylog-3\n    image: graylog:5.0.6\n    volumes:\n      - /opt/graylog-config/slave/:/usr/share/graylog/data/config/\n    networks:\n      - my-overlay-2\n    ports:\n      - 9002:9000 # Graylog web interface and REST API\n      - 1516:1514 # Syslog TCP\n      - 1516:1514/udp # Syslog UDP\n      - 12203:12201 # GELF TCP\n      - 12203:12201/udp # GELF UDP\n      - 5046:5044 #Logstash port\n    extra_hosts: \n      - \u0026#34;es01.example.net:172.17.93.170\u0026#34; \n      - \u0026#34;es02.example.net:172.17.93.171\u0026#34;\n      - \u0026#34;es03.example.net:172.17.93.172\u0026#34;\n    deploy:\n      restart_policy:\n        condition: on-failure\n      placement:\n        constraints:\n          - node.labels.type == worker-2    \n      replicas: 1\n    entrypoint: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ]\nnetworks:\n  my-overlay-2:\n    external: true\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHere\u0026rsquo;s the breakdown:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewe\u0026rsquo;re setting up 3 instances of Graylog, one master and two slaves.\u003c/li\u003e\n\u003cli\u003eThe master node gets its own configuration file via volumes and the slaves get their own.\u003c/li\u003e\n\u003cli\u003eWe use placement constraints to place containers on specific nodes, just as we did for the Elasticsearch cluster and MongoDB cluster. This ensures we have the highest level of availability if one of the nodes in our Docker Swarm goes down.\u003c/li\u003e\n\u003cli\u003eWe use \u003ccode\u003eextra_hosts\u003c/code\u003e to specify the IP address of the nodes where Graylog can access elasticsearch. The addresses specified such as \u003ccode\u003ees01.example.net\u003c/code\u003e is configured as the elasticsearch node in the Graylog configuration file mentioned before.\u003c/li\u003e\n\u003cli\u003ePlease note that all of our containers in the cluster (Elastic, Mongo, Graylog) use a single shared overlay network (\u003ccode\u003emy-overlay-2\u003c/code\u003e) so they can access each other.\u003c/li\u003e\n\u003c/ul\u003e\n",
        "url": "https://workingtitle.pro/posts/graylog-on-docker-part-3/",
        "date_published": "30076-30-09T731:3030:00+03:30",
        "date_modified": "30076-30-09T731:3030:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "018295609775066032752556983d32cee1879ba8",
        "title": "Graylog on Docker — Part 2: MongoDB  ",
        "summary": "",
        "content_text": "Now that we\u0026rsquo;ve setup our Elasticsearch cluster in part one, we can move to the second stage which is setting up a MongoDB replica set.\nConfigure MongoDB replica set On our master machine (docker swarm), we will be add a new docker compose which will be used for the MongoDB deployment.\nmongo.yaml version: \u0026#34;3.8\u0026#34; services: mongo-1: image: mongo volumes: - /data/mongo/mongo1:/data/db command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39; networks: - my-overlay-2 ports: - 27017:27017 deploy: placement: constraints: - node.labels.type == master replicas: 1 restart_policy: condition: on-failure mongo-2: image: mongo volumes: - /data/mongo/mongo2:/data/db command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39; networks: - my-overlay-2 ports: - 27018:27017 deploy: placement: constraints: - node.labels.type == worker-1 replicas: 1 restart_policy: condition: on-failure mongo-3: image: mongo volumes: - /data/mongo/mongo3:/data/db command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39; networks: - my-overlay-2 ports: - 27019:27017 deploy: placement: constraints: - node.labels.type == worker-2 replicas: 1 restart_policy: condition: on-failure volumes: mongodb: driver: \u0026#34;local\u0026#34; networks: my-overlay-2: external: true Now let\u0026rsquo;s walk through it:\nWe create 3 containers mongo-1,mongo-2 and mongo-3 Each container is always placed on a different node using constraints. I\u0026rsquo;ve configured my master node with the master label, and each of the worker nodes with a worker label. MongoDB files are stored under the /data/mongo directory on each node. I\u0026rsquo;ve used NFS to mount this directory on a different server for larger storage but you can manually create these directories or each node, or change them entirely based on your needs. The containers use the same network my-overlay-2 as the rest of the containers described on Part 1. This is crucial since we need all the components of the Graylog cluster (Elasticsearch, MongoDB, Graylog) to talk to each other. The command mongod --oplogSize 128 --replSet dbrs tells our mongo instances that we want to initiate a replica set called dbrs. You can change this to your desired name. I\u0026rsquo;ve used different host ports for each container (27017, 27018, 27019) since initially I didn\u0026rsquo;t plan to place each container on a different node. If you\u0026rsquo;re planning to omit the constrains so the containers can be placed on any node, you should keep this port configuration. Otherwise, you can switch to 27017:27017 for all the containers. Start the Stack That\u0026rsquo;s pretty much it. Once you\u0026rsquo;ve made the changes to the compose file, save it somewhere on the master node and run it:\ndocker stack deploy -c mongo.yaml mongo Initialize the replica set Once your containers are up and running, you should check the logs to make sure everything is okay. You can do so by this command:\ndocker logs -f \u0026lt;container_name\u0026gt; If everything is done correctly, you should see something like waiting to Initialize ... in the logs. If that\u0026rsquo;s the case, you need to run this command from a machine that has access to your cluster:\nmongosh --host \u0026#39;172.17.93.171:27017\u0026#39; --eval \u0026#39;rs.initiate({ _id: \u0026#34;dbrs\u0026#34;, members: [{ _id: 0, host : \u0026#34;172.17.93.171:27017\u0026#34; }, { _id: 1, host : \u0026#34;172.17.93.172:27018\u0026#34; }, { _id: 2, host : \u0026#34;172.17.93.170:27019\u0026#34; }]})\u0026#39; You should have mongosh installed on the node that\u0026rsquo;s running this. Replace the IPs with your docker swarm node IPs. If you\u0026rsquo;ve changed the replica set name in the docker-compose file, make sure to change it from dbrs to your name. Once this command is executed, you should see a message regarding the successful replica set initialization.\nAnd that\u0026rsquo;s it! You now have a working MongoDB replica set on docker swarm! 🥳🥳🥳\nIn the next post, we\u0026rsquo;re going to finish up the cluster by deploying Graylog.\n",
        "content_html": "\u003cp\u003eNow that we\u0026rsquo;ve setup our Elasticsearch cluster in \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-1/\"\u003epart one\u003c/a\u003e, we can move to the second stage which is setting up a MongoDB replica set.\u003c/p\u003e\n\u003ch2 id=\"configure-mongodb-replica-set\"\u003eConfigure MongoDB replica set\u003c/h2\u003e\n\u003cp\u003eOn our master machine (docker swarm), we will be add a new docker compose which will be used for the MongoDB deployment.\u003c/p\u003e\n\u003ch3 id=\"mongoyaml\"\u003emongo.yaml\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eversion: \u0026#34;3.8\u0026#34;\n\nservices:\n  mongo-1:\n    image: mongo\n    volumes:\n      - /data/mongo/mongo1:/data/db\n    command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39;\n    networks:\n      - my-overlay-2\n    ports:\n      - 27017:27017\n    deploy:\n      placement:\n        constraints:\n          - node.labels.type == master\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n  mongo-2:\n    image: mongo\n    volumes:\n      - /data/mongo/mongo2:/data/db\n    command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39;\n    networks:\n      - my-overlay-2\n    ports:\n      - 27018:27017\n    deploy:\n      placement:\n        constraints:\n          - node.labels.type == worker-1\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n  mongo-3:\n    image: mongo\n    volumes:\n      - /data/mongo/mongo3:/data/db\n    command: \u0026#39;mongod --oplogSize 128 --replSet dbrs\u0026#39;\n    networks:\n      - my-overlay-2\n    ports:\n      - 27019:27017\n    deploy:\n      placement:\n        constraints:\n          - node.labels.type == worker-2\n      replicas: 1\n      restart_policy:\n        condition: on-failure\nvolumes:\n  mongodb:\n    driver: \u0026#34;local\u0026#34;\nnetworks:\n  my-overlay-2:\n    external: true\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow let\u0026rsquo;s walk through it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWe create 3 containers \u003ccode\u003emongo-1\u003c/code\u003e,\u003ccode\u003emongo-2\u003c/code\u003e and \u003ccode\u003emongo-3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEach container is always placed on a different node using \u003ccode\u003econstraints\u003c/code\u003e. I\u0026rsquo;ve configured my master node with the \u003ccode\u003emaster\u003c/code\u003e label, and each of the worker nodes with a \u003ccode\u003eworker\u003c/code\u003e label.\u003c/li\u003e\n\u003cli\u003eMongoDB files are stored under the \u003ccode\u003e/data/mongo\u003c/code\u003e directory on each node. I\u0026rsquo;ve used NFS to mount this directory on a different server for larger storage but you can manually create these directories or each node, or change them entirely based on your needs.\u003c/li\u003e\n\u003cli\u003eThe containers use the same network \u003ccode\u003emy-overlay-2\u003c/code\u003e as the rest of the containers described on \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-1/\"\u003ePart 1\u003c/a\u003e. This is crucial since we need all the components of the Graylog cluster (Elasticsearch, MongoDB, Graylog) to talk to each other.\u003c/li\u003e\n\u003cli\u003eThe command \u003ccode\u003emongod --oplogSize 128 --replSet dbrs\u003c/code\u003e tells our mongo instances that we want to initiate a replica set called \u003ccode\u003edbrs\u003c/code\u003e. You can change this to your desired name.\u003c/li\u003e\n\u003cli\u003eI\u0026rsquo;ve used different host ports for each container (\u003ccode\u003e27017\u003c/code\u003e, \u003ccode\u003e27018\u003c/code\u003e, \u003ccode\u003e27019\u003c/code\u003e) since initially I didn\u0026rsquo;t plan to place each container on a different node. If you\u0026rsquo;re planning to omit the constrains so the containers can be placed on any node, you should keep this port configuration. Otherwise, you can switch to \u003ccode\u003e27017:27017\u003c/code\u003e for all the containers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"start-the-stack\"\u003eStart the Stack\u003c/h2\u003e\n\u003cp\u003eThat\u0026rsquo;s pretty much it. Once you\u0026rsquo;ve made the changes to the compose file, save it somewhere on the master node and run it:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker stack deploy -c mongo.yaml mongo\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"initialize-the-replica-set\"\u003eInitialize the replica set\u003c/h2\u003e\n\u003cp\u003eOnce your containers are up and running, you should check the logs to make sure everything is okay. You can do so by this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker logs -f \u0026lt;container_name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf everything is done correctly, you should see something like \u003ccode\u003ewaiting to Initialize ...\u003c/code\u003e in the logs.  If that\u0026rsquo;s the case, you need to run this command from a machine that has access to your cluster:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emongosh --host \u0026#39;172.17.93.171:27017\u0026#39; --eval \u0026#39;rs.initiate({ _id: \u0026#34;dbrs\u0026#34;, members: [{ _id: 0, host : \u0026#34;172.17.93.171:27017\u0026#34; }, { _id: 1, host : \u0026#34;172.17.93.172:27018\u0026#34; }, { _id: 2, host : \u0026#34;172.17.93.170:27019\u0026#34; }]})\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eYou should have \u003ccode\u003emongosh\u003c/code\u003e installed on the node that\u0026rsquo;s running this.\u003c/li\u003e\n\u003cli\u003eReplace the IPs with your docker swarm node IPs.\u003c/li\u003e\n\u003cli\u003eIf you\u0026rsquo;ve changed the replica set name in the docker-compose file, make sure to change it from \u003ccode\u003edbrs\u003c/code\u003e to your name.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOnce this command is executed, you should see a message regarding the successful replica set initialization.\u003c/p\u003e\n\u003cp\u003eAnd that\u0026rsquo;s it! You now have a working MongoDB replica set on docker swarm! 🥳🥳🥳\u003c/p\u003e\n\u003cp\u003eIn the next post, we\u0026rsquo;re going to finish up the cluster by deploying Graylog.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/graylog-on-docker-part-2/",
        "date_published": "9076-09-09T738:99:00+03:30",
        "date_modified": "9076-09-09T738:99:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "3cf82ce922984c0e16f9114eb49d9fbe1c6ceb4e",
        "title": "Graylog Deflector Problem",
        "summary": "",
        "content_text": "So I\u0026rsquo;ve been setting up and testing a Graylog multi-node setup, and I\u0026rsquo;ve come across an annoying problem. Sometimes for some unknown reason as Graylog starts, it creates an index called graylog_deflector which is supposed to point to the correct index; an alias of some sorts.\nBut as it happens, Graylog sometimes messes this up and ends up creating an actual index called graylog_deflector. So when I would log into my Graylog UI, I\u0026rsquo;d see this error:\nElasticsearch exception [type=index_not_found_exception, reason=no such index []] Graylog can\u0026rsquo;t create the index needed so it starts to complain.\nThe solution is just to delete the graylog_deflector.\nFirst, stop your Graylog server instance. Delete the index (replace the IP and port with your own) curl -X DELETE \u0026#34;172.17.93.170:9201/graylog_deflector?pretty\u0026#34; Start your Graylog server again. Now when you log in, Graylog shouldn\u0026rsquo;t complain about the index again. If your issue isn\u0026rsquo;t solved, comment here and let me know.\n",
        "content_html": "\u003cp\u003eSo I\u0026rsquo;ve been setting up and testing a \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-1/\"\u003eGraylog multi-node\u003c/a\u003e setup, and I\u0026rsquo;ve come across an annoying problem. Sometimes for some unknown reason as Graylog starts, it creates an index called \u003ccode\u003egraylog_deflector\u003c/code\u003e which is \u003cem\u003esupposed\u003c/em\u003e to point to the correct index; an alias of some sorts.\u003c/p\u003e\n\u003cp\u003eBut as it happens, \u003ca href=\"https://community.graylog.org/t/graylog-deflector-exists-as-an-indexer-and-is-not-an-alias/7413\"\u003eGraylog sometimes messes this up\u003c/a\u003e and ends up creating an actual index called \u003ccode\u003egraylog_deflector\u003c/code\u003e. So when I would log into my Graylog UI, I\u0026rsquo;d see this error:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eElasticsearch exception [type=index_not_found_exception, reason=no such index []]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eGraylog can\u0026rsquo;t create the index needed so it starts to complain.\u003c/p\u003e\n\u003cp\u003eThe solution is just to delete the \u003ccode\u003egraylog_deflector\u003c/code\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst, stop your Graylog server instance.\u003c/li\u003e\n\u003cli\u003eDelete the index (replace the IP and port with your own)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecurl -X DELETE \u0026#34;172.17.93.170:9201/graylog_deflector?pretty\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eStart your Graylog server again.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNow when you log in, Graylog shouldn\u0026rsquo;t complain about the index again. If your issue isn\u0026rsquo;t solved, comment here and let me know.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/graylog-deflector-problem/",
        "date_published": "9076-09-09T731:99:00+03:30",
        "date_modified": "9076-09-09T731:99:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "39ed44f227c54b4525c3dfedaff9ae94737b9fa7",
        "title": "Graylog on Docker — Part 1: Elasticsearch ",
        "summary": "",
        "content_text": "My goal is to setup a highly-available Graylog instance using multiple containers on 3 Virtual Machines.\nOur setup needs the following:\n3 Linux VMs running the latest version of docker Docker swarm configured \u0026amp; initiated on all servers (1 master, 2 workers) What we\u0026rsquo;re going to do is deploy 3 containers for each component of the Graylog ecosystem. Each one of the these containers will be placed on one of our 3 VMs, giving us a high level of availability and redundancy.\n3 Elasticsearch containers running as a elastic cluster 3 MongoDB containers working as a ReplicaSet 3 Graylog containers working in a master-worker system 3 HAProxy containers responsible for load balancing the requests to the graylog containers. I\u0026rsquo;ve tried to follow the rough design described by Graylog in their documentation, but I\u0026rsquo;ve taken some liberties with how I\u0026rsquo;ve approached it.\nSince Graylog has to be setup in a specific order (Elasticsearch 🠒 MongoDB 🠒 Graylog), we\u0026rsquo;re going to start part 1 with the Elasticsearch setup.\nElasticsearch cluster on Docker Keep in mind that before using this docker compose file, you should have already configured your Docker swarm nodes \u0026amp; setup its networking correctly so all the nodes can connect to each other.\nContainers This docker compose file starts three instances of Elasticsearch called es01, es02 and es03 and places them on different nodes in the swarm. Once the containers are created, it initiates the elasticsearch cluster called es-docker-cluster. The value for discovery.seed_hosts should always point to other containers in the cluster.\nVolumes I\u0026rsquo;ve setup NFS storage on each of the VMs and mounted it to /data/elastic/. In this directory there are 3 subdirectories for each of the elasticsearch instances, so we have:\n/data/elastic/es01 /data/elastic/es02 /data/elastic/es03 You should configure NFS or GlusterFS and specify your volumes for each container.\nNetworking The network I\u0026rsquo;ve used is a simple overlay network which should be setup and configured before the cluster initialization. The overlay network is part of Docker Swarm and it allows different nodes in the Swarm to talk to each other.\nConstraints Using node.role constraint, we can specify exactly where the container should be placed. This is necessary for ensuring maximum availability and redundancy in case of one or two of the VMs crashing or going offline.\nStarting the Elasticsearch cluster Once everything is setup, copy the docker compose file below to a directory on your Docker Swarm master node and then start the cluster using this command:\ndocker stack deploy -c elasticsearch.yaml elastic elasticsearch.yaml version: \u0026#34;3.8\u0026#34; services: es01: image: elasticsearch:7.5.2 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - /data/elastic/es01:/usr/share/elasticsearch/data ports: - 9201:9200 networks: - my-overlay-2 deploy: placement: constraints: [node.role == manager] es02: image: elasticsearch:7.5.2 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - /data/elastic/es02:/usr/share/elasticsearch/data ports: - 9202:9200 networks: - my-overlay-2 deploy: placement: constraints: [node.role == worker] es03: image: elasticsearch:7.5.2 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; ulimits: memlock: soft: -1 hard: -1 volumes: - /data/elastic/es03:/usr/share/elasticsearch/data ports: - 9203:9200 networks: - my-overlay-2 deploy: placement: constraints: [node.role == worker] networks: my-overlay-2: external: true In Part 2, we\u0026rsquo;re going to be deploying a MongoDB replica set on Docker Swarm.\n",
        "content_html": "\u003cp\u003eMy goal is to setup a highly-available Graylog instance using multiple containers on 3 Virtual Machines.\u003c/p\u003e\n\u003cp\u003eOur setup needs the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3 Linux VMs running the latest version of docker\u003c/li\u003e\n\u003cli\u003eDocker swarm configured \u0026amp; initiated on all servers (1 master, 2 workers)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhat we\u0026rsquo;re going to do is deploy 3 containers for each component of the Graylog ecosystem. Each one of the these containers will be placed on one of our 3 VMs, giving us a high level of availability and redundancy.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3 Elasticsearch containers running as a elastic cluster\u003c/li\u003e\n\u003cli\u003e3 MongoDB containers working as a ReplicaSet\u003c/li\u003e\n\u003cli\u003e3 Graylog containers working in a master-worker system\u003c/li\u003e\n\u003cli\u003e3 HAProxy containers responsible for load balancing the requests to the graylog containers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI\u0026rsquo;ve tried to follow the rough design described by Graylog \u003ca href=\"https://go2docs.graylog.org/5-0/setting_up_graylog/multi-node_setup.html\"\u003ein their documentation\u003c/a\u003e, but I\u0026rsquo;ve taken some liberties with how I\u0026rsquo;ve approached it.\u003c/p\u003e\n\u003cp\u003eSince Graylog has to be setup in a specific order (Elasticsearch 🠒 MongoDB 🠒 Graylog), we\u0026rsquo;re going to start part 1 with the Elasticsearch setup.\u003c/p\u003e\n\u003ch2 id=\"elasticsearch-cluster-on-docker\"\u003eElasticsearch cluster on Docker\u003c/h2\u003e\n\u003cp\u003eKeep in mind that before using this docker compose file, you should have already configured your Docker swarm nodes \u0026amp; setup its networking correctly so all the nodes can connect to each other.\u003c/p\u003e\n\u003ch3 id=\"containers\"\u003eContainers\u003c/h3\u003e\n\u003cp\u003eThis docker compose file starts three instances of Elasticsearch called \u003ccode\u003ees01\u003c/code\u003e, \u003ccode\u003ees02\u003c/code\u003e and \u003ccode\u003ees03\u003c/code\u003e and places them on different nodes in the swarm. Once the containers are created, it initiates the elasticsearch cluster called \u003ccode\u003ees-docker-cluster\u003c/code\u003e. The value for \u003ccode\u003ediscovery.seed_hosts\u003c/code\u003e should always point to other containers in the cluster.\u003c/p\u003e\n\u003ch3 id=\"volumes\"\u003eVolumes\u003c/h3\u003e\n\u003cp\u003eI\u0026rsquo;ve setup NFS storage on each of the VMs and mounted it to \u003ccode\u003e/data/elastic/\u003c/code\u003e. In this directory there are 3 subdirectories for each of the elasticsearch instances, so we have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e/data/elastic/es01\u003c/li\u003e\n\u003cli\u003e/data/elastic/es02\u003c/li\u003e\n\u003cli\u003e/data/elastic/es03\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou should configure NFS or GlusterFS and specify your volumes for each container.\u003c/p\u003e\n\u003ch3 id=\"networking\"\u003eNetworking\u003c/h3\u003e\n\u003cp\u003eThe network I\u0026rsquo;ve used is a simple \u003ccode\u003eoverlay\u003c/code\u003e network which should be setup and configured before the cluster initialization. The \u003ccode\u003eoverlay\u003c/code\u003e network is part of Docker Swarm and it allows different nodes in the Swarm to talk to each other.\u003c/p\u003e\n\u003ch3 id=\"constraints\"\u003eConstraints\u003c/h3\u003e\n\u003cp\u003eUsing \u003ccode\u003enode.role\u003c/code\u003e constraint, we can specify exactly where the container should be placed. This is necessary for ensuring maximum availability and redundancy in case of one or two of the VMs crashing or going offline.\u003c/p\u003e\n\u003ch2 id=\"starting-the-elasticsearch-cluster\"\u003eStarting the Elasticsearch cluster\u003c/h2\u003e\n\u003cp\u003eOnce everything is setup, copy the docker compose file below to a directory on your Docker Swarm master node and then start the cluster using this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker stack deploy -c elasticsearch.yaml elastic\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"elasticsearchyaml\"\u003eelasticsearch.yaml\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eversion: \u0026#34;3.8\u0026#34;\nservices:\n  es01:\n    image: elasticsearch:7.5.2\n    container_name: es01\n    environment:\n      - node.name=es01\n      - cluster.name=es-docker-cluster\n      - discovery.seed_hosts=es02,es03\n      - cluster.initial_master_nodes=es01,es02,es03\n      - bootstrap.memory_lock=true\n      - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34;\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - /data/elastic/es01:/usr/share/elasticsearch/data\n    ports:\n      - 9201:9200\n    networks:\n      - my-overlay-2\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  es02:\n    image: elasticsearch:7.5.2\n    container_name: es02\n    environment:\n      - node.name=es02\n      - cluster.name=es-docker-cluster\n      - discovery.seed_hosts=es01,es03\n      - cluster.initial_master_nodes=es01,es02,es03\n      - bootstrap.memory_lock=true\n      - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34;\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - /data/elastic/es02:/usr/share/elasticsearch/data\n    ports:\n      - 9202:9200\n    networks:\n      - my-overlay-2\n    deploy:\n      placement:\n        constraints: [node.role == worker]\n\n  es03:\n    image: elasticsearch:7.5.2\n    container_name: es03\n    environment:\n      - node.name=es03\n      - cluster.name=es-docker-cluster\n      - discovery.seed_hosts=es01,es02\n      - cluster.initial_master_nodes=es01,es02,es03\n      - bootstrap.memory_lock=true\n      - \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34;\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - /data/elastic/es03:/usr/share/elasticsearch/data\n    ports:\n      - 9203:9200\n    networks:\n      - my-overlay-2\n    deploy:\n      placement:\n        constraints: [node.role == worker]\n\n\nnetworks:\n  my-overlay-2:\n    external: true\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn \u003ca href=\"https://workingtitle.pro/posts/graylog-on-docker-part-2/\"\u003ePart 2\u003c/a\u003e, we\u0026rsquo;re going to be deploying a MongoDB replica set on Docker Swarm.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/graylog-on-docker-part-1/",
        "date_published": "2076-02-09T78:22:00+03:30",
        "date_modified": "2076-02-09T78:22:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "5f7f1a6c26bdf07a2dcc4166602e48d544da7634",
        "title": "Permission Denied Problem with Tcpdump rotation",
        "summary": "",
        "content_text": "I needed to start a tcpdump process which rotates the PCAP once every hour. Using the -G option we can specify the number of seconds the process should run before rotation (in my case its 3600), and using -W we can tell it how many PCAP files should be retained. So if I need the PCAPs of the last 2 days, I would use -W 48 since I\u0026rsquo;m rotating them once every hour.\nHere\u0026rsquo;s my command:\ntcpdump -G 3600 -W 48 -i ens1f1 -w /data/dp-pcap/srv12-ens1f1-%Y-%m-%d_%H.%M.%S.pcap It\u0026rsquo;s all pretty straightforward, and it should work. As you start the process, it works as expected but when its time to rotate the PCAP, it will throw a permission denied error and terminates the processes. So what happens?\nWhen you first start capture, tcpdump starts the process with owner that you\u0026rsquo;re logged in with (in my case root), but once its time for rotation, it tries to write the output file to the directory using tcpdump as the owner. Since it doesn\u0026rsquo;t have access to your write directory, it fails.\nYeah\u0026hellip;it\u0026rsquo;s not fun to find out about this in a production environment where PCAPs are critical.\nSo how to fix it?\nOption 1: Set world-write permissions to the directory you\u0026rsquo;re saving the files to. (in my case /data/dp-pcap)\nOption 2: Change the owner of the directory to tcpdump\nI got mad so I did option 3 which was chmod 777 dp-pcap/. Would not recommend.\n",
        "content_html": "\u003cp\u003eI needed to start a \u003ccode\u003etcpdump\u003c/code\u003e process which rotates the PCAP once every hour. Using the \u003ccode\u003e-G\u003c/code\u003e option we can specify the number of seconds the process should run before rotation (in my case its 3600), and using \u003ccode\u003e-W\u003c/code\u003e we can tell it how many PCAP files should be retained. So if I need the PCAPs of the last 2 days, I would use \u003ccode\u003e-W 48\u003c/code\u003e since I\u0026rsquo;m rotating them once every hour.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s my command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003etcpdump -G 3600 -W 48 -i ens1f1 -w /data/dp-pcap/srv12-ens1f1-%Y-%m-%d_%H.%M.%S.pcap\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIt\u0026rsquo;s all pretty straightforward, and it \u003cem\u003eshould\u003c/em\u003e work. As you start the process, it works as expected but when its time to rotate the PCAP, it will throw a \u003ccode\u003epermission denied\u003c/code\u003e error and terminates the processes. So what happens?\u003c/p\u003e\n\u003cp\u003eWhen you first start capture, tcpdump starts the process with owner that you\u0026rsquo;re logged in with (in my case root), but once its time for rotation, it tries to write the output file to the directory using \u003ccode\u003etcpdump\u003c/code\u003e as the owner. Since it doesn\u0026rsquo;t have access to your write directory, it fails.\u003c/p\u003e\n\u003cp\u003eYeah\u0026hellip;it\u0026rsquo;s not fun to find out about this in a production environment where PCAPs are critical.\u003c/p\u003e\n\u003cp\u003eSo how to fix it?\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOption 1\u003c/strong\u003e: Set world-write permissions to the directory you\u0026rsquo;re saving the files to. (in my case /data/dp-pcap)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOption 2\u003c/strong\u003e: Change the owner of the directory to \u003ccode\u003etcpdump\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eI got mad so I did \u003cstrong\u003eoption 3\u003c/strong\u003e which was \u003ccode\u003echmod 777 dp-pcap/\u003c/code\u003e. Would not recommend.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/rotate-tcpdump-capture-with--g-and--w-/",
        "date_published": "27046-27-09T411:2727:00+03:30",
        "date_modified": "27046-27-09T411:2727:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "6aed3ec305dca42aed3eaffcb7dca872abee8e85",
        "title": "Create a MongoDB Replica Set Using Docker Compose",
        "summary": "",
        "content_text": "I\u0026rsquo;ve been trying to setup a Graylog multi-node cluster for testing purposes and for that I needed to create a mongoDB replica set. Using docker seemed like the most logical choice so here\u0026rsquo;s how I did it using Docker Compose.\nI created 3 MongoDB containers, exposed the relevant ports and started the mongod process using the --replSet option and then specifying my replica set name such as rs01. Then you create your volumes for each MongoDB container.\nAfter that I use a bash script rs-init.sh (placed under the scripts folder at the root of project) to initiate the replica set on mongo1 container.\nFinally we use a another bash script StartReplicaSet.sh to stop and start the containers using docker compose.\nSo here\u0026rsquo;s my full docker compose file:\nversion: \u0026#39;3.8\u0026#39; services: mongo1: container_name: mongo1 image: mongo volumes: - ./scripts/rs-init.sh:/scripts/rs-init.sh - /opt/mongo1/:/data/db networks: - mongo-network ports: - 27017:27017 depends_on: - mongo2 - mongo3 links: - mongo2 - mongo3 restart: always entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ] mongo2: container_name: mongo2 image: mongo volumes: - /opt/mongo2/:/data/db networks: - mongo-network ports: - 27018:27017 restart: always entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ] mongo3: container_name: mongo3 image: mongo volumes: - /opt/mongo3/:/data/db networks: - mongo-network ports: - 27019:27017 restart: always entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ] networks: mongo-network: name: mongo-network driver: bridge rs-init.sh: This is the bash script which initiates the replica set:\nDELAY=25 mongosh \u0026lt;\u0026lt;EOF var config = { \u0026#34;_id\u0026#34;: \u0026#34;rs01\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;members\u0026#34;: [ { \u0026#34;_id\u0026#34;: 1, \u0026#34;host\u0026#34;: \u0026#34;mongo1:27017\u0026#34;, \u0026#34;priority\u0026#34;: 2 }, { \u0026#34;_id\u0026#34;: 2, \u0026#34;host\u0026#34;: \u0026#34;mongo2:27017\u0026#34;, \u0026#34;priority\u0026#34;: 1 }, { \u0026#34;_id\u0026#34;: 3, \u0026#34;host\u0026#34;: \u0026#34;mongo3:27017\u0026#34;, \u0026#34;priority\u0026#34;: 1 } ] }; rs.initiate(config, { force: true }); EOF echo \u0026#34;====\u0026gt; Waiting for ${DELAY} seconds for replica set configuration to be applied\u0026#34; sleep $DELAY StartReplicaSet.sh: This is the final script that starts the replica set:\n#!/bin/bash DELAY=10 docker compose down docker compose up -d echo \u0026#34;====\u0026gt; Waiting for ${DELAY} seconds for containers to go up\u0026#34; sleep $DELAY docker exec mongo1 sh -c \u0026#39;chmod +x /scripts/rs-init.sh\u0026#39; docker exec mongo1 /scripts/rs-init.sh Now let\u0026rsquo;s walk through each step:\nWe start by executing the bash script StartReplicaSet.sh which first stops previous containers and then starts the containers using docker compose. It then waits for containers to start and get ready by using sleep in seconds. (specified by the DELAY variable) After the containers are up, we make the rs-init.sh script which is one mongo1 container executable and run it. rs-init.sh on mongo1 then initiates the replica set, electing mongo1 as the primary since we\u0026rsquo;ve specified the priority And that\u0026rsquo;s pretty much it. The cluster should then be up and running. One last thing to note is that if you\u0026rsquo;re using an older version of MongoDB you might have to change the command mongosh to mongo\n",
        "content_html": "\u003cp\u003eI\u0026rsquo;ve been trying to setup a Graylog multi-node cluster for testing purposes and for that I needed to create a mongoDB replica set. Using docker seemed like the most logical choice so here\u0026rsquo;s how I did it using Docker Compose.\u003c/p\u003e\n\u003cp\u003eI created 3 MongoDB containers, exposed the relevant ports and started the \u003ccode\u003emongod\u003c/code\u003e process using the \u003ccode\u003e--replSet\u003c/code\u003e option and then specifying my replica set name such as \u003ccode\u003ers01\u003c/code\u003e. Then you create your volumes for each MongoDB container.\u003c/p\u003e\n\u003cp\u003eAfter that I use a bash script \u003ccode\u003ers-init.sh\u003c/code\u003e (placed under the scripts folder at the root of project) to initiate the replica set on \u003ccode\u003emongo1\u003c/code\u003e container.\u003c/p\u003e\n\u003cp\u003eFinally we use a another bash script \u003ccode\u003eStartReplicaSet.sh\u003c/code\u003e to stop and start the containers using docker compose.\u003c/p\u003e\n\u003cp\u003eSo here\u0026rsquo;s my full docker compose file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eversion: \u0026#39;3.8\u0026#39;\n\nservices:\n  mongo1:\n    container_name: mongo1\n    image: mongo\n    volumes:\n      - ./scripts/rs-init.sh:/scripts/rs-init.sh\n      - /opt/mongo1/:/data/db\n    networks:\n      - mongo-network\n    ports:\n      - 27017:27017\n    depends_on:\n      - mongo2\n      - mongo3\n    links:\n      - mongo2\n      - mongo3\n    restart: always\n    entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ]\n\n  mongo2:\n    container_name: mongo2\n    image: mongo\n    volumes:\n      - /opt/mongo2/:/data/db\n    networks:\n      - mongo-network\n    ports:\n      - 27018:27017\n    restart: always\n    entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ]\n\n  mongo3:\n    container_name: mongo3\n    image: mongo\n    volumes:\n      - /opt/mongo3/:/data/db\n    networks:\n      - mongo-network\n    ports:\n      - 27019:27017\n    restart: always\n    entrypoint: [ \u0026#34;/usr/bin/mongod\u0026#34;, \u0026#34;--bind_ip_all\u0026#34;, \u0026#34;--replSet\u0026#34;, \u0026#34;rs01\u0026#34; ]\n\nnetworks:\n  mongo-network:\n    name: mongo-network\n    driver: bridge\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003ers-init.sh\u003c/code\u003e: This is the bash script which initiates the replica set:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eDELAY=25\n\nmongosh \u0026lt;\u0026lt;EOF\nvar config = {\n    \u0026#34;_id\u0026#34;: \u0026#34;rs01\u0026#34;,\n    \u0026#34;version\u0026#34;: 1,\n    \u0026#34;members\u0026#34;: [\n        {\n            \u0026#34;_id\u0026#34;: 1,\n            \u0026#34;host\u0026#34;: \u0026#34;mongo1:27017\u0026#34;,\n            \u0026#34;priority\u0026#34;: 2\n        },\n        {\n            \u0026#34;_id\u0026#34;: 2,\n            \u0026#34;host\u0026#34;: \u0026#34;mongo2:27017\u0026#34;,\n            \u0026#34;priority\u0026#34;: 1\n        },\n        {\n            \u0026#34;_id\u0026#34;: 3,\n            \u0026#34;host\u0026#34;: \u0026#34;mongo3:27017\u0026#34;,\n            \u0026#34;priority\u0026#34;: 1\n        }\n    ]\n};\nrs.initiate(config, { force: true });\nEOF\n\necho \u0026#34;====\u0026gt; Waiting for ${DELAY} seconds for replica set configuration to be applied\u0026#34;\n\nsleep $DELAY\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003eStartReplicaSet.sh\u003c/code\u003e: This is the final script that starts the replica set:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\n\nDELAY=10\n\ndocker compose down\ndocker compose up -d\n\necho \u0026#34;====\u0026gt; Waiting for ${DELAY} seconds for containers to go up\u0026#34;\nsleep $DELAY\n\ndocker exec mongo1 sh -c \u0026#39;chmod +x /scripts/rs-init.sh\u0026#39;\ndocker exec mongo1 /scripts/rs-init.sh\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow let\u0026rsquo;s walk through each step:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe start by executing the bash script \u003ccode\u003eStartReplicaSet.sh\u003c/code\u003e which first stops previous containers and then starts the containers using \u003ccode\u003edocker compose\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eIt then waits for containers to start and get ready by using sleep in seconds. (specified by the \u003ccode\u003eDELAY\u003c/code\u003e variable)\u003c/li\u003e\n\u003cli\u003eAfter the containers are up, we make the \u003ccode\u003ers-init.sh\u003c/code\u003e script which is one mongo1 container executable and run it.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ers-init.sh\u003c/code\u003e on mongo1 then initiates the replica set, electing mongo1 as the primary since we\u0026rsquo;ve specified the priority\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAnd that\u0026rsquo;s pretty much it. The cluster should then be up and running. One last thing to note is that if you\u0026rsquo;re using an older version of MongoDB you might have to change the command \u003ccode\u003emongosh\u003c/code\u003e to \u003ccode\u003emongo\u003c/code\u003e\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/create-a-mongodb-replicaset-using-docker-compose/",
        "date_published": "15046-15-09T427:1515:00+03:30",
        "date_modified": "15046-15-09T427:1515:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "ae908db3eac02380921f4c8e301f41794b1e4a4f",
        "title": "Install IPA Client on Debian 11",
        "summary": "",
        "content_text": "Couple of months ago I was setting up an IPA server for our infrastructure using CentOS. That went pretty smoothly but when I started joining clients to the IPA server, I quickly found out that the IPA client package was missing from Debian 11 repositories due to some bugs. This was clearly not ideal since most of our infrastructure runs on Debian 11.\nMy second solution was to use the backports package from Debian, but that also failed on my machines due to some missing libraries that were present in Debian 10 but were removed from Debian 11.\nNext, I tried to manually install and configure each service (SSSD, LDAP, NSS, Kerberos, etc) but to no avail. I feel like doing the configuration manually is prone to a lot of mistakes. So then I decided to write a bash script for it. Using the script everything is configured manually on the client machine. It needs to have access to the IPA server via ssh public key so it can copy some data from it such as CA certificate keys. It most definitely has many bugs, but it seems to be working for me now and I\u0026rsquo;ve joined 40+ Debian 11 nodes using this script.\nI\u0026rsquo;m sharing the link here in the hopes that some poor soul stuck on this issue finds it and can use it.\nhttps://github.com/ali-foroughi/ipa-client-install\nIf you do end up using it, please let me know how it went. 😅\nHere\u0026rsquo;s hoping that Debian finally gets their act together and fixes the package 🥂\n",
        "content_html": "\u003cp\u003eCouple of months ago I was setting up an IPA server for our infrastructure using CentOS. That went pretty smoothly but when I started joining clients to the IPA server, I quickly found out that the \u003ca href=\"https://groups.google.com/g/linux.debian.project/c/0tZoaWBLtlg\"\u003eIPA client package was missing from Debian 11\u003c/a\u003e repositories due to some bugs. This was clearly not ideal since most of our infrastructure runs on Debian 11.\u003c/p\u003e\n\u003cp\u003eMy second solution was to use the \u003ca href=\"https://packages.debian.org/bullseye-backports/freeipa-client\"\u003ebackports package\u003c/a\u003e from Debian, but that also failed on my machines due to some missing libraries that were present in Debian 10 but were removed from Debian 11.\u003c/p\u003e\n\u003cp\u003eNext, I tried to manually install and configure each service (SSSD, LDAP, NSS, Kerberos, etc) but to no avail. I feel like doing the configuration manually is prone to a lot of mistakes. So then I decided to write a bash script for it. Using the script everything is configured manually on the client machine. It needs to have access to the IPA server via ssh public key so it can copy some data from it such as CA certificate keys. It most definitely has many bugs, but it seems to be working for me now and I\u0026rsquo;ve joined 40+ Debian 11 nodes using this script.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m sharing the link here in the hopes that some poor soul stuck on this issue finds it and can use it.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ali-foroughi/ipa-client-install\"\u003ehttps://github.com/ali-foroughi/ipa-client-install\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you do end up using it, please let me know how it went. 😅\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s hoping that Debian finally gets their act together and fixes the package 🥂\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/install-ipa-client-on-debian/",
        "date_published": "5046-05-09T427:55:00+03:30",
        "date_modified": "5046-05-09T427:55:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "4e701a6a23658822a33b679712525bb36cebfb64",
        "title": "Updating Timezone Using Ansible",
        "summary": "",
        "content_text": "Recently Iran has stopped observing day-light savings (DST) time, which has caused various problems for everyone in the IT field. All of our servers were showing the wrong time since the tzdata package wasn\u0026rsquo;t updated. I wrote this Ansible playbook to upgrade everything in an instance.\nThis playbook is for upgrading the tzdata package on Debain-based OS, using APT package mananger. If you don\u0026rsquo;t need an HTTP proxy for APT, simply remove the tasks related to it.\n- name: Upgrade tzdata package for correcting timezone hosts: all become: yes tasks: - name: Create a directory for apt proxy ansible.builtin.file: path: /etc/apt/apt.conf.d/ state: directory mode: \u0026#39;0744\u0026#39; - name: Create a proxy file if it doesn\u0026#39;t exist ansible.builtin.file: path: /etc/apt/apt.conf.d/10proxy state: touch mode: \u0026#39;0744\u0026#39; - name: Edit /etc/apt/apt.conf.d/10proxy and insert http proxy IP lineinfile: path: /etc/apt/apt.conf.d/10proxy state: present line: Acquire::http { Proxy \u0026#34;http://172.17.93.162:3142\u0026#34;; } - name: Edit /etc/apt/apt.conf.d/10proxy and insert https proxy IP lineinfile: path: /etc/apt/apt.conf.d/10proxy state: present line: Acquire::https { Proxy \u0026#34;http://172.17.93.162:3142\u0026#34;; } - name: Upgrade tzdata ansible.builtin.apt: name: tzdata state: latest update_cache: yes update_cache_retries: 2 only_upgrade: true ###UPDATE###:\nSo it turns out some services like Syslog need to be restarted so they can read the correct time from the system. If you\u0026rsquo;re able to, I\u0026rsquo;d suggest to just go ahead and restart the server. Otherwise, you can just restart Syslog:\nsystemctl restart syslog.socket ",
        "content_html": "\u003cp\u003eRecently Iran has stopped observing day-light savings (DST) time, which has caused various problems for everyone in the IT field. All of our servers were showing the wrong time since the tzdata package wasn\u0026rsquo;t updated. I wrote this Ansible playbook to upgrade everything in an instance.\u003c/p\u003e\n\u003cp\u003eThis playbook is for upgrading the tzdata package on Debain-based OS, using APT package mananger.\nIf you don\u0026rsquo;t need an HTTP proxy for APT, simply remove the tasks related to it.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e- name: Upgrade tzdata package for correcting timezone\n  hosts: all\n  become: yes\n  tasks:\n    - name: Create a directory for apt proxy\n      ansible.builtin.file:\n        path: /etc/apt/apt.conf.d/\n        state: directory\n        mode: \u0026#39;0744\u0026#39;\n\n    - name: Create a proxy file if it doesn\u0026#39;t exist\n      ansible.builtin.file:\n        path: /etc/apt/apt.conf.d/10proxy\n        state: touch\n        mode: \u0026#39;0744\u0026#39;\n\n    - name: Edit /etc/apt/apt.conf.d/10proxy and insert http proxy IP\n      lineinfile:\n        path: /etc/apt/apt.conf.d/10proxy\n        state: present\n        line: Acquire::http { Proxy \u0026#34;http://172.17.93.162:3142\u0026#34;; }\n\n    - name: Edit /etc/apt/apt.conf.d/10proxy and insert https proxy IP\n      lineinfile:\n        path: /etc/apt/apt.conf.d/10proxy\n        state: present\n        line: Acquire::https { Proxy \u0026#34;http://172.17.93.162:3142\u0026#34;; }\n\n    - name: Upgrade tzdata\n      ansible.builtin.apt:\n        name: tzdata\n        state: latest\n        update_cache: yes\n        update_cache_retries: 2\n        only_upgrade: true\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003e###UPDATE###\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eSo it turns out some services like Syslog need to be restarted so they can read the correct time from the system. If you\u0026rsquo;re able to, I\u0026rsquo;d suggest to just go ahead and restart the server. Otherwise, you can just restart Syslog:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl restart syslog.socket\n\u003c/code\u003e\u003c/pre\u003e",
        "url": "https://workingtitle.pro/posts/updating-timezone-using-ansible/",
        "date_published": "3046-03-09T455:33:00+03:30",
        "date_modified": "3046-03-09T455:33:00+03:30",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "4aa25417b9de86f088dff29dda6764746530e04d",
        "title": "Create a Highly Available Kubernetes Cluster From Scratch",
        "summary": "",
        "content_text": "In this guide we’re looking to create a highly available Kubernetes cluster with multiple control plane nodes, loadbalancers and worker nodes.\nThe architecture of this Kubernetes cluster ensures a good level of availability and reliability for use in a production environment, but it is by no means fail-safe.\nI’ve followed the recommendations from Kubernetes documentations which you can find here. All I’ve done is to present them in a curated manner.\nWhat you’ll need 3 Virtual machines for master nodes running Debian or CentOS with at least 2 GB of RAM and 2 CPU cores 2 worker nodes running Debian or CentOs. It can be either VM’s or bare-metal servers. Use full bare metal servers if you have heavy workloads At least 2 virtual machines running Debian or CentOs for load balancing Architecture 3 separate master nodes (control planes) for redundancy the master nodes are connected via loadbalancer we’ll have at least 2 load balancing instance where they negotiate a virtual IP between the instances worker nodes connect to the loadbalancer and the loadbalancer distributes request between control plane nodes Setting up the Load balancers We’ll be using HA proxy and Keepalived for the load balancing solution. I’ve followed this guide for reference.\nInstall HAProxy and Keepalived on both load balancing server apt install haproxy apt install keepalived Edit /etc/keepalived/keepalived.conf and make the configurations ! /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_apiserver { script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state ${STATE} interface ${INTERFACE} virtual_router_id ${ROUTER_ID} priority ${PRIORITY} authentication { auth_type PASS auth_pass ${AUTH_PASS} } virtual_ipaddress { ${APISERVER_VIP} } track_script { check_apiserver } } Add the script for health checking /etc/keepalived/check_apiserver.sh #!/bin/sh errorExit() { echo \u0026#34;*** $*\u0026#34; 1\u0026gt;\u0026amp;2 exit 1 } curl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \u0026#34;Error GET https://localhost:${APISERVER_DEST_PORT}/\u0026#34; if ip addr | grep -q ${APISERVER_VIP}; then curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \u0026#34;Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/\u0026#34; fi Edit /etc/haproxy/haproxy.cfg and make the configurations based on the guide # /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global log /dev/log local0 log /dev/log local1 notice daemon #--------------------------------------------------------------------- # common defaults that all the \u0026#39;listen\u0026#39; and \u0026#39;backend\u0026#39; sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 1 timeout http-request 10s timeout queue 20s timeout connect 5s timeout client 20s timeout server 20s timeout http-keep-alive 10s timeout check 10s #--------------------------------------------------------------------- # apiserver frontend which proxys to the control plane nodes #--------------------------------------------------------------------- frontend apiserver bind *:${APISERVER_DEST_PORT} mode tcp option tcplog default_backend apiserver #--------------------------------------------------------------------- # round robin balancing for apiserver #--------------------------------------------------------------------- backend apiserver option httpchk GET /healthz http-check expect status 200 mode tcp option ssl-hello-chk balance roundrobin server ${HOST1_ID} ${HOST1_ADDRESS}:${APISERVER_SRC_PORT} check The configuration on both servers can be identical except two parts in the keepalived configuration: state MASTER state BACKUP The MASTER state should be on the main load balancer node and the BACKUP state should be used on all others. You can have many BACKUP nodes with the same state.\npriority ${PRIORITY} Should be LOWER on the MASTER server. For example you can configure priority 100 on the MASTER server, priority 101 on the first BACKUP server, priority 102 on the second BACKUP server and so on.\noption httpchk GET /healthz This option should probably be changed to /livez. Check your kube-apiserver configuration file and match it to this value.\nOnce the configuration on both servers is done restart the services service haproxy restart service keepalived restart Setting up master nodes First master node (main control plane) IMPORTANT NOTE:\nOn Debian machines, you need to edit /etc/default/grub and set systemd.unified_cgroup_hierarchy=0 as the value for GRUB_CMDLINE_LINUX_DEFAULT as so:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;systemd.unified_cgroup_hierarchy=0\u0026#34; Then update grub:\nupdate-grub and reboot the server.\nyum update OR apt update \u0026amp;\u0026amp; apt upgrade\nDisable SElinux (for CentOS)\nnano /etc/selinux/config . . . SELINUX=disabled Letting iptables see bridged traﬃc cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system Set all hostnames in /etc/hosts if you’re not using a DNS serve Turn off swap swapoff -a Open /etc/fstab and comment out the section related to swap Install Docker Install kubeadm Open ports with firewalld sudo firewall-cmd --zone=public --permanent --add-port=6443/tcp sudo firewall-cmd --zone=public --permanent --add-port=2379-2381/tcp sudo firewall-cmd --zone=public --permanent --add-port=10250/tcp sudo firewall-cmd --zone=public --permanent --add-port=10257/tcp sudo firewall-cmd --reload Configure native cgroups driver cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ] } EOF Then apply the changes systemctl daemon-reload systemctl restart docker Pull kubeadm images kubeadm config images pull --kubernetes-version v1.24.0 You can specify the desired version with \u0026ndash;kubernetes-version. It’s recommended for all nodes to have the same version so it’s better to manually pull the same version on each master node as to avoid confusion.\nAfter successfully pulling the images, initialize the master node via this command: kubeadm init --apiserver-advertise-address=CLUSTER-ENDPOINT --control-plane-endpoint=cluster-endpoint --pod-network-cidr=10.244.0.0/16 --upload-certs --kubernetes-version v1.24.0 CLUSTER-ENDPOINT should point to the virtual IP of the loadbalancer. You can define it in /etc/hosts if you’re not using a DNS server.\nApply flannel for cluster networking kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml Check the status of pods via: kubectl get pods -A Everything should be running normally.\nSave the output from the kubeadm init command so it can be used for starting other master and worker nodes Second and Third Master nodes Follow all of the steps mentioned in the previous section and pull the kubeadm images with: kubeadm config images pull --kubernetes-version v1.24.0 Join the second and third master nodes to the cluster via the output from the first master node. Kubectl join … Adding worker nodes to the cluster Login to your worker node(s) Follow the same steps from the master node installation and pull the kubeadm images with: kubeadm config images pull --kubernetes-version v1.24.0 Open the following ports with iptables or other firewall iptables -A INPUT -p tcp --dport 10250 -j ACCEPT iptables -A INPUT -p tcp --dport 30000:35000 -j ACCEPT iptables -A INPUT -p tcp --dport 10248 -j ACCEPT iptables-save \u0026gt; /etc/iptables/rules.v4 Make sure DNS names are configured in /etc/hosts and the nameservers are set in /etc/resolv.conf\nUse kubeadm join command with the token acquired from the first master node to join the server into the cluster\nFinally, log into one of your master node (control plane) and run the following command to see all the joined nodes:\nkubectl get nodes -A You should see an output of all your nodes. (master + worker nodes)\nHopefully this article helped you in learning how to create a highly available Kubernetes cluster.\nResources Create a highly available Kubernetes cluster Creating clusters with kubeadm Installing kubeadm kubelet configuration Issues with coreDNS edit KUBELET_NETWORK_ARGS Disable selinux Disable GPG checking Define cgroups driver systemd kube-schedular fails on debain ",
        "content_html": "\u003cp\u003eIn this guide we’re looking to create a highly available Kubernetes cluster with multiple control plane nodes, loadbalancers and worker nodes.\u003c/p\u003e\n\u003cp\u003eThe architecture of this Kubernetes cluster ensures a good level of availability and reliability for use in a production environment, but it is by no means fail-safe.\u003c/p\u003e\n\u003cp\u003eI’ve followed the recommendations from Kubernetes documentations which you can find \u003ca href=\"https://kubernetes.io/docs/home/\"\u003ehere\u003c/a\u003e. All I’ve done is to present them in a curated manner.\u003c/p\u003e\n\u003ch2 id=\"what-youll-need\"\u003eWhat you’ll need\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e3 Virtual machines for master nodes  running Debian or CentOS  with at least 2 GB of RAM and 2 CPU cores\u003c/li\u003e\n\u003cli\u003e2 worker nodes running Debian or CentOs. It can be either VM’s or bare-metal servers. Use full bare metal servers if you have heavy workloads\u003c/li\u003e\n\u003cli\u003eAt least 2 virtual machines running Debian or CentOs for load balancing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"architecture\"\u003eArchitecture\u003c/h2\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/kuber-arch.png\"\n  alt=\"kuber-arch\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e3 separate master nodes (control planes) for redundancy\u003c/li\u003e\n\u003cli\u003ethe master nodes are connected via loadbalancer\u003c/li\u003e\n\u003cli\u003ewe’ll have at least 2 load balancing instance where they negotiate a virtual IP between the instances\u003c/li\u003e\n\u003cli\u003eworker nodes connect to the loadbalancer and the loadbalancer distributes request between control plane nodes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"setting-up-the-load-balancers\"\u003eSetting up the Load balancers\u003c/h2\u003e\n\u003cp\u003eWe’ll be using HA proxy and Keepalived for the load balancing solution. I’ve followed \u003ca href=\"https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing\"\u003ethis guide\u003c/a\u003e for reference.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInstall HAProxy and Keepalived on both load balancing server\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e apt install haproxy\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapt install keepalived\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eEdit /etc/keepalived/keepalived.conf and make the configurations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e! /etc/keepalived/keepalived.conf\n! Configuration File for keepalived\nglobal_defs {\n    router_id LVS_DEVEL\n}\n\nvrrp_script check_apiserver {\n  script \u0026#34;/etc/keepalived/check_apiserver.sh\u0026#34;\n  interval 3\n  weight -2\n  fall 10\n  rise 2\n}\n\nvrrp_instance VI_1 {\n    state ${STATE}\n    interface ${INTERFACE}\n    virtual_router_id ${ROUTER_ID}\n    priority ${PRIORITY}\n    authentication {\n        auth_type PASS\n        auth_pass ${AUTH_PASS}\n    }\n\n    virtual_ipaddress {\n        ${APISERVER_VIP}\n    }\n\n    track_script {\n        check_apiserver\n    }\n\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eAdd the script for health checking \u003ccode\u003e/etc/keepalived/check_apiserver.sh\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/sh\nerrorExit() {\n    echo \u0026#34;*** $*\u0026#34; 1\u0026gt;\u0026amp;2\n    exit 1\n}\n\ncurl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \u0026#34;Error GET https://localhost:${APISERVER_DEST_PORT}/\u0026#34;\n\nif ip addr | grep -q ${APISERVER_VIP}; then\n\n    curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \u0026#34;Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/\u0026#34;\n\nfi\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eEdit \u003ccode\u003e/etc/haproxy/haproxy.cfg\u003c/code\u003e and make the configurations based on the guide\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# /etc/haproxy/haproxy.cfg\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    log /dev/log local0\n    log /dev/log local1 notice\n    daemon\n#---------------------------------------------------------------------\n# common defaults that all the \u0026#39;listen\u0026#39; and \u0026#39;backend\u0026#39; sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 1\n    timeout http-request    10s\n    timeout queue           20s\n    timeout connect         5s\n    timeout client          20s\n    timeout server          20s\n    timeout http-keep-alive 10s\n    timeout check           10s\n#---------------------------------------------------------------------\n# apiserver frontend which proxys to the control plane nodes\n#---------------------------------------------------------------------\nfrontend apiserver\n    bind *:${APISERVER_DEST_PORT}\n    mode tcp\n    option tcplog\n    default_backend apiserver\n#---------------------------------------------------------------------\n# round robin balancing for apiserver\n#---------------------------------------------------------------------\nbackend apiserver\n    option httpchk GET /healthz\n    http-check expect status 200\n    mode tcp\n    option ssl-hello-chk\n    balance     roundrobin\n        server ${HOST1_ID} ${HOST1_ADDRESS}:${APISERVER_SRC_PORT} check\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eThe configuration on both servers can be identical except two parts in the keepalived configuration:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003estate MASTER\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003estate BACKUP\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe \u003ccode\u003eMASTER\u003c/code\u003e state should be on the main load balancer node and the \u003ccode\u003eBACKUP\u003c/code\u003e  state should be used on all others. You can have many \u003ccode\u003eBACKUP\u003c/code\u003e  nodes with the same state.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epriority ${PRIORITY}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eShould be \u003cb\u003eLOWER\u003c/b\u003e on the \u003ccode\u003eMASTER\u003c/code\u003e server. For example you can configure priority 100 on the \u003ccode\u003eMASTER\u003c/code\u003e server, priority 101 on the first \u003ccode\u003eBACKUP\u003c/code\u003e server, priority 102 on the second \u003ccode\u003eBACKUP\u003c/code\u003e server and so on.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eoption httpchk GET /healthz\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis option should probably be changed to /livez. Check your kube-apiserver configuration file and match it to this value.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOnce the configuration on both servers is done restart the services\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice haproxy restart\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice keepalived restart\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"setting-up-master-nodes\"\u003eSetting up master nodes\u003c/h2\u003e\n\u003cbr\u003e\n\u003ch3 id=\"first-master-node-main-control-plane\"\u003eFirst master node (main control plane)\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003eIMPORTANT NOTE:\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003eOn Debian machines, you need to edit \u003ccode\u003e/etc/default/grub\u003c/code\u003e and set \u003ccode\u003esystemd.unified_cgroup_hierarchy=0\u003c/code\u003e as the value for \u003ccode\u003eGRUB_CMDLINE_LINUX_DEFAULT\u003c/code\u003e as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;systemd.unified_cgroup_hierarchy=0\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen update grub:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eupdate-grub\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eand reboot the server.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eyum update\u003c/code\u003e OR \u003ccode\u003eapt update\u003c/code\u003e \u0026amp;\u0026amp; apt upgrade\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDisable SElinux (for CentOS)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enano  /etc/selinux/config\n.\n.\n.\nSELINUX=disabled\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eLetting iptables see bridged traﬃc\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esysctl --system\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eSet all hostnames in \u003ccode\u003e/etc/hosts\u003c/code\u003e if you’re not using a DNS serve\u003c/li\u003e\n\u003cli\u003eTurn off swap\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eswapoff -a\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eOpen /etc/fstab and comment out the section related to swap\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.docker.com/engine/install/\"\u003eInstall Docker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\"\u003eInstall kubeadm\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eOpen ports with firewalld\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo firewall-cmd --zone=public --permanent --add-port=6443/tcp\nsudo firewall-cmd --zone=public --permanent --add-port=2379-2381/tcp\nsudo firewall-cmd --zone=public --permanent --add-port=10250/tcp\nsudo firewall-cmd --zone=public --permanent --add-port=10257/tcp\nsudo firewall-cmd --reload\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eConfigure native cgroups driver\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF{\n  \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;],\n  \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;,\n  \u0026#34;log-opts\u0026#34;: {\n    \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;\n  },\n  \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;,\n  \u0026#34;storage-opts\u0026#34;: [\n    \u0026#34;overlay2.override_kernel_check=true\u0026#34;\n  ]\n}\nEOF\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eThen apply the changes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl daemon-reload\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl restart docker\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003ePull kubeadm images\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubeadm config images pull --kubernetes-version v1.24.0\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can specify the desired version with \u003ccode\u003e\u0026ndash;kubernetes-version\u003c/code\u003e. It’s recommended for all nodes to have the same version so it’s better to manually pull the same version on each master node as to avoid confusion.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAfter successfully pulling the images, initialize the master node via this command:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubeadm init  --apiserver-advertise-address=CLUSTER-ENDPOINT --control-plane-endpoint=cluster-endpoint --pod-network-cidr=10.244.0.0/16 --upload-certs --kubernetes-version v1.24.0\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003eCLUSTER-ENDPOINT\u003c/code\u003e should point to the virtual IP of the loadbalancer. You can define it in \u003ccode\u003e/etc/hosts\u003c/code\u003e if you’re not using a DNS server.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eApply flannel for cluster networking\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eCheck the status of pods via:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubectl  get pods -A\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eEverything should be running normally.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSave the output from the kubeadm init command so it can be used for starting other master and worker nodes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"second-and-third-master-nodes\"\u003eSecond and Third Master nodes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eFollow all of the steps mentioned in the previous section and pull the kubeadm images with:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubeadm config images pull --kubernetes-version v1.24.0\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eJoin the second and third master nodes to the cluster via the output from the first master node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eKubectl join … \n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"adding-worker-nodes-to-the-cluster\"\u003eAdding worker nodes to the cluster\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLogin to your worker node(s)\u003c/li\u003e\n\u003cli\u003eFollow the same steps from the master node installation and pull the kubeadm images with:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubeadm config images pull --kubernetes-version v1.24.0\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eOpen the following ports with iptables or other firewall\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eiptables -A INPUT -p tcp --dport 10250 -j ACCEPT\niptables -A INPUT -p tcp --dport 30000:35000 -j ACCEPT\niptables -A INPUT -p tcp --dport 10248 -j ACCEPT\niptables-save \u0026gt; /etc/iptables/rules.v4\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMake sure DNS names are configured in \u003ccode\u003e/etc/hosts\u003c/code\u003e and the nameservers are set in \u003ccode\u003e/etc/resolv.conf\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse kubeadm join command with the token acquired from the first master node to join the server into the cluster\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, log into one of your master node (control plane) and run the following command to see all the joined nodes:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ekubectl get nodes -A\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou should see an output of all your nodes. (master + worker nodes)\u003c/p\u003e\n\u003cp\u003eHopefully this article helped you in learning how to create a highly available Kubernetes cluster.\u003c/p\u003e\n\u003ch2 id=\"resources\"\u003eResources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/\"\u003eCreate a highly available Kubernetes cluster\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\"\u003eCreating clusters with kubeadm\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\"\u003eInstalling kubeadm\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/\"\u003ekubelet configuration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://stackoverflow.com/questions/52645473/coredns-fails-to-run-in-kubernetes-cluster\"\u003eIssues with coreDNS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://serverfault.com/questions/1055263/kube-apiserver-exits-while-control-plane-joining-the-ha-cluster\"\u003eedit KUBELET_NETWORK_ARGS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://linuxize.com/post/how-to-disable-selinux-on-centos-7/\"\u003eDisable selinux\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://serverfault.com/questions/288648/disable-the-public-key-check-for-rpm-installation\"\u003eDisable GPG checking\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd\"\u003eDefine cgroups driver systemd\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://discuss.kubernetes.io/t/why-does-etcd-fail-with-debian-bullseye-kernel/19696\"\u003ekube-schedular fails on debain\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
        "url": "https://workingtitle.pro/posts/create-a-highly-available-kubernetes-cluster-from-scratch/",
        "date_published": "20066-20-09T60:2020:00+00:00",
        "date_modified": "20066-20-09T60:2020:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "7acfa21181a86dda9dc3af514cc1a49c2a5e92c3",
        "title": "How to Recover a MySQL Database With frm and .ibd Files",
        "summary": "",
        "content_text": "In order to recover a MySQL database, you need to have access to the .frm and .ibd files. I won’t get into how to acquire them, but if you have them, you can follow these steps to recover your database. (at least partially)\nWhat you’ll need .frm and .ibd files related to your database A server running the same version of MySQL as the database you want to restore Recover database structure First thing we need to do is recover the database structure by using mysqlfrm\nmysqlfrm is a utility use to read .frm files and create the database structure based on those files. Here’s the steps:\nRun mysqlfrm under the diagnostic mode to get the table structure.\nmysqlfrm --diagnostic TABLE_NAME.frm \u0026gt; TABLE_NAME.txt This table will create the database structure (the mysql CREATE comands) and store them in a text file for reference. If you look into the txt file you’ll see an output like this:\nCREATE TABLE `wp_users` ( `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `user_login` varchar(128) NOT NULL, `user_pass` varchar(128) NOT NULL, `user_nicename` varchar(128) NOT NULL, `user_email` varchar(64) NOT NULL, `user_url` varchar(128) NOT NULL, `user_registered` datetime NOT NULL, `user_activation_key` varchar(1020) NOT NULL, `user_status` int(11) NOT NULL, `display_name` varchar(1000) NOT NULL, PRIMARY KEY `PRIMARY` (`ID`), KEY `user_login_key` (`user_login`), KEY `user_nicename` (`user_nicename`), KEY `user_email` (`user_email`) ) ENGINE=InnoDB ROW_FORMAT=compact; Create the database on MySQL Log into your MySQL server and create the MySQL database where you want the files to be restored. Take note that your MySQL query for creating the database has to include the default character set and collate. Here’s how I made my database:\nCREATE DATABASE my_database DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci; Create table and import the .ibd file Now we need to create the table using the .frm file that we extracted. Select the database and run your CREATE command that was included in the TABLE_NAME.txt\nmysql \u0026gt; USE my_database; mysql \u0026gt; CREATE TABLE `wp_users` ( `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `user_login` varchar(128) NOT NULL, `user_pass` varchar(128) NOT NULL, `user_nicename` varchar(128) NOT NULL, `user_email` varchar(64) NOT NULL, `user_url` varchar(128) NOT NULL, `user_registered` datetime NOT NULL, `user_activation_key` varchar(1020) NOT NULL, `user_status` int(11) NOT NULL, `display_name` varchar(1000) NOT NULL, PRIMARY KEY `PRIMARY` (`ID`), KEY `user_login_key` (`user_login`), KEY `user_nicename` (`user_nicename`), KEY `user_email` (`user_email`) ) ENGINE=InnoDB ROW_FORMAT=compact; Now we need to drop the .ibd file that’s created with the above command and replace it with the .ibd file we already have.\nALTER TABLE table_name DISCARD TABLESPACE; Copy your own .ibd file to the directory where the database resides. On an ubuntu/debian server that directory would be:\n/var/lib/mysql now we need to import the new .ibd file for the table we just created:\nALTER TABLE table_name IMPORT TABLESPACE; You should see a message that the query was successfully executed.\nRepeat the same steps for every single table. you could write a bash script to automatically run the same tasks for each table. Export the restored database After following the steps for each of the tables, your database should be back to its original state. (more or less)\nThere might be some data corruption, but you’ll likely get back most of the content.\nRun the following command to export your MySQL database:\nmysqldump my_database \u0026gt; database_name_export.sql ",
        "content_html": "\u003cp\u003eIn order to recover a MySQL database, you need to have access to the .frm and .ibd files. I won’t get into how to acquire them, but if you have them, you can follow these steps to recover your database. (at least partially)\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"what-youll-need\"\u003eWhat you’ll need\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e.frm and .ibd files related to your database\u003c/li\u003e\n\u003cli\u003eA server running the same version of MySQL as the database you want to restore\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"recover-database-structure\"\u003eRecover database structure\u003c/h2\u003e\n\u003cp\u003eFirst thing we need to do is recover the database structure by using \u003ccode\u003emysqlfrm\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://helpmanual.io/help/mysqlfrm/\"\u003emysqlfrm\u003c/a\u003e is a utility use to read .frm files and create the database structure based on those files. Here’s the steps:\u003c/p\u003e\n\u003cp\u003eRun mysqlfrm under the diagnostic mode to get the table structure.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emysqlfrm --diagnostic TABLE_NAME.frm \u0026gt; TABLE_NAME.txt\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis table will create the database structure (the mysql CREATE comands) and store them in a text file for reference. If you look into the txt file you’ll see an output like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE TABLE `wp_users` (\n  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, \n  `user_login` varchar(128) NOT NULL, \n  `user_pass` varchar(128) NOT NULL, \n  `user_nicename` varchar(128) NOT NULL, \n  `user_email` varchar(64) NOT NULL, \n  `user_url` varchar(128) NOT NULL, \n  `user_registered` datetime NOT NULL, \n  `user_activation_key` varchar(1020) NOT NULL, \n  `user_status` int(11) NOT NULL, \n  `display_name` varchar(1000) NOT NULL, \nPRIMARY KEY `PRIMARY` (`ID`),\nKEY `user_login_key` (`user_login`),\nKEY `user_nicename` (`user_nicename`),\nKEY `user_email` (`user_email`)\n) ENGINE=InnoDB\n  ROW_FORMAT=compact;\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"create-the-database-on-mysql\"\u003eCreate the database on MySQL\u003c/h2\u003e\n\u003cp\u003eLog into your MySQL server and create the MySQL database where you want the files to be restored. Take note that your MySQL query for creating the database has to include the default character set and collate. Here’s how I made my database:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE DATABASE my_database DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"create-table-and-import-the-ibd-file\"\u003eCreate table and import the .ibd file\u003c/h2\u003e\n\u003cp\u003eNow we need to create the table using the .frm file that we extracted. Select the database and run your CREATE command that was included in the \u003ccode\u003eTABLE_NAME.txt\u003c/code\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emysql \u0026gt; USE my_database;\nmysql \u0026gt; CREATE TABLE `wp_users` (\n  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, \n  `user_login` varchar(128) NOT NULL, \n  `user_pass` varchar(128) NOT NULL, \n  `user_nicename` varchar(128) NOT NULL, \n  `user_email` varchar(64) NOT NULL, \n  `user_url` varchar(128) NOT NULL, \n  `user_registered` datetime NOT NULL, \n  `user_activation_key` varchar(1020) NOT NULL, \n  `user_status` int(11) NOT NULL, \n  `display_name` varchar(1000) NOT NULL, \nPRIMARY KEY `PRIMARY` (`ID`),\nKEY `user_login_key` (`user_login`),\nKEY `user_nicename` (`user_nicename`),\nKEY `user_email` (`user_email`)\n) ENGINE=InnoDB\n  ROW_FORMAT=compact;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we need to drop the .ibd file that’s created with the above command and replace it with the .ibd file we already have.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eALTER TABLE table_name DISCARD TABLESPACE;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eCopy your own .ibd file to the directory where the database resides. On an ubuntu/debian server that directory would be:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e/var/lib/mysql\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003enow we need to import the new .ibd file for the table we just created:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eALTER TABLE table_name IMPORT TABLESPACE;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou should see a message that the query was successfully executed.\u003c/p\u003e\n\u003cp\u003eRepeat the same steps for every single table. you could write a bash script to automatically run the same tasks for each table.\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2 id=\"export-the-restored-database\"\u003eExport the restored database\u003c/h2\u003e\n\u003cp\u003eAfter following the steps for each of the tables, your database should be back to its original state. (more or less)\u003c/p\u003e\n\u003cp\u003eThere might be some data corruption, but you’ll likely get back most of the content.\u003c/p\u003e\n\u003cp\u003eRun the following command to export your MySQL database:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003emysqldump my_database \u0026gt; database_name_export.sql\n\u003c/code\u003e\u003c/pre\u003e",
        "url": "https://workingtitle.pro/posts/how-to-recover-a-mysql-database-with-frm-and-ibd-files/",
        "date_published": "28056-28-09T50:2828:00+00:00",
        "date_modified": "28056-28-09T50:2828:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "f4a958378c179f79850957b7b1350ef5e75ddd2c",
        "title": "Linux Log Management With Graylog",
        "summary": "",
        "content_text": "I’ve been searching the web for a free and open-source log monitoring solution for Linux for a while now. I’ve tried everything from Nagios to Kibana. However I found out the best solution for me is Graylog. I basically just want to manage web server and mail server logs and get alerts if certain conditions are met. So Let’s take a look at Linux log management with Graylog.\nAlthough the official Graylog documentation is great, I’ve found out it has some shortcomings when it comes to explaining how to actually transfer the logs from your own servers to the intended master Graylog server, especially if you’ve never done this before (like me!). If you’re not familiar with Syslog for example, you’re gonna have a hard time configuration just by following the official docs. That’s why I wanted to document how I approached it which is very bare bones and simple.\nGetting Started So here’s what we’re going to do. We’re planning to transfer all of our logs from a server to a Graylog node, using Rsyslog as the transfer protocol.\nFor setting up a Graylog node we’re going to use a Ubuntu 20.04 virtual machine with 2 Gigabytes of ram and 2 CPU cores. Keep in mind that this is just for testing purposes. In a production environment, you’re going to need a much more powerful server, most likely dedicated.\nInstalling Graylog on Ubuntu 20.04 First we’re going to start by installing the prerequisite packages. Run the following commands:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade sudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen If you get a “package not found” error, you need to add the universe repositry as so:\nsudo add-apt-repository universe sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade sudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen Install MongoDB Next step is to install MongoDB via the following commands:\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 9DA31620334BD75D9DCB49F368818C72E52529D4 echo \u0026#34;deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse\u0026#34; | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list sudo apt-get update sudo apt-get install -y mongodb-org After the installation is complete, enable it and verify its running correctly.\nsudo systemctl daemon-reload sudo systemctl enable mongod.service sudo systemctl restart mongod.service sudo systemctl --type=service --state=active | grep mongod Install Elasticsearch Graylog uses Elasticsearch 7 to manage the logs. You can install it via the following commands.\nwget -q https://artifacts.elastic.co/GPG-KEY-elasticsearch -O myKey sudo apt-key add myKey echo \u0026#34;deb https://artifacts.elastic.co/packages/oss-7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install elasticsearch-oss Then make the following changes to the to the configuration file.\n$ sudo tee -a /etc/elasticsearch/elasticsearch.yml \u0026gt; /dev/null \u0026lt;\u0026lt;EOT cluster.name: graylog action.auto_create_index: false EOT Install Graylog Finally, we can add the Graylog repository and install it using these commands.\n$ wget https://packages.graylog2.org/repo/packages/graylog-4.0-repository_latest.deb $ sudo dpkg -i graylog-4.0-repository_latest.deb $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install graylog-server graylog-enterprise-plugins graylog-integrations-plugins graylog-enterprise-integrations-plugins Edit the configuration file There are a couple of changes we need to make to the Graylog configuration file before we can start it. Open the config file using your favorite text editor:\nsudo nano /etc/graylog/server/server.conf Here are the changes we need to make:\nFind the password_secret parameter. You can create a password for this section with this command: pwgen -N 1 -s 96 This parameter is used when creating a cluster for Graylog so make sure you save the password. root_password_sha2 has your desired password and save it here. Use this command to hash your password: echo -n yourpassword | shasum -a 256 Definitely save this password. You will need it to log into the control panel. I would suggest specifying the time zone as well with the root_timezone You can also choose the username for the panel with the root_username directive, but this is not required. The default username is admin If you want to have access to the control panel from other locations, you need to change the http_bind_address. Change it from 127.0.0.0 to the IP address of your server Start Graylog Finally, we’re ready to start the Graylog server. Run the following commands.\n$ sudo systemctl daemon-reload $ sudo systemctl enable graylog-server.service $ sudo systemctl start graylog-server.service $ sudo systemctl --type=service --state=active | grep graylog Server Firewall configuration There are at least two ports that need to be opened with this configuration. Depending on what protocol you use for transfer of logs , you might need to open the corresponding ports.\nOn Ubuntu with the UFW firewall, you can open ports very easily like this:\nufw allow 9000/tcp ufw allow 514/tcp Port 514 is the Rsyslog port which will be specified in the configuration. Port 9000 is the web interface for Graylog which we will get to later. If you’re using UDP for the transfer of the files, change the rules to UDP instead of TCP\nReload the firewall to apply the rules:\nufw reload Then check the configuration to make sure\n$ ufw status To Action From -- ------ ---- 9000/tcp ALLOW Anywhere 22/tcp ALLOW Anywhere 514/tcp ALLOW Anywhere Client configuration with Rsyslog Rsyslog is a protocol for managing logs on Linux servers, and its installed by default on most distributions including Ubuntu. Using Rsyslog, we can transfer all or some of logs of a server to another server like Graylog so we can perform analysis.\nHere’s how I configured a CentOS server to send all of its logs to the Graylog server we just configured.\nFirst, navigate to the Rsyslog configuration directory:\ncd /etc/rsyslog.d Create a new configuration file and open it.\nnano 01-client.conf and copy the content below inside the file:\n## GTLS driver $DefaultNetstreamDriver gtls # Certificates $DefaultNetstreamDriverCAFile /etc/ssl/rsyslog/CA.pem $DefaultNetstreamDriverCertFile /etc/ssl/rsyslog/client-cert.pem $DefaultNetstreamDriverKeyFile /etc/ssl/rsyslog/client-key.pem # Auth mode $ActionSendStreamDriverAuthMode x509/name $ActionSendStreamDriverPermittedPeer [server-hostname] # Only use TLS $ActionSendStreamDriverMode 1 # Forward everything to destination server *.* @@[server-IP]:514 The key setting here is the SSL keys. It is highly encouraged to use SSL to transfer the logs across the network. If you don’t use SSL, the log will transfer in a clear-text format and can be readable by anyone who has access to either nodes, or has access to your network in transit. (e.g your ISP).\nIssue two SSL certificates for your hostname. One for your server and one for client. For example server.yourdomain.com and client.yourdomain.com. If your server and client on part of the same domain, you could use a single Wild Card certificate for both.\nStart Rsyslog After your save the configuration file, make sure to install the rsyslog-gnutls package as well.\napt install rsyslog-gnutls Finally restart Rsyslog to apply the configuration.\nsystemctl restart rsyslog.service Now to make sure everything is running smoothly, check the status with this command. If there are any errors, you should see them.\njournalctl -f -u rsyslog Graylog web interface configuration Now that everything is setup and running smoothly (hopefully), we can begin receiving logs in the Graylog interface.\nNavigate to the web interface by opening the server IP via port 9000 in the URL bar of your browser:\nhttp://127.0.0.1:9000 If you’ve defined your server IP in the configuration, replace it with 127.0.0.1\nIf everything goes will you will be greeted with this page: Enter admin as the default username and the password you created in the configuration file under the root_password directive.\nAdding an Input with Rsyslog So far we’ve configured our client to send its log to the Graylog server, but we need to add that as an input in order to process it within Graylog.\nAfter logging in the Graylog interface, go to the System tab and click on Inputs.\nFrom the drop down menu, click \u0026ldquo;Syslog TCP\u0026rdquo; and then click the Launch \u0026ldquo;New Input\u0026rdquo; button.\nThe first field is your Graylog node. Since we only have a single Graylog server, you don’t need to select anything. In the Title filed just select a name for your input – for example Rsyslog. You can leave the IP on 0.0.0.0 sine we want to listen to Rsyslog inputs over the public IP. The default port for Rsyslog is 514 so leave this field unchanged. Important Note: In order to use port 514, you need to run Gray log as the root user which is not recommended. If Graylog isn’t running as root, port 514 won’t likely work. You can change it to something else for example 5514. However. make sure that you specify the new port in your client Rsyslog configuration file. Leave the rest of the fields at default setting. TLS cert file: The path to the certificate file for your server. TLS private key: The path to the private key. The TLS client Auth Trusted Certs: The path to the CA certificate. Make sure to include the root certificate in this file as well because Graylog can’t retrieve it from the web. The rest of the configuration is up to you but only these mentioned parameters are crucial and needed.\nYou’re not required to use *LINK SSL/TLS certificate, but if you don’t, all of your logs will be transferred in clear-text format which is far from ideal.\nAfter adding the input you should see this section with the input correctly running:\nNow navigate to the homepage of Graylog and you should see the logs coming in:\nConclusion So that’s all! We now have full graylog node running and recieving inputs from Rsyslog on the node. You can continue to add new servers. Just forward the logs using the same method to the Ryslog port from the client.\nBased on personal experience I’ve found Linux log managment with Graylog to be the best for my intentions and the easiest.\nThere is range of graphs and alert systems that you can create in Graylog which is well beyond the scope of this post. I suggest checking out the official docs to get familiar with it, or just play around with the settings!\n",
        "content_html": "\u003cp\u003eI’ve been searching the web for a free and open-source log monitoring solution for Linux for a while now. I’ve tried everything from Nagios to Kibana. However I found out the best solution for me is Graylog. I basically just want to manage web server and mail server logs and get alerts if certain conditions are met. So Let’s take a look at Linux log management with Graylog.\u003c/p\u003e\n\u003cp\u003eAlthough the official Graylog documentation is great, I’ve found out it has some shortcomings when it comes to explaining how to actually transfer the logs from your own servers to the intended master Graylog server, especially if you’ve never done this before (like me!). If you’re not familiar with Syslog for example, you’re gonna have a hard time configuration just by following the official docs. That’s why I wanted to document how I approached it which is very bare bones and simple.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"getting-started\"\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eSo here’s what we’re going to do. We’re planning to transfer all of our logs from a server to a Graylog node, using Rsyslog as the transfer protocol.\u003c/p\u003e\n\u003cp\u003eFor setting up a Graylog node we’re going to use a Ubuntu 20.04 virtual machine with 2 Gigabytes of ram and 2 CPU cores. Keep in mind that this is just for testing purposes. In a production environment, you’re going to need a much more powerful server, most likely dedicated.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"installing-graylog-on-ubuntu-2004\"\u003eInstalling Graylog on Ubuntu 20.04\u003c/h2\u003e\n\u003cp\u003eFirst we’re going to start by installing the prerequisite packages. Run the following commands:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade\nsudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you get a “package not found” error, you need to add the universe repositry as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo add-apt-repository universe\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade\nsudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"install-mongodb\"\u003eInstall MongoDB\u003c/h2\u003e\n\u003cp\u003eNext step is to install MongoDB via the following commands:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 9DA31620334BD75D9DCB49F368818C72E52529D4\necho \u0026#34;deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse\u0026#34; | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter the installation is complete, enable it and verify its running correctly.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo systemctl daemon-reload\nsudo systemctl enable mongod.service\nsudo systemctl restart mongod.service\nsudo systemctl --type=service --state=active | grep mongod\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"install-elasticsearch\"\u003eInstall Elasticsearch\u003c/h2\u003e\n\u003cp\u003eGraylog uses Elasticsearch 7 to manage the logs. You can install it via the following commands.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ewget -q https://artifacts.elastic.co/GPG-KEY-elasticsearch -O myKey\nsudo apt-key add myKey\necho \u0026#34;deb https://artifacts.elastic.co/packages/oss-7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install elasticsearch-oss\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen make the following changes to the to the configuration file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ sudo tee -a /etc/elasticsearch/elasticsearch.yml \u0026gt; /dev/null \u0026lt;\u0026lt;EOT\ncluster.name: graylog\naction.auto_create_index: false\nEOT\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"install-graylog\"\u003eInstall Graylog\u003c/h2\u003e\n\u003cp\u003eFinally, we can add the Graylog repository and install it using these commands.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ wget https://packages.graylog2.org/repo/packages/graylog-4.0-repository_latest.deb\n$ sudo dpkg -i graylog-4.0-repository_latest.deb\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install graylog-server graylog-enterprise-plugins graylog-integrations-plugins graylog-enterprise-integrations-plugins\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"edit-the-configuration-file\"\u003eEdit the configuration file\u003c/h2\u003e\n\u003cp\u003eThere are a couple of changes we need to make to the Graylog configuration file before we can start it. Open the config file using your favorite text editor:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo nano /etc/graylog/server/server.conf\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHere are the changes we need to make:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFind the password_secret parameter. You can create a password for this section with this command: pwgen -N 1 -s 96\n    This parameter is used when creating a cluster for Graylog so make sure you save the password.\u003c/li\u003e\n\u003cli\u003eroot_password_sha2 has your desired password and save it here. Use this command to hash your password:\n    echo -n yourpassword | shasum -a 256\n    Definitely save this password. You will need it to log into the control panel.\u003c/li\u003e\n\u003cli\u003eI would suggest specifying the time zone as well with the root_timezone\u003c/li\u003e\n\u003cli\u003eYou can also choose the username for the panel with the root_username directive, but this is not required. The default username is admin\u003c/li\u003e\n\u003cli\u003eIf you want to have access to the control panel from other locations, you need to change the http_bind_address. Change it from 127.0.0.0 to the IP address of your server\u003c/li\u003e\n\u003ch2 id=\"start-graylog\"\u003eStart Graylog\u003c/h2\u003e\n\u003cp\u003eFinally, we’re ready to start the Graylog server. Run the following commands.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ sudo systemctl daemon-reload\n$ sudo systemctl enable graylog-server.service\n$ sudo systemctl start graylog-server.service\n$ sudo systemctl --type=service --state=active | grep graylog\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"server-firewall-configuration\"\u003eServer Firewall configuration\u003c/h2\u003e\n\u003cp\u003eThere are at least two ports that need to be opened with this configuration. Depending on what protocol you use for transfer of logs , you might need to open the corresponding ports.\u003c/p\u003e\n\u003cp\u003eOn Ubuntu with the UFW firewall, you can open ports very easily like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eufw allow 9000/tcp\nufw allow 514/tcp\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePort 514 is the Rsyslog port which will be specified in the configuration. Port 9000 is the web interface for Graylog which we will get to later. If you’re using UDP for the transfer of the files, change the rules to UDP instead of TCP\u003c/p\u003e\n\u003cp\u003eReload the firewall to apply the rules:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eufw reload\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen check the configuration to make sure\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$ ufw status\n\nTo                         Action      From\n--                         ------      ----\n9000/tcp                   ALLOW       Anywhere                  \n22/tcp                     ALLOW       Anywhere                  \n514/tcp                    ALLOW       Anywhere                  \n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"client-configuration-with-rsyslog\"\u003eClient configuration with Rsyslog\u003c/h2\u003e\n\u003cp\u003eRsyslog is a protocol for managing logs on Linux servers, and its installed by default on most distributions including Ubuntu. Using Rsyslog, we can transfer all or some of logs of a server to another server like Graylog so we can perform analysis.\u003c/p\u003e\n\u003cp\u003eHere’s how I configured a CentOS server to send all of its logs to the Graylog server we just configured.\u003c/p\u003e\n\u003cp\u003eFirst, navigate to the Rsyslog configuration directory:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecd /etc/rsyslog.d\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eCreate a new configuration file and open it.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enano 01-client.conf\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eand copy the content below inside the file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e## GTLS driver\n$DefaultNetstreamDriver gtls\n# Certificates\n$DefaultNetstreamDriverCAFile /etc/ssl/rsyslog/CA.pem\n$DefaultNetstreamDriverCertFile /etc/ssl/rsyslog/client-cert.pem\n$DefaultNetstreamDriverKeyFile /etc/ssl/rsyslog/client-key.pem\n# Auth mode\n$ActionSendStreamDriverAuthMode x509/name\n$ActionSendStreamDriverPermittedPeer [server-hostname]\n# Only use TLS\n$ActionSendStreamDriverMode 1\n# Forward everything to destination server\n*.* @@[server-IP]:514\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe key setting here is the SSL keys. It is highly encouraged to use SSL to transfer the logs across the network. If you don’t use SSL, the log will transfer in a clear-text format and can be readable by anyone who has access to either nodes, or has access to your network in transit. (e.g your ISP).\u003c/p\u003e\n\u003cp\u003eIssue two SSL certificates for your hostname. One for your server and one for client. For example server.yourdomain.com and client.yourdomain.com. If your server and client on part of the same domain, you could use a single Wild Card certificate for both.\u003c/p\u003e\n\u003ch2 id=\"start-rsyslog\"\u003eStart Rsyslog\u003c/h2\u003e\n\u003cp\u003eAfter your save the configuration file, make sure to install the \u003ccode\u003ersyslog-gnutls\u003c/code\u003e package as well.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapt install rsyslog-gnutls\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFinally restart Rsyslog to apply the configuration.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl restart rsyslog.service\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow to make sure everything is running smoothly, check the status with this command. If there are any errors, you should see them.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ejournalctl -f -u rsyslog\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"graylog-web-interface-configuration\"\u003eGraylog web interface configuration\u003c/h2\u003e\n\u003cp\u003eNow that everything is setup and running smoothly (hopefully), we can begin receiving logs in the Graylog interface.\u003c/p\u003e\n\u003cp\u003eNavigate to the web interface by opening the server IP via port 9000 in the URL bar of your browser:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehttp://127.0.0.1:9000\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you’ve defined your server IP in the configuration, replace it with 127.0.0.1\u003c/p\u003e\n\u003cp\u003eIf everything goes will you will be greeted with this page:\n\u003cimg\n  src=\"https://workingtitle.pro/images/graylog-web-interface.png\"\n  alt=\"graylog-web-interface\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eEnter admin as the default username and the password you created in the configuration file under the root_password directive.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"adding-an-input-with-rsyslog\"\u003eAdding an Input with Rsyslog\u003c/h2\u003e\n\u003cp\u003eSo far we’ve configured our client to send its log to the Graylog server, but we need to add that as an input in order to process it within Graylog.\u003c/p\u003e\n\u003cp\u003eAfter logging in the Graylog interface, go to the System tab and click on Inputs.\u003c/p\u003e\n\u003cp\u003eFrom the drop down menu, click \u0026ldquo;Syslog TCP\u0026rdquo; and then click the Launch \u0026ldquo;New Input\u0026rdquo; button.\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/graylog-new-input.png\"\n  alt=\"graylog-new-input\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe first field is your Graylog node. Since we only have a single Graylog server, you don’t need to select anything.\u003c/li\u003e\n\u003cli\u003eIn the Title filed just select a name for your input – for example Rsyslog.\u003c/li\u003e\n\u003cli\u003eYou can leave the IP on 0.0.0.0 sine we want to listen to Rsyslog inputs over the public IP.\u003c/li\u003e\n\u003cli\u003eThe default port for Rsyslog is 514 so leave this field unchanged.\u003c/li\u003e\n\u003cli\u003eImportant Note: In order to use port 514, you need to run Gray log as the root user which is not recommended. If Graylog isn’t running as root, port 514 won’t likely work. You can change it to something else for example 5514. However. make sure that you specify the new port in your client Rsyslog configuration file.\u003c/li\u003e\n\u003cli\u003eLeave the rest of the fields at default setting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/graylog-new-input-part2.png\"\n  alt=\"graylog-new-input-part2\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTLS cert file: The path to the certificate file for your server.\u003c/li\u003e\n\u003cli\u003eTLS private key: The path to the private key.\u003c/li\u003e\n\u003cli\u003eThe TLS client Auth Trusted Certs: The path to the CA certificate. Make sure to include the root certificate in this file as well because Graylog can’t retrieve it from the web.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe rest of the configuration is up to you but only these mentioned parameters are crucial and needed.\u003c/p\u003e\n\u003cp\u003eYou’re not required to use *\u003cstrong\u003e\u003cstrong\u003eLINK\u003c/strong\u003e\u003c/strong\u003e SSL/TLS certificate, but if you don’t, all of your logs will be transferred in clear-text format which is far from ideal.\u003c/p\u003e\n\u003cp\u003eAfter adding the input you should see this section with the input correctly running:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/graylog-local-input.png\"\n  alt=\"graylog-local-input\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cp\u003eNow navigate to the homepage of Graylog and you should see the logs coming in:\u003c/p\u003e\n\u003cp\u003e\u003cimg\n  src=\"https://workingtitle.pro/images/graylog-dashboard.png\"\n  alt=\"graylog-dashboard\"\n  loading=\"lazy\"\n  decoding=\"async\"\n  class=\"full-width\"\n/\u003e\n\n\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eSo that’s all! We now have full graylog node running and recieving inputs from Rsyslog on the node. You can continue to add new servers. Just forward the logs using the same method to the Ryslog port from the client.\u003c/p\u003e\n\u003cp\u003eBased on personal experience I’ve found Linux log managment with Graylog to be the best for my intentions and the easiest.\u003c/p\u003e\n\u003cp\u003eThere is range of graphs and alert systems that you can create in Graylog which is well beyond the scope of this post. I suggest checking out the \u003ca href=\"https://docs.graylog.org/en/4.0/\"\u003eofficial docs\u003c/a\u003e to get familiar with it, or just play around with the settings!\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/linux-log-management-with-graylog/",
        "date_published": "19056-19-09T50:1919:00+00:00",
        "date_modified": "19056-19-09T50:1919:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "6c27b90803fb5b4e9c09e6a0d573b1402a874b26",
        "title": "Homelab: LXC/LXD Hypervisor and using Linux Containers",
        "summary": "",
        "content_text": "I’m planning to set up an entire network on a home server for testing and practice purposes, using Linux containers. I’m looking to setup the follow services (more might be added in the future):\nLXC/LXD host machine Cobbler server (for automatic VM creation) or maybe Proxmox DHCP DNS FTP and file storage NFS share Postgres database server Apache web server NGINX load balancing LDAP server for authentication and access management at least 10 clients running various Linux distributions Mail server using Exim (or Postfix) Nagios server for monitoring Syslog server (kibana and logstash) Setting up LCX/LXD machine The first thing we need to do for setting up this network is to have a host machine for our containers. For all the services I mentioned in the last part we will be using Linux containers instead of full virtual machines. Because this network is only for testing and practice purposes, we don’t need large resource. I’m setting up this network on a host machine with only 8 GB of ram and a modest Intel core i5 CPU. Nothing crazy.\nUse this guide to setup and install LXD. It’s recommended to use Ubuntu or Debian as your host server in this case but you could technically use other distributions as well, but in this case since we’re going to be using Proxmox as well, we’re going with the recommended Ubuntu option.\nCreating a new LXD container Using Linux containers is different from virtual machines because they can share resources among them such as kernel to reduce load on the host machine. (Check out KSM). This is great for our use case because we will be spinning up a lot of containers so we can run all the services on a modest host machine.\nThe full guide on how to spin up and use LXD containers are in the official guide, but generally you can start up a container with a command like this:\nlxc launch ubuntu:20.04 TestServer ubuntu is the operating system we want and 20.04 is the version of the server.\nIf you want to see all the images available, you can run this command:\nlxc image list images: This command will show all the images available on official repository. Once you setup a new container, the image will be downloaded and stored locally.\nYou can search for all images of a specific distribution like so:\nlxc image list images: debian Most of the useful commands for LXD/LXC can be found here. Play around with different commands, create new containers, connect to them, start and stop them to get the hang of it.\nIn the next post, we’re going to set up a Proxmox service to control these containers in a easier fashion, but it’s important to familiarize yourself with how they work at a command-line level.\n",
        "content_html": "\u003cp\u003eI’m planning to set up an entire network on a home server for testing and practice purposes, using Linux containers. I’m looking to setup the follow services (more might be added in the future):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLXC/LXD host machine\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cobbler.github.io/\"\u003eCobbler\u003c/a\u003e server (for automatic VM creation) or maybe \u003ca href=\"https://www.proxmox.com/en/\"\u003eProxmox\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDHCP\u003c/li\u003e\n\u003cli\u003eDNS\u003c/li\u003e\n\u003cli\u003eFTP and file storage\u003c/li\u003e\n\u003cli\u003eNFS share\u003c/li\u003e\n\u003cli\u003ePostgres database server\u003c/li\u003e\n\u003cli\u003eApache web server\u003c/li\u003e\n\u003cli\u003eNGINX load balancing\u003c/li\u003e\n\u003cli\u003eLDAP server for authentication and access management\u003c/li\u003e\n\u003cli\u003eat least 10 clients running various Linux distributions\u003c/li\u003e\n\u003cli\u003eMail server using \u003ca href=\"https://www.exim.org/\"\u003eExim\u003c/a\u003e (or \u003ca href=\"http://www.postfix.org/\"\u003ePostfix\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nagios.org/\"\u003eNagios\u003c/a\u003e server for monitoring\u003c/li\u003e\n\u003cli\u003eSyslog server (\u003ca href=\"https://www.elastic.co/kibana/\"\u003ekibana\u003c/a\u003e and \u003ca href=\"https://www.elastic.co/logstash/\"\u003elogstash\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"setting-up-lcxlxd-machine\"\u003eSetting up LCX/LXD machine\u003c/h2\u003e\n\u003cp\u003eThe first thing we need to do for setting up this network is to have a host machine for our containers. For all the services I mentioned in the last part we will be using Linux containers instead of full virtual machines. Because this network is only for testing and practice purposes, we don’t need large resource. I’m setting up this network on a host machine with only 8 GB of ram and a modest Intel core i5 CPU. Nothing crazy.\u003c/p\u003e\n\u003cp\u003eUse \u003ca href=\"https://linuxcontainers.org/lxd/getting-started-cli/\"\u003ethis\u003c/a\u003e guide to setup and install LXD. It’s recommended to use Ubuntu or Debian as your host server in this case but you could technically use other distributions as well, but in this case since we’re going to be using Proxmox as well, we’re going with the recommended Ubuntu option.\u003c/p\u003e\n\u003ch2 id=\"creating-a-new-lxd-container\"\u003eCreating a new LXD container\u003c/h2\u003e\n\u003cp\u003eUsing Linux containers is different from virtual machines because they can share resources among them such as kernel to reduce load on the host machine. (Check out \u003ca href=\"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-ksm\"\u003eKSM\u003c/a\u003e). This is great for our use case because we will be spinning up a lot of containers so we can run all the services on a modest host machine.\u003c/p\u003e\n\u003cp\u003eThe full guide on how to spin up and use LXD containers are in the official guide, but generally you can start up a container with a command like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elxc launch ubuntu:20.04 TestServer\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eubuntu is the operating system we want and 20.04 is the version of the server.\u003c/p\u003e\n\u003cp\u003eIf you want to see all the images available, you can run this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elxc image list images:\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis command will show all the images available on official repository. Once you setup a new container, the image will be downloaded and stored locally.\u003c/p\u003e\n\u003cp\u003eYou can search for all images of a specific distribution like so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elxc image list images: debian\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eMost of the useful commands for LXD/LXC can be found here.\nPlay around with different commands, create new containers, connect to them, start and stop them to get the hang of it.\u003c/p\u003e\n\u003cp\u003eIn the next post, we’re going to set up a Proxmox service to control these containers in a easier fashion, but it’s important to familiarize yourself with how they work at a command-line level.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/homelab-lxc-lxd-hypervisor-and-using-linux-containers/",
        "date_published": "3016-03-09T10:33:00+00:00",
        "date_modified": "3016-03-09T10:33:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "5db4d4020eaf4b7baa57e5259e301e66778f6843",
        "title": "Personal Cloud Storage Solution With Nextcloud",
        "summary": "",
        "content_text": "Ever wanted to have your own personal cloud storage space? I did. After I noticed my (free) Google Drive space running out.\nGoogling “Personal cloud” brings up bunch of results, such as ownCloud that I tried to get started with but I found it unnecessarily complicated.\nThe next best choice for a personal cloud storage is NextCloud, which is a completely free and open-source solution for setting up your own cloud storage server.\nHere’s what you need to setup NextCloud as a personal cloud storage:\nA Linux VPS, preferably running on Ubuntu 18.04 or Redhat 8. For this example I’m using CentOS 7 which is also fine. A valid IP Optional: DNS Server so you could set it up as a sub directory for your domain. For example: cloud.workingtitle.pro would be the address of your server for the web interface. MySQL Apache PHP Getting started with a personal cloud storage Firstly download the installation file from NextCloud’s website. Link found here.\nThe first thing you need to do is to go ahead and update your server. You can do so via this command:\nsudo yum update Make sure that you have MySQL ready and have access for creating a database and giving permissions.\nYou should also have Apache installed. If you don’t, install it via this command:\nyum install httpd yum install httpd The next step is to install PHP on your server if you don’t already have it. To check if you have PHP installed or not run the following command:\nphp -v If PHP is installed, you’ll get output showing the version, if its not your server will be confused.\nNextCloud recommends PHP version 7.3 or 7.4. So if don’t have it, go ahead and install using the following commands.\nFirst add the EPEL repository to your server:\nyum install epel-release yum-utils -y yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm Now add PHP 7.4 to your repository:\nyum-config-manager --enable remi-php74 And finally install PHP and some of its core modules:\nyum install php php-common php-opcache php-mcrypt php-cli php-gd php-curl php-mysql -y After this PHP should be setup and ready to go.\nNow NextCloud requires bunch of different modules. The full list can be found here:\nPHP module ctype PHP module curl PHP module dom PHP module GD PHP module hash (only on FreeBSD) PHP module iconv PHP module JSON PHP module libxml (Linux package libxml2 must be \u0026gt;=2.7.0) PHP module mbstring PHP module openssl PHP module posix PHP module session PHP module SimpleXML PHP module XMLReader PHP module XMLWriter PHP module zip PHP module zlib PHP module memcached PHP module imagick All of these should be included in the source tar.gz file that we install later, but that wasn’t the case for me and I had to install some of these manually.\nApache configuration Create a file named nextcloud.conf under the directory /etc/httpd/conf.d/ and add the following lines to the file:\n\u0026lt;VirtualHost *:80\u0026gt; DocumentRoot /var/www/nextcloud/ ServerName cloud.workingtitle.pro \u0026lt;Directory /var/www/nextcloud/\u0026gt; Require all granted AllowOverride All Options FollowSymLinks MultiViews \u0026lt;IfModule mod_dav.c\u0026gt; Dav off \u0026lt;/IfModule\u0026gt; \u0026lt;/Directory\u0026gt; DocumentRoot is the path fo the NextCloud files and it’s up to you where you want to place it. Just create a directory of your choice and the download the installation files and extract them within that diectory. Then specify the path within this nextcloud.conf file.\nYou should also change the ServerName section to the host name you want to set for your server. This is where the DNS server part I talked about comes into play. If you don’t know how to setup a DNS server, you can check out my previous post.\nInstalling SSL The next step is to setup SSL for your domain. If you don’t know how, you can check my other guide about setting up SSL on NGINX.\nAn easy and fast way is to use Certbot. It helps you to setup a Let’s Encrypt SSL on your server that renews itself automatically. It’s very easy to install as it’s only a few clicks. It’s 2020, if you don’t have SSL then you should reevaluate your life as a system admin 🙂\nInstallation Wizard Now we’re getting to the final stages. After you have configured Apache and installed SSL, restart it to make sure everything is okay.\nsystemctl restart httpd Open the URL in your browser to continue with the web installation.\nhttp://localhost/nextcloud Type in the Administrator username and password of your choice. Then click on storage and database option to enter the database info.\nNextClould recommends to user MySQL as the database. So go on your server and and create a new database and user name and enter the information in the fields.\nUsing SQLite is not recommended as it’s only intended for testing and educational purposes and not be used in a production environment.\nDefining trusted domains By default only the domain name that’s entered in the Apache configuration will be allowed to access NextCloud. If you want to add another domain or subdomain you will have to manually add it to the trusted domains list.\nOpen the config.php file. The syntax for the trusted domains is like this:\n\u0026#39;trusted_domains\u0026#39; =\u0026gt; array ( 0 =\u0026gt; \u0026#39;localhost\u0026#39;, 1 =\u0026gt; \u0026#39;cloud.workingtitle.pro\u0026#39;, 2 =\u0026gt; \u0026#39;newDomain.workingtitle.pro\u0026#39;, 3 =\u0026gt; \u0026#39;192.168.1.50\u0026#39;, ), With this configuration you can access Nextclould via localhost address, as well as the hostnames and the IP 192.168.1.50. You can add and subtract from this list as you wish.\nWe’re done now. Open up the URL in your browser and you should see the login page.\nFor further information regarding user creation and file sharing management, I suggest checking out the Official documentation.\n",
        "content_html": "\u003cp\u003eEver wanted to have your own personal cloud storage space? I did. After I noticed my (free) Google Drive space running out.\u003c/p\u003e\n\u003cp\u003eGoogling “Personal cloud” brings up bunch of results, such as ownCloud that I tried to get started with but I found it unnecessarily complicated.\u003c/p\u003e\n\u003cp\u003eThe next best choice for a personal cloud storage is NextCloud, which is a completely free and open-source solution for setting up your own cloud storage server.\u003c/p\u003e\n\u003cp\u003eHere’s what you need to setup NextCloud as a personal cloud storage:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA Linux VPS, preferably running on Ubuntu 18.04 or Redhat 8. For this example I’m using CentOS 7 which is also fine.\u003c/li\u003e\n\u003cli\u003eA valid IP\u003c/li\u003e\n\u003cli\u003eOptional: DNS Server so you could set it up as a sub directory for your domain. For example: cloud.workingtitle.pro would be the address of your \u003cli\u003eserver for the web interface.\u003c/li\u003e\n\u003cli\u003eMySQL\u003c/li\u003e\n\u003cli\u003eApache\u003c/li\u003e\n\u003cli\u003ePHP\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"getting-started-with-a-personal-cloud-storage\"\u003eGetting started with a personal cloud storage\u003c/h2\u003e\n\u003cp\u003eFirstly download the installation file from NextCloud’s website. Link found here.\u003c/p\u003e\n\u003cp\u003eThe first thing you need to do is to go ahead and update your server. You can do so via this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo yum update\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eMake sure that you have MySQL ready and have access for creating a database and giving permissions.\u003c/p\u003e\n\u003cp\u003eYou should also have Apache installed. If you don’t, install it via this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum install httpd\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"yum-install-httpd\"\u003eyum install httpd\u003c/h2\u003e\n\u003cp\u003eThe next step is to install PHP on your server if you don’t already have it. To check if you have PHP installed or not run the following command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ephp -v\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf PHP is installed, you’ll get output showing the version, if its not your server will be confused.\u003c/p\u003e\n\u003cp\u003eNextCloud recommends PHP version 7.3 or 7.4. So if don’t have it, go ahead and install using the following commands.\u003c/p\u003e\n\u003cp\u003eFirst add the EPEL repository to your server:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum install epel-release yum-utils -y\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow add PHP 7.4 to your repository:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum-config-manager --enable remi-php74\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd finally install PHP and some of its core modules:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum install php php-common php-opcache php-mcrypt php-cli php-gd php-curl php-mysql -y\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter this PHP should be setup and ready to go.\u003c/p\u003e\n\u003cp\u003eNow NextCloud requires bunch of different modules. The full list can be found here:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ePHP module ctype\nPHP module curl\nPHP module dom\nPHP module GD\nPHP module hash (only on FreeBSD)\nPHP module iconv\nPHP module JSON\nPHP module libxml (Linux package libxml2 must be \u0026gt;=2.7.0)\nPHP module mbstring\nPHP module openssl\nPHP module posix\nPHP module session\nPHP module SimpleXML\nPHP module XMLReader\nPHP module XMLWriter\nPHP module zip\nPHP module zlib\nPHP module memcached\nPHP module imagick\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAll of these should be included in the source tar.gz file that we install later, but that wasn’t the case for me and I had to install some of these manually.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"apache-configuration\"\u003eApache configuration\u003c/h2\u003e\n\u003cp\u003eCreate a file named \u003ccode\u003enextcloud.conf\u003c/code\u003e under the directory \u003ccode\u003e/etc/httpd/conf.d/\u003c/code\u003e and add the following lines to the file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;VirtualHost *:80\u0026gt;\n  DocumentRoot /var/www/nextcloud/\n  ServerName  cloud.workingtitle.pro\n\n  \u0026lt;Directory /var/www/nextcloud/\u0026gt;\n    Require all granted\n    AllowOverride All\n    Options FollowSymLinks MultiViews\n\n    \u0026lt;IfModule mod_dav.c\u0026gt;\n      Dav off\n    \u0026lt;/IfModule\u0026gt;\n\n  \u0026lt;/Directory\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003eDocumentRoot\u003c/code\u003e is the path fo the NextCloud files and it’s up to you where you want to place it. Just create a directory of your choice and the download the installation files and extract them within that diectory. Then specify the path within this \u003ccode\u003enextcloud.conf\u003c/code\u003e file.\u003c/p\u003e\n\u003cp\u003eYou should also change the \u003ccode\u003eServerName\u003c/code\u003e section to the host name you want to set for your server. This is where the DNS server part I talked about comes into play. If you don’t know how to setup a DNS server, you can check out \u003cb\u003emy previous post\u003c/b\u003e.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"installing-ssl\"\u003eInstalling SSL\u003c/h2\u003e\n\u003cp\u003eThe next step is to setup SSL for your domain. If you don’t know how, you can check my other guide about \u003cb\u003esetting up SSL on NGINX\u003c/b\u003e.\u003c/p\u003e\n\u003cp\u003eAn easy and fast way is to use \u003ca href=\"https://certbot.eff.org/\"\u003eCertbot\u003c/a\u003e. It helps you to setup a Let’s Encrypt SSL on your server that renews itself automatically. It’s very easy to install as it’s only a few clicks. It’s 2020, if you don’t have SSL then you should reevaluate your life as a system admin 🙂\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"installation-wizard\"\u003eInstallation Wizard\u003c/h2\u003e\n\u003cp\u003eNow we’re getting to the final stages. After you have configured Apache and installed SSL, restart it to make sure everything is okay.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl restart httpd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOpen the URL in your browser to continue with the web installation.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehttp://localhost/nextcloud\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eType in the Administrator username and password of your choice. Then click on storage and database option to enter the database info.\u003c/p\u003e\n\u003cp\u003eNextClould recommends to user MySQL as the database. So go on your server and and create a new database and user name and enter the information in the fields.\u003c/p\u003e\n\u003cp\u003eUsing SQLite is not recommended as it’s only intended for testing and educational purposes and not be used in a production environment.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"defining-trusted-domains\"\u003eDefining trusted domains\u003c/h2\u003e\n\u003cp\u003eBy default only the domain name that’s entered in the Apache configuration will be allowed to access NextCloud. If you want to add another domain or subdomain you will have to manually add it to the trusted domains list.\u003c/p\u003e\n\u003cp\u003eOpen the config.php file. The syntax for the trusted domains is like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026#39;trusted_domains\u0026#39; =\u0026gt;\n  array (\n   0 =\u0026gt; \u0026#39;localhost\u0026#39;,\n   1 =\u0026gt; \u0026#39;cloud.workingtitle.pro\u0026#39;,\n   2 =\u0026gt; \u0026#39;newDomain.workingtitle.pro\u0026#39;,\n   3 =\u0026gt; \u0026#39;192.168.1.50\u0026#39;,\n),\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWith this configuration you can access Nextclould via localhost address, as well as the hostnames and the IP 192.168.1.50. You can add and subtract from this list as you wish.\u003c/p\u003e\n\u003cp\u003eWe’re done now. Open up the URL in your browser and you should see the login page.\u003c/p\u003e\n\u003cp\u003eFor further information regarding user creation and file sharing management, I suggest checking out the \u003ca href=\"https://docs.nextcloud.com/server/20/admin_manual/index.html\"\u003eOfficial documentation\u003c/a\u003e.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/personal-cloud-storage-solution-with-nextcloud/",
        "date_published": "10106-10-09T100:1010:00+00:00",
        "date_modified": "10106-10-09T100:1010:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "c57d5c1fbaf97f51e27743ca8a27eb1259f68368",
        "title": "Simple squid proxy server with basic authentication",
        "summary": "",
        "content_text": "In previous posts I talked about setting up a double-proxy server using Squid. In this guide I’m gonna walk you through setting up a simple proxy server using Squid, and apply a simple authentication method.\nThe official Squid documentation on this issue is very vague and all over the place and I couldn’t find a good and straightforward guide for it.\nSo here we go.\nWhat you will need A Linux server (VPS). In this example I’m using a CentOS 7 machine but the steps should generally be the same on different distributions. A valid IP address Installing Squid The first step is to install Squid on your machine. Use the following command on CentOS.\nFirst update your repositories via this command:\nsudo yum -y update Then install squid:\nyum -y install squid Start squid and enable it for system startup:\ncsystemctl start squid systemctl enable squid Squid configuration Create a User for Squid The first thing you’ll need to do is to set up a username and password for connecting to the proxy server. The username information for Squid is stored in this file:\nnano /etc/squid/passwd Create a new user with this command:\nsudo htpasswd /etc/squid/passwd [username-here] After entering the command a prompt shows up for defining new password. Enter your password and make sure to save this password because you will need it.\nEdit the configuration file Now we need to make the changes to the main squid configuration file. Open the file with your favorite text editor:\nnano /etc/squid/squid.conf Leave the default configuration in place (you might need them later) but add the following lines to the beginning of the file:\nauth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd auth_param basic children 5 auth_param basic realm Squid Basic Authentication auth_param basic credentialsttl 2 hours acl auth_users proxy_auth REQUIRED http_access allow auth_users /usr/lib64/squid/basic_ncsa_auth is the library we’re using for authentication and the accepted user list is found at /etc/squid/passwd which we defined a user for early on.\nNext add the following line to configuration:\nacl localnet src 0.0.0.0/8 This tells Squid to accept connections from any IP. (After authentication). I added this because I want to be able to connect to the server from any location.\nAnother important section is the port configuration. In my case, Squid only seemed to be listening on IPv6 which was not ideal. So in order to change it I had to edit this section:\nhttp_port 3128 In order to let Squid know you want it to listen on IPv4 add the IP 0.0.0.0 in front of it. As so:\nhttp_port 0.0.0.0:3128 After this, save the configuration file and restart Squid:\nservice squid restart Firewall configuration If you have firewall on your server, you’ll have to open the port 3128.\nUse the following command on CentOS:\nsudo firewall-cmd --zone=external --permanent --add-port=3128/tcp sudo firewall-cmd --zone=external --permanent --add-port=3128/udp sudo firewall-cmd --reload If you’re not using firewall-cmd, you’ll have to open the port using iptables.\nsudo iptables -A INPUT -p tcp --dport 3128 -j ACCEPT sudo iptables -A INPUT -p udp --dport 3128 -j ACCEPT service iptables save And that’s all. Everything is set now. Restart Squid and you should be good to go.\nservice squid restart If you’re having trouble connecting, make sure to check the Squid logs at /var/log/squid/\nIf you have any questions you can leave a comment on this post or email me at admin@workingtitle.pro\n",
        "content_html": "\u003cp\u003eIn previous posts I talked about setting up a double-proxy server using Squid. In this guide I’m gonna walk you through setting up a simple proxy server using Squid, and apply a simple authentication method.\u003c/p\u003e\n\u003cp\u003eThe official \u003ca href=\"http://www.squid-cache.org/\"\u003eSquid\u003c/a\u003e documentation on this issue is very vague and all over the place and I couldn’t find a good and straightforward guide for it.\u003c/p\u003e\n\u003cp\u003eSo here we go.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"what-you-will-need\"\u003eWhat you will need\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA Linux server (VPS). In this example I’m using a CentOS 7 machine but the steps should generally be the same on different distributions.\u003c/li\u003e\n\u003cli\u003eA valid IP address\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"installing-squid\"\u003eInstalling Squid\u003c/h2\u003e\n\u003cp\u003eThe first step is to install Squid on your machine. Use the following command on CentOS.\u003c/p\u003e\n\u003cp\u003eFirst update your repositories via this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo yum -y update\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen install squid:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum -y install squid\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eStart squid and enable it for system startup:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecsystemctl start squid\nsystemctl enable squid\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"squid-configuration\"\u003eSquid configuration\u003c/h2\u003e\n\u003ch3 id=\"create-a-user-for-squid\"\u003eCreate a User for Squid\u003c/h3\u003e\n\u003cp\u003eThe first thing you’ll need to do is to set up a username and password for connecting to the proxy server. The username information for Squid is stored in this file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enano /etc/squid/passwd\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eCreate a new user with this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo htpasswd /etc/squid/passwd [username-here]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter entering the command a prompt shows up for defining new password. Enter your password and make sure to save this password because you will need it.\u003c/p\u003e\n\u003ch3 id=\"edit-the-configuration-file\"\u003eEdit the configuration file\u003c/h3\u003e\n\u003cp\u003eNow we need to make the changes to the main squid configuration file. Open the file with your favorite text editor:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enano /etc/squid/squid.conf\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLeave the default configuration in place (you might need them later) but add the following lines to the beginning of the file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eauth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd\nauth_param basic children 5\nauth_param basic realm Squid Basic Authentication\nauth_param basic credentialsttl 2 hours\nacl auth_users proxy_auth REQUIRED\nhttp_access allow auth_users\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003e/usr/lib64/squid/basic_ncsa_auth\u003c/code\u003e is the library we’re using for authentication and the accepted user list is found at \u003ccode\u003e/etc/squid/passwd\u003c/code\u003e which we defined a user for early on.\u003c/p\u003e\n\u003cp\u003eNext add the following line to configuration:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eacl localnet src 0.0.0.0/8\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis tells Squid to accept connections from any IP. (After authentication). I added this because I want to be able to connect to the server from any location.\u003c/p\u003e\n\u003cp\u003eAnother important section is the port configuration. In my case, Squid only seemed to be listening on IPv6 which was not ideal. So in order to change it I had to edit this section:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehttp_port 3128\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn order to let Squid know you want it to listen on IPv4 add the IP 0.0.0.0 in front of it. As so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehttp_port 0.0.0.0:3128\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter this, save the configuration file and restart Squid:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice squid restart\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"firewall-configuration\"\u003eFirewall configuration\u003c/h2\u003e\n\u003cp\u003eIf you have firewall on your server, you’ll have to open the port 3128.\u003c/p\u003e\n\u003cp\u003eUse the following command on CentOS:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo firewall-cmd --zone=external --permanent --add-port=3128/tcp\nsudo firewall-cmd --zone=external --permanent --add-port=3128/udp\nsudo firewall-cmd --reload\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you’re not using firewall-cmd, you’ll have to open the port using \u003ccode\u003eiptables\u003c/code\u003e.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo iptables -A INPUT -p tcp --dport 3128 -j ACCEPT\nsudo iptables -A INPUT -p udp --dport 3128 -j ACCEPT\nservice iptables save\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd that’s all. Everything is set now. Restart Squid and you should be good to go.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice squid restart\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you’re having trouble connecting, make sure to check the Squid logs at \u003ccode\u003e/var/log/squid/\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIf you have any questions you can leave a comment on this post or email me at \u003ca href=\"mailto:admin@workingtitle.pro\"\u003eadmin@workingtitle.pro\u003c/a\u003e\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/simple-squid-proxy-server-with-basic-authentication/",
        "date_published": "1086-01-09T80:11:00+00:00",
        "date_modified": "1086-01-09T80:11:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "ae177fb38db4ef24e1b415bb12285992f24769de",
        "title": "Setting Up a DNS Server on CentOS From Scratch",
        "summary": "",
        "content_text": "Setting up a DNS server with BIND is not a very difficult process, but most of the guides and walk-through I’ve read on the subject tend to make it more complicated than it has to be, or that their information is very outdated. The results are a group of confused system admins who get stuck in an endless loop of troubleshooting their damn DNS server!\nSo my goal here is to help you set up your DNS server and get it running with minimal hassle.\nWhat you will need to start A freshly-installed Linux server ready to go. In this example, I’m going to be using CentOS 7 A valid public IP address A registered domain name, e.g workingtitle.pro. If you don’t already have a domain name, you can register one through GoDaddy or Namecheap. Domain settings The first thing you’ll need to do is to create DNS records in your domain control panel. Login into your domain registry panel and create the nameserver records as so:\nns1.yourdomain.com IP: [your-server-ip] ns2.yourdomain.com IP: [your-server-ip] Installing BIND and initial configuration The first step is to install BIND, which is the software that controls our DNS server. You can do so via this command:\nyum -y install bind bind-utils After the installation process is done, we’ll need to make our changes to the BIND configuration file in order to function correctly. In CentOS BIND is managed by a process called “named“. Open the configuration file with the text editor of your choice:\nvim /etc/named.conf The first section we need to change is this part:\nlisten-on port 53 { 127.0.0.1; }; listen-on-v6 port 53 { ::1; }; However as it stands, the nameserver is only responding to requests from the server itself. (127.0.0.1). Therefore, if any computer on the internet tries to reach this server it will be rejected. We will need to change the value to any as so:\nlisten-on port 53 { any; }; listen-on-v6 port 53 { any; }; Next, we need to allow the nameserver to accept query requests from any IP and to reject recursion. If Recursive DNS is active on a server, it will try to respond to DNS queries for domains that are NOT within your namesever. So technically, someone could ask your server for the IP of www.google.com. Obviously your server is not the authority for that, so it’s standard practice to disable recursion. Make these changes as so:\nallow-query { any; }; recursion no; In the next step we need to define zones for your domain. In the end of the /etc/named.conf file, add the following lines:\nzone \u0026#34;workingtitle.pro\u0026#34; IN { type master; file \u0026#34;/var/named/workingtitle.pro.db\u0026#34;; }; Replace “workingtitle.pro” with your own domain name. This section tells the users that the zone for the domain “workingtitle.pro” exists on this sever and the details can be found at /var/named/workingtitle.pro.db.\nThat’s all the changes we need to make to the configuration file. The rest of the settings are optional and you can change them based on your need.\nCreating Zone files The next step is to create zone files for your domains. The zone files contain the DNS records, showing where each resource is located.\nCreate the zone file for the domain by editing this file:\nvim /var/named/workingtitle.pro.db This is the same directory we defined in the previous step. Copy and paste the content blew into the file.\n$TTL 14400 workingtitle.pro. IN SOA ns1.workingtitle.pro. hostmaster.workingtitle.pro. ( 1001 ;Serial 4H ;Refresh 15M ;Retry 1W ;Expire 1D ;Minimum TTL ) ;Name Server Information workingtitle.pro.\t14400\tIN\tNS ns1.workingtitle.pro. workingtitle.pro.\t14400\tIN NS ns2.workingtitle.pro. ;IP address of Name Server ns1\tIN\tA 8.8.8.8\t;replace with your server public IP address ns2\tIN\tA\t8.8.8.8\t;replace with your server public IP address ;Mail exchanger workingtitle.pro.\tIN MX 10\tmail.workingtitle.pro. ;A - Record HostName To IP Address workingtitle.pro. IN A 8.8.8.8 ;replace with your server public IP address www\tIN\tA\t8.8.8.8\t;replace with your server public IP address mail\tIN\tA\t8.8.8.8\t;replace with your server public IP address ;CNAME record ftp IN\tCNAME workingtitle.pro. Let’s take a look at each section individually:\n;Name Server Information workingtitle.pro.\t14400\tIN\tNS ns1.workingtitle.pro. workingtitle.pro.\t14400\tIN NS ns2.workingtitle.pro. Here we’re defining our nameserver address so We’re essentially telling anyone who is querying our server that the nameservers for domain “workingtitle.pro” are “ns1.workingtitle.pro” and “ns2.workingtitle.pro”. You should obviously change this to your own domain name.\n;IP address of Name Server ns1\tIN\tA 8.8.8.8\t;replace with your server public IP address ns2\tIN\tA\t8.8.8.8\t;replace with your server public IP address Now we need to define the IP address of our nameservers. Here you can see that the IP for “ns1” and “ns2” has been defined. Replace 8.8.8.8 with your server public IP.\nImportant note: It is strongly advised to use at least two different servers with different IPs for your nameservers. This is to provide redundancy in case one of the nameservers fails, but in the spirit of keeping things simple, I’ve only used one server with a single IP.\n;A - Record HostName To IP Address workingtitle.pro. IN A 8.8.8.8 ;replace with your server public IP address www\tIN\tA\t8.8.8.8\t;replace with your server public IP address mail\tIN\tA\t8.8.8.8\t;replace with your server public IP address ;CNAME record ftp IN\tCNAME workingtitle.pro. In this section we can set the main DNS records such as the A record for the domain, the IP of the mail server and other services.\nFirewall configuration We need to open port 53 on the server firewall in order to allow DNS traffic through. You can use the following command:\nfirewall-cmd --zone=external --permanent --add-port=53/udp firewall-cmd --reload If you’re not using firewalld, you can use the following command to open the port with iptables.\niptables -A INPUT -p udp --dport 53 -j ACCEPT Start the DNS service The configuration is done! We can go ahead and start the DNS service:\nsystemctl enable named systemctl start named ",
        "content_html": "\u003cp\u003eSetting up a DNS server with BIND is not a very difficult process, but most of the guides and walk-through I’ve read on the subject tend to make it more complicated than it has to be, or that their information is very outdated. The results are a group of confused system admins who get stuck in an endless loop of troubleshooting their damn DNS server!\u003c/p\u003e\n\u003cp\u003eSo my goal here is to help you set up your DNS server and get it running with minimal hassle.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"what-you-will-need-to-start\"\u003eWhat you will need to start\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA freshly-installed Linux server ready to go. In this example, I’m going to be using CentOS 7\u003c/li\u003e\n\u003cli\u003eA valid public IP address\u003c/li\u003e\n\u003cli\u003eA registered domain name, e.g workingtitle.pro. If you don’t already have a domain name, you can register one through \u003ca href=\"https://www.godaddy.com/it-it\"\u003eGoDaddy\u003c/a\u003e or \u003ca href=\"https://www.namecheap.com/\"\u003eNamecheap\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003ch2 id=\"domain-settings\"\u003eDomain settings\u003c/h2\u003e\n\u003cp\u003eThe first thing you’ll need to do is to create DNS records in your domain control panel. Login into your domain registry panel and create the nameserver records as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ens1.yourdomain.com     IP: [your-server-ip]\nns2.yourdomain.com     IP: [your-server-ip]\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"installing-bind-and-initial-configuration\"\u003eInstalling BIND and initial configuration\u003c/h2\u003e\n\u003cp\u003eThe first step is to install BIND, which is the software that controls our DNS server. You can do so via this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eyum -y install bind bind-utils\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter the installation process is done, we’ll need to make our changes to the BIND configuration file in order to function correctly. In CentOS BIND is managed by a process called “named“. Open the configuration file with the text editor of your choice:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003evim /etc/named.conf\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe first section we need to change is this part:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elisten-on port 53 { 127.0.0.1; };\nlisten-on-v6 port 53 { ::1; };\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHowever as it stands, the nameserver is only responding to requests from the server itself. (127.0.0.1). Therefore, if any computer on the internet tries to reach this server it will be rejected. We will need to change the value to any as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elisten-on port 53 { any; };\nlisten-on-v6 port 53 { any; };\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNext, we need to allow the nameserver to accept query requests from any IP and to reject recursion. If Recursive DNS is active on a server, it will try to respond to DNS queries for domains that are NOT within your namesever. So technically, someone could ask your server for the IP of \u003ca href=\"https://www.google.com\"\u003ewww.google.com\u003c/a\u003e. Obviously your server is not the authority for that, so it’s standard practice to disable recursion.\nMake these changes as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eallow-query { any; };\nrecursion no;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn the next step we need to define zones for your domain. In the end of the /etc/named.conf file, add the following lines:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ezone \u0026#34;workingtitle.pro\u0026#34; IN {\n\n         type master;\n         file \u0026#34;/var/named/workingtitle.pro.db\u0026#34;;\n};\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eReplace “workingtitle.pro” with your own domain name. This section tells the users that the zone for the domain “workingtitle.pro” exists on this sever and the details can be found at /var/named/workingtitle.pro.db.\u003c/p\u003e\n\u003cp\u003eThat’s all the changes we need to make to the configuration file. The rest of the settings are optional and you can change them based on your need.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"creating-zone-files\"\u003eCreating Zone files\u003c/h2\u003e\n\u003cp\u003eThe next step is to create zone files for your domains. The zone files contain the DNS records, showing where each resource is located.\u003c/p\u003e\n\u003cp\u003eCreate the zone file for the domain by editing this file:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003evim /var/named/workingtitle.pro.db\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis is the same directory we defined in the previous step. Copy and paste the content blew into the file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e$TTL 14400\nworkingtitle.pro.   IN  SOA     ns1.workingtitle.pro. hostmaster.workingtitle.pro. (\n                                                1001    ;Serial\n                                                4H      ;Refresh\n                                                15M     ;Retry\n                                                1W      ;Expire\n                                                1D      ;Minimum TTL\n                                                )\n\n;Name Server Information\nworkingtitle.pro.\t14400\tIN\tNS      ns1.workingtitle.pro.\nworkingtitle.pro.\t14400\tIN      NS      ns2.workingtitle.pro.\n\n;IP address of Name Server\nns1\tIN\tA       8.8.8.8\t;replace with your server public IP address\nns2\tIN\tA\t8.8.8.8\t;replace with your server public IP address\n\n;Mail exchanger\nworkingtitle.pro.\tIN  \tMX \t10\tmail.workingtitle.pro.\n\n;A - Record HostName To IP Address\nworkingtitle.pro.     IN  A       8.8.8.8 ;replace with your server public IP address\nwww\tIN\tA\t8.8.8.8\t;replace with your server public IP address\nmail\tIN\tA\t8.8.8.8\t;replace with your server public IP address\n\n;CNAME record\nftp     IN\tCNAME        workingtitle.pro.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLet’s take a look at each section individually:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e;Name Server Information\nworkingtitle.pro.\t14400\tIN\tNS      ns1.workingtitle.pro.\nworkingtitle.pro.\t14400\tIN      NS      ns2.workingtitle.pro.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eHere we’re defining our nameserver address so We’re essentially telling anyone who is querying our server that the nameservers for domain “workingtitle.pro” are “ns1.workingtitle.pro” and “ns2.workingtitle.pro”. You should obviously change this to your own domain name.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e;IP address of Name Server\nns1\tIN\tA       8.8.8.8\t;replace with your server public IP address\nns2\tIN\tA\t8.8.8.8\t;replace with your server public IP address\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow we need to define the IP address of our nameservers. Here you can see that the IP for “ns1” and “ns2” has been defined. Replace 8.8.8.8 with your server public IP.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eImportant note\u003c/b\u003e: It is strongly advised to use at least two different servers with different IPs for your nameservers. This is to provide redundancy in case one of the nameservers fails, but in the spirit of keeping things simple, I’ve only used one server with a single IP.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e;A - Record HostName To IP Address\nworkingtitle.pro.     IN  A       8.8.8.8 ;replace with your server public IP address\nwww\tIN\tA\t8.8.8.8\t;replace with your server public IP address\nmail\tIN\tA\t8.8.8.8\t;replace with your server public IP address\n\n;CNAME record\nftp     IN\tCNAME        workingtitle.pro.\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIn this section we can set the main DNS records such as the A record for the domain, the IP of the mail server and other services.\u003c/p\u003e\n\u003cbr\u003e\n\u003ch2 id=\"firewall-configuration\"\u003eFirewall configuration\u003c/h2\u003e\n\u003cp\u003eWe need to open port 53 on the server firewall in order to allow DNS traffic through. You can use the following command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003efirewall-cmd --zone=external --permanent --add-port=53/udp\nfirewall-cmd --reload\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you’re not using \u003ccode\u003efirewalld\u003c/code\u003e, you can use the following command to open the port with \u003ccode\u003eiptables\u003c/code\u003e.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eiptables -A INPUT -p udp --dport 53 -j ACCEPT\n\u003c/code\u003e\u003c/pre\u003e\u003cbr\u003e\n\u003ch2 id=\"start-the-dns-service\"\u003eStart the DNS service\u003c/h2\u003e\n\u003cp\u003eThe configuration is done! We can go ahead and start the DNS service:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esystemctl enable named\nsystemctl start named\n\u003c/code\u003e\u003c/pre\u003e",
        "url": "https://workingtitle.pro/posts/setting-up-a-dns-server-on-centos-from-scratch/",
        "date_published": "13076-13-09T70:1313:00+00:00",
        "date_modified": "13076-13-09T70:1313:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "53cd9bf089d6ce81d309e145ed049a42466d47f3",
        "title": "Install SSL on Adobe Connect",
        "summary": "",
        "content_text": "Adobe Connect is a software primarily used for online classes and web conferencing. Installing SSL on the Connect services such as application and meeting are essential so we’re going to look at how to install SSL on Adobe Connect.\nI’m going to assume you already have Adobe Connect installed on your server. If not, you can you this guide to install it.\nNow let’s start. Please note that this guide is for Adobe Connect version 9 or higher.\nBefore you start:\nMake sure your SSL keys are ready. They should be in this format:\nYour main certificate key in .pem format. You might have received two separate files from your CA but you can combine these two to make a single file. Private key in .pem format. Remember not to use passphrase on your SSL keys. Install Stunnel Now you need to install stunnel. It’s a program that adds SSL/TLS functionality to your website. Firstly, install it from here: https://www.stunnel.org/downloads.html\nSecondly, open stunnel.exe and follow the installation process. It’s better to install in the C:\\Connect\\stunnel directory so it will be in the same directory as Adobe Connect and easier to troubleshoot.\nStunnel configuration Copy the code below to the stunnel.conf file located at C:\\Connect\\Stunnel\\conf\\ and remove all the previous code found in the file so we can add the new configuration. but before making any changes, make sure you make a copy of the stunnel.conf file.\n; Protocol version (all, SSLv2, SSLv3, TLSv1) sslVersion = all options = NO_SSLv2 options = NO_SSLv3 options = DONT_INSERT_EMPTY_FRAGMENTS options = CIPHER_SERVER_PREFERENCE renegotiation=no fips = no ;Some performance tunings socket = l:TCP_NODELAY=1 socket = r:TCP_NODELAY=1 TIMEOUTclose=0 ; application server SSL / HTTPS 3[https-vip] accept = 10.1.1.1:443 connect = 127.0.0.1:8443 cert = C:\\Connect\\stunnel\\certs\\public_certificate_app-server.pem key = C:\\Connect\\stunnel\\certs\\private_key_app-server.key ;configure ciphers as per your requirement and client support. ;this should work for most: ciphers = TLSv1+HIGH:!SSLv2:!aNULL:!eNULL:!3DES Remember to replace 10.1.1.1 with your server IP and In the “cert” and “key” directories you have to specifies the location of the certificate file and the private key respectively. These are the same keys with the .pem format I mentioned at the start.\nNow, we can check to see if the certificate is working correctly with our configurations before taking it live. Follow these steps:\nOpen Stunnel.exe located at /bin/ folder. After doing that an icon appears in the notification area. Right-click the icon and select “check configuration” If everything is working correctly you should see a message like this: “2020.07.04 11:40:18 LOG5[main]: Configuration successful” As the result of configurations being correct, we can go ahead and set up Stunnel as a service so you don’t need to manually start it every time you reboot the server.\nNavigate to the /bin/ directory via Windows Command Line (CMD) Type the following command: stunnel.exe -install In the Windows Services menu a new services will be created named Stunnel TLS Wrapper. Make sure that it’s set to automatic. Custom.ini After making the changes to the configuration file, open the custom.ini file located at c:\\Connect\\9.x\\\nAdd the following lines to the end of custom.ini:\nADMIN_PROTOCOL=https:// SSL_ONLY=yes Server.xml Find this file and open it: C:\\Connect\\9.x\\appserv\\conf\\server.xml\nYou need to make two changes and uncomment certain sections. First uncomment this part:\n\u0026lt;Connector port=\u0026#34;8443\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; executor=\u0026#34;httpsThreadPool\u0026#34; enableLookups=\u0026#34;false\u0026#34; acceptCount=\u0026#34;250\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; SSLEnabled=\u0026#34;false\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; proxyPort=\u0026#34;443\u0026#34; URIEncoding=\u0026#34;utf-8\u0026#34;/\u0026gt; and then this:\n\u0026lt;Executor name=\u0026#34;httpsThreadPool\u0026#34; namePrefix=\u0026#34;https-8443-\u0026#34; maxThreads=\u0026#34;350\u0026#34; minSpareThreads=\u0026#34;25\u0026#34;/\u0026gt; And you’re done. You have followed all the steps to install ssl on Adobe Connect.\nFirst, Restart all related services, Adobe Connect, Adobe Media and Stunnel wrapper Make sure that port 443 is open on your firewall. If there is a need, write an additional rule to let the traffic through. If after restarting the services Adobe Connect doesn’t start and you see a “Not Ready” error, try restarting your MySQL server. That helped me. Note that this is a simple configuration for the Application only. You can find the full details in this official guide. ",
        "content_html": "\u003cp\u003eAdobe Connect is a software primarily used for online classes and web conferencing. Installing SSL on the Connect services such as application and meeting are essential so we’re going to look at how to install SSL on Adobe Connect.\u003c/p\u003e\n\u003cp\u003eI’m going to assume you already have Adobe Connect installed on your server. If not, you can you \u003ca href=\"https://helpx.adobe.com/adobe-connect/installconfigure/install-connect-using-installer.html\"\u003ethis guide\u003c/a\u003e to install it.\u003c/p\u003e\n\u003cp\u003eNow let’s start. Please note that this guide is for Adobe Connect version 9 or higher.\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eBefore you start:\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003eMake sure your SSL keys are ready. They should be in this format:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYour main certificate key in .pem format. You might have received two separate files from your CA but you can combine these two to make a single file.\u003c/li\u003e\n\u003cli\u003ePrivate key in .pem format.\u003c/li\u003e\n\u003cli\u003eRemember not to use passphrase on your SSL keys.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"install-stunnel\"\u003eInstall Stunnel\u003c/h2\u003e\n\u003cp\u003eNow you need to install stunnel. It’s a program that adds SSL/TLS functionality to your website. Firstly, install it from here: \u003ca href=\"https://www.stunnel.org/downloads.html\"\u003ehttps://www.stunnel.org/downloads.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSecondly, open stunnel.exe and follow the installation process. It’s better to install in the C:\\Connect\\stunnel directory so it will be in the same directory as Adobe Connect and easier to troubleshoot.\u003c/p\u003e\n\u003ch2 id=\"stunnel-configuration\"\u003eStunnel configuration\u003c/h2\u003e\n\u003cp\u003eCopy the code below to the stunnel.conf file located at C:\\Connect\\Stunnel\\conf\\ and remove all the previous code found in the file so we can add the new configuration. but before making any changes, make sure you make a copy of the stunnel.conf file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e; Protocol version (all, SSLv2, SSLv3, TLSv1)\nsslVersion = all\noptions = NO_SSLv2\noptions = NO_SSLv3\noptions = DONT_INSERT_EMPTY_FRAGMENTS\noptions = CIPHER_SERVER_PREFERENCE\nrenegotiation=no\nfips = no\n;Some performance tunings\nsocket = l:TCP_NODELAY=1\nsocket = r:TCP_NODELAY=1\nTIMEOUTclose=0\n; application server SSL / HTTPS\n3[https-vip]\naccept = 10.1.1.1:443\nconnect = 127.0.0.1:8443\ncert = C:\\Connect\\stunnel\\certs\\public_certificate_app-server.pem\nkey = C:\\Connect\\stunnel\\certs\\private_key_app-server.key\n;configure ciphers as per your requirement and client support.\n;this should work for most:\nciphers = TLSv1+HIGH:!SSLv2:!aNULL:!eNULL:!3DES\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eRemember to replace 10.1.1.1 with your server IP and In the “cert” and “key” directories you have to specifies the location of the certificate file and the private key respectively. These are the same keys with the .pem format I mentioned at the start.\u003c/p\u003e\n\u003cp\u003eNow, we can check to see if the certificate is working correctly with our configurations before taking it live. Follow these steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpen Stunnel.exe located at /bin/ folder. After doing that an icon appears in the notification area.\u003c/li\u003e\n\u003cli\u003eRight-click the icon and select “check configuration”\u003c/li\u003e\n\u003cli\u003eIf everything is working correctly you should see a message like this:\n“2020.07.04 11:40:18 LOG5[main]: Configuration successful”\u003c/li\u003e \n\u003c/ul\u003e\n\u003cp\u003eAs the result of configurations being correct, we can go ahead and set up Stunnel as a service so you don’t need to manually start it every time you reboot the server.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNavigate to the /bin/ directory via Windows Command Line (CMD)\u003c/li\u003e\n\u003cli\u003eType the following command: stunnel.exe -install\u003c/li\u003e\n\u003cli\u003eIn the Windows Services menu a new services will be created named Stunnel TLS Wrapper. Make sure that it’s set to automatic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"customini\"\u003eCustom.ini\u003c/h2\u003e\n\u003cp\u003eAfter making the changes to the configuration file, open the custom.ini file located at c:\\Connect\\9.x\\\u003c/p\u003e\n\u003cp\u003eAdd the following lines to the end of custom.ini:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eADMIN_PROTOCOL=https://\nSSL_ONLY=yes\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"serverxml\"\u003eServer.xml\u003c/h2\u003e\n\u003cp\u003eFind this file and open it: \u003ci\u003eC:\\Connect\\9.x\\appserv\\conf\\server.xml\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003eYou need to make two changes and \u003cb\u003euncomment\u003c/b\u003e certain sections. First uncomment this part:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;Connector port=\u0026#34;8443\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34;\n    executor=\u0026#34;httpsThreadPool\u0026#34;\n        enableLookups=\u0026#34;false\u0026#34;\n        acceptCount=\u0026#34;250\u0026#34;\n        connectionTimeout=\u0026#34;20000\u0026#34;\n        SSLEnabled=\u0026#34;false\u0026#34;\n        scheme=\u0026#34;https\u0026#34;\n        secure=\u0026#34;true\u0026#34;\n        proxyPort=\u0026#34;443\u0026#34;\n        URIEncoding=\u0026#34;utf-8\u0026#34;/\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eand then this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026lt;Executor name=\u0026#34;httpsThreadPool\u0026#34;\n    namePrefix=\u0026#34;https-8443-\u0026#34;\n    maxThreads=\u0026#34;350\u0026#34;\n    minSpareThreads=\u0026#34;25\u0026#34;/\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd you’re done. You have followed all the steps to install ssl on Adobe Connect.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFirst, Restart all related services, Adobe Connect, Adobe Media and Stunnel wrapper\u003c/li\u003e\n\u003cli\u003eMake sure that port 443 is open on your firewall. If there is a need, write an additional rule to let the traffic through.\u003c/li\u003e\n\u003cli\u003eIf after restarting the services Adobe Connect doesn’t start and you see a “Not Ready” error, try restarting your MySQL server. That helped me.\u003c/li\u003e\n\u003cli\u003eNote that this is a simple configuration for the Application only. You can find the full details in this \u003ca href=\"https://blogs.adobe.com/connectsupport/files/2016/04/Connect-SSL-Guide.pdf\"\u003eofficial guide\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e",
        "url": "https://workingtitle.pro/posts/install-ssl-on-adobe-connect/",
        "date_published": "7076-07-09T70:77:00+00:00",
        "date_modified": "7076-07-09T70:77:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "d9c27ec987950647d3e12113d88a334c12fb4ce5",
        "title": "What's the difference between SSL and TLS?",
        "summary": "",
        "content_text": "SSL/TLS is a protocol for encrypting the communications between server and client. Netscape developed the first version of SSL in 1995 never released to the public because of major security flaws.\nThe first version that was used by the general public was SSL 2.0 which was released in 1995 via RFC 6176. However in the following year, the protocol received a complete redesign and SSL version 3.0 was released in 1996.\nTLS or Transport Layer Security is just an updated version of SSL. So you could think of TLS as SSL 4.0. Because of some convoluted reasons however, a name change for the protocol occurred. Today when we say “SSL” we’re actually refereeing to TLS because the actual SSL protocol has long been deprecated. Hopefully no servers today are using the older versions of SSL.\nThe latest version of TLS is 1.3. Version 1.0 and 1.1 are officially deprecated as of 2020, so it’s absolutely necessary for any servers that are using these protocols to immediately update to version 1.2 or 1.3.\nBut how does it work? For encrypting the data between the client and server, TLS uses a mechanism called “handshake”. During a TLS handshake a number of actions need to occur in order to establish a secure connection between the server and the client:\nPick the version of TLS used by the client and server ( 1.2 , 1.3 etc) ecide on which ciphers to use Verify the authenticity of the server via the SSL certificate and and public key (we’ll look at this later) And finally, create session keys or symmetric encryption after successfully completing the handshake During the TLS handshake, the server and client need to exchange variety of different information in order to set up a secure connection.\nRSA and Elliptic Curve are two of the most notable algorithms used for the client and server to exchange keys during the handshake. In this example we’re going to take a look at the most widely used algorithm: RSA\nHow a TLS connection works Client hello message: The client starts the process by sending a “hello” message to the server. This message usually contains the cipher suites and the TLS version which the client wants to use, and a string of random bytes. Server hello message: The server replies to this message by sending the SSL certificate to the client (the public key). Along with this message, the server sends cipher suites and it’s own string of random characters. Authentication: The client now has to verify the authenticity of the SSL certificate it received from the server by checking it against the data provided by the Certificate Authority (e.g GoDaddy, Comodo, Digicert, etc). This is done so by using the Certificate Authority root cert that’s usually embedded in most known programs and browsers. This step is essential so we can make sure we’re talking to the correct server and not some malicious server trying to lure us. Client response: After authentication is complete, the client generates another set of random characters called the “premaster key”, encrypts it via the public key it received from the server and then sends it to the server. It’s important to note only the server can decrypt this message, because only the server has the required private key. Session key creation: The client and server have 3 keys: the client random, the server random and the premaster key. Using these 3 string of bytes, both client and server create a session key. They both create a “finished” message, sign it with the new session key and send it over. Both client and server can now confirm they have the same session key. TLS connection established: We have established a secure connection. The client and server will now continue to communicate and encrypt the data via the session key. Conclusion So this is roughly the process that happens when you try to connect to a server or webpage that’s using SSL/TLS. So far that the technology allows, this is the best way we can secure connections between client and server.\nIt would take a normal computer 3 trillion years to break a 2048-bit RSA key. So it’s safe to say it’s pretty secure as of now. But with the rise of quantum computing that we’re seeing, maybe in 10-20 years time, this will not be such a safe method of encryption. We’ll have to come up with new ways!\n",
        "content_html": "\u003cp\u003eSSL/TLS is a protocol for encrypting the communications between server and client. Netscape developed the first version of SSL in 1995 never released to the public because of major security flaws.\u003c/p\u003e\n\u003cp\u003eThe first version that was used by the general public was SSL 2.0 which was released in 1995 via RFC 6176. However in the following year, the protocol received a complete redesign and SSL version 3.0 was released in 1996.\u003c/p\u003e\n\u003cp\u003eTLS or Transport Layer Security is just an updated version of SSL. So you could think of TLS as SSL 4.0. Because of some convoluted reasons however, a name change for the protocol occurred. Today when we say “SSL” we’re actually refereeing to TLS because the actual SSL protocol has long been deprecated. Hopefully no servers today are using the older versions of SSL.\u003c/p\u003e\n\u003cp\u003eThe latest version of TLS is 1.3. Version 1.0 and 1.1 are officially deprecated as of 2020, so it’s absolutely necessary for any servers that are using these protocols to immediately update to version 1.2 or 1.3.\u003c/p\u003e\n\u003ch2 id=\"but-how-does-it-work\"\u003eBut how does it work?\u003c/h2\u003e\n\u003cp\u003eFor encrypting the data between the client and server, TLS uses a mechanism called “handshake”. During a TLS handshake a number of actions need to occur in order to establish a secure connection between the server and the client:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePick the version of TLS used by the client and server ( 1.2 , 1.3 etc)\u003c/li\u003e\n\u003cli\u003eecide on which ciphers to use\u003c/li\u003e\n\u003cli\u003eVerify the authenticity of the server via the SSL certificate and and public key (we’ll look at this later)\u003c/li\u003e\n\u003cli\u003eAnd finally, create session keys or symmetric encryption after successfully completing the handshake\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDuring the TLS handshake, the server and client need to exchange variety of different information in order to set up a secure connection.\u003c/p\u003e\n\u003cp\u003eRSA and Elliptic Curve are two of the most notable algorithms used for the client and server to exchange keys during the handshake. In this example we’re going to take a look at the most widely used algorithm: RSA\u003c/p\u003e\n\u003ch2 id=\"how-a-tls-connection-works\"\u003eHow a TLS connection works\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eClient hello message:\u003c/b\u003e The client starts the process by sending a “hello” message to the server. This message usually contains the cipher suites and the TLS version which the client wants to use, and a string of random bytes.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eServer hello message:\u003c/b\u003e The server replies to this message by sending the SSL certificate to the client (the public key). Along with this message, the server sends cipher suites and it’s own string of random characters.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eAuthentication:\u003c/b\u003e The client now has to verify the authenticity of the SSL certificate it received from the server by checking it against the data provided by the Certificate Authority (e.g GoDaddy, Comodo, Digicert, etc). This is done so by using the Certificate Authority root cert that’s usually embedded in most known programs and browsers. This step is essential so we can make sure we’re talking to the correct server and not some malicious server trying to lure us.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eClient response:\u003c/b\u003e After authentication is complete, the client generates another set of random characters called the “premaster key”, encrypts it via the public key it received from the server and then sends it to the server. It’s important to note only the server can decrypt this message, because only the server has the required private key.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eSession key creation:\u003c/b\u003e The client and server have 3 keys: the client random, the server random and the premaster key. Using these 3 string of bytes, both client and server create a session key. They both create a “finished” message, sign it with the new session key and send it over. Both client and server can now confirm they have the same session key.\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eTLS connection established:\u003c/b\u003e We have established a secure connection. The client and server will now continue to communicate and encrypt the data via the session key.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eSo this is roughly the process that happens when you try to connect to a server or webpage that’s using SSL/TLS. So far that the technology allows, this is the best way we can secure connections between client and server.\u003c/p\u003e\n\u003cp\u003eIt would take a normal computer 3 trillion years to break a 2048-bit RSA key. So it’s safe to say it’s pretty secure as of now. But with the rise of quantum computing that we’re seeing, maybe in 10-20 years time, this will not be such a safe method of encryption. We’ll have to come up with new ways!\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/whats-the-difference-between-ssl-tls/",
        "date_published": "23066-23-09T60:2323:00+00:00",
        "date_modified": "23066-23-09T60:2323:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "4c1d55851ec854f7b1d11b5e3e5419eae5a7c8be",
        "title": "Setting Up Python App on Virtualenv",
        "summary": "",
        "content_text": "Virtualenv is a tool for creating isolated environments for running your python application. Most python programs require a variety of different libraries that are needed for running the applications on any machine. So If you don’t use a virtual environment, you will have to install all the libraries all the time. Overtime this will cause conflicts between different packages. So it’s always a good idea to isolate the environment for your python project. That’s why, most of the applications need to be written in a virtual environment to avoid problems. In this guide we’re going to look at how to set up Python app on virtualenv.\nHere’s what you need to do to create a virtual environment.\nFirst, make sure that you have Python installed. You can install the latest version of Python from python.org. I recommend to use Python 3 which is the latest version with long-term support.\nCreating a virtual environment Now we need to create a virtual environment path using this command:\npython3 -m venv /path/to/new/virtual/environment Then we can use the source command to activate the virtual environment as so:\nsource bin/activate Now your shell should look something like this:\nnew-test is the directory path we created in the previous step with the python3 -m command. This shows that we’re currently inside the virtual environment for Python. Any package and library that you will install will only be accessible inside this virtual environment and will not have system-wide effects.\nFor example we’re going to install the pygame which is a set of modules for creating games in python. You can install it with the pip tool:\npip install pygame You can connect to this virtual environment at anytime using the source command. Install all the packages and modules within this environment and run your application. I always recommend to run your python app within virtualenv to avoid environment problems in future.\nOfficial documentation on Python venv can be found here.\n",
        "content_html": "\u003cp\u003e\u003ca href=\"https://virtualenv.pypa.io/en/latest/\"\u003eVirtualenv\u003c/a\u003e is a tool for creating isolated environments for running your python application. Most python programs require a variety of different libraries that are needed for running the applications on any machine. So If you don’t use a virtual environment, you will have to install all the libraries all the time. Overtime this will cause conflicts between different packages. So it’s always a good idea to isolate the environment for your python project. That’s why, most of the applications need to be written in a virtual environment to avoid problems. In this guide we’re going to look at how to set up Python app on virtualenv.\u003c/p\u003e\n\u003cp\u003eHere’s what you need to do to create a virtual environment.\u003c/p\u003e\n\u003cp\u003eFirst, make sure that you have Python installed. You can install the latest version of Python from \u003ca href=\"https://www.python.org/\"\u003epython.org\u003c/a\u003e. I recommend to use Python 3 which is the latest version with long-term support.\u003c/p\u003e\n\u003ch2 id=\"creating-a-virtual-environment\"\u003eCreating a virtual environment\u003c/h2\u003e\n\u003cp\u003eNow we need to create a virtual environment path using this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epython3 -m venv /path/to/new/virtual/environment\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen we can use the source command to activate the virtual environment as so:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esource bin/activate\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow your shell should look something like this:\u003c/p\u003e\n\u003cimg src=\"https://i.ibb.co/61T5MhZ/python01.png\"\u003e\n\u003cp\u003e\u003ccode\u003enew-test\u003c/code\u003e is the directory path we created in the previous step with the \u003ccode\u003epython3 -m\u003c/code\u003e command. This shows that we’re currently inside the virtual environment for Python. Any package and library that you will install will only be accessible inside this virtual environment and will not have system-wide effects.\u003c/p\u003e\n\u003cp\u003eFor example we’re going to install the pygame which is a set of modules for creating games in python. You can install it with the pip tool:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003epip install pygame\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can connect to this virtual environment at anytime using the source command. Install all the packages and modules within this environment and run your application. I always recommend to run your python app within virtualenv to avoid environment problems in future.\u003c/p\u003e\n\u003cp\u003eOfficial documentation on Python venv can be found \u003ca href=\"https://docs.python.org/3/library/venv.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/setting-up-python-app-on-virtualenv/",
        "date_published": "20066-20-09T60:2020:00+00:00",
        "date_modified": "20066-20-09T60:2020:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "5f9f1c9811098a09597c820632ba8a01421ad773",
        "title": "Install and Configure SSL on Nginx",
        "summary": "",
        "content_text": "NGINX is web server, reverse proxy and caching tool that is relatively new compared to Apache. It’s gaining popularity around the world due to its applications. Because of its load-balancing feature, it is widely used in websites with heavy traffic, the best example of which is YouTube. In this guide we’re going to look at how to install ssl on nginx\nI’ll walk you through installing an SSL certificate on Nginx. Its a fairly easy process even if you have had no prior experience with Nginx.\nI am going to assume you already have a valid SSL certificate, ready for installation. You can purchase a certificate from a variety of different sources such as Digicert or Comodo, but I would personally recommend using Let’s Encrypt which is a non-profit SSL CA backed by the Linux Foundation.\nThe first step is to log into your sever via ssh:\nssh root@[your-server-IP] Navigate to the Nginx directory:\ncd /etc/nginx/sites-available Editing virtual host configs There should be a virtual host configuration file for your website under sites-available/example.com. Open the the file via a text editor. It should look like this:\nserver { listen 80; listen [::]:80; server_name your.domain.com; access_log /var/log/nginx/nginx.vhost.access.log; error_log /var/log/nginx/nginx.vhost.error.log; location / { root /home/www/public_html/your.domain.com/public/; index index.html; } } As you can see, Nginx is currently listening on port 80 which means your website is only using HTTP. In order to access the website with HTTPS, you need to create another server block. Copy the entire block and paste it below.\nAfter copying the block, add the following lines to the new block:\nlisten 443; ssl on; ssl_certificate /etc/ssl/your_domain_name.pem; (or bundle.crt) ssl_certificate_key /etc/ssl/your_domain_name.key; This configuration tells Nginx to listen on port 443 (SSL) and it specifies the directories for your SSL keys.\n“ssl_certificate” is the directory for your main SSL key or the bundle.\n“ssl_certificate_key” is the directory for the private key.\nMake sure that the paths specified are correct and your keys are places in the those directories. Of course you can change to any other directory and update the configuration file accordingly.\nIn the end, your configuration file should look something like this:\nserver { listen 80; listen [::]:80; server_name your.domain.com; access_log /var/log/nginx/nginx.vhost.access.log; error_log /var/log/nginx/nginx.vhost.error.log; location / { root /home/www/public_html/your.domain.com/public/; index index.html; } } server { listen 443; listen [::]:443; ssl on; ssl_certificate /etc/ssl/your_domain_name.pem; (or bundle.crt) ssl_certificate_key /etc/ssl/your_domain_name.key; server_name your.domain.com; access_log /var/log/nginx/nginx.vhost.access.log; error_log /var/log/nginx/nginx.vhost.error.log; location / { root /home/www/public_html/your.domain.com/public/; index index.html; } } Check syntax As you can see, with this configuration, your website is accessible through both port 80 and 443. In case you want your website to be available ONLY through HTTPS , you can remove the block with port 80, but that’s generally not recommended.\nNow make sure that port 443 is open on your firewall. Then run this command to check Nginx syntax and configuration:\nnginx -t If it’s successful, then go ahead and restart Nginx for the changes to take effect.\nservice nginx restart And that’s it. you’re pretty much done. We have learned how to install ssl on nginx. The website should be accessible through HTTPS.\nIf you would like to learn more about SSL/TLS in general, you can read this.\n",
        "content_html": "\u003cp\u003eNGINX is web server, reverse proxy and caching tool that is relatively new compared to Apache. It’s gaining popularity around the world due to its applications. Because of its load-balancing feature, it is widely used in websites with heavy traffic, the best example of which is YouTube. In this guide we’re going to look at how to install ssl on nginx\u003c/p\u003e\n\u003cp\u003eI’ll walk you through installing an SSL certificate on Nginx. Its a fairly easy process even if you have had no prior experience with Nginx.\u003c/p\u003e\n\u003cp\u003eI am going to assume you already have a valid SSL certificate, ready for installation. You can purchase a certificate from a variety of different sources such as Digicert or Comodo, but I would personally recommend using \u003ca href=\"https://letsencrypt.org/\"\u003eLet’s Encrypt\u003ca/\u003e which is a non-profit SSL CA backed by the Linux Foundation.\u003c/p\u003e\n\u003cp\u003eThe first step is to log into your sever via ssh:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003essh root@[your-server-IP]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNavigate to the Nginx directory:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ecd /etc/nginx/sites-available\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"editing-virtual-host-configs\"\u003eEditing virtual host configs\u003c/h2\u003e\n\u003cp\u003eThere should be a virtual host configuration file for your website under \u003ci\u003esites-available/example.com.\u003c/i\u003e Open the the file via a text editor. It should look like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eserver {\n        listen 80;\n        listen [::]:80;\n\n        server_name your.domain.com;\n        access_log /var/log/nginx/nginx.vhost.access.log;\n        error_log /var/log/nginx/nginx.vhost.error.log;\n        location / {\n        root   /home/www/public_html/your.domain.com/public/;\n        index  index.html;\n        }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAs you can see, Nginx is currently listening on port 80 which means your website is only using HTTP. In order to access the website with HTTPS, you need to create another server block. Copy the entire block and paste it below.\u003c/p\u003e\n\u003cp\u003eAfter copying the block, add the following lines to the new block:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003elisten   443;\n\nssl    on;\nssl_certificate    /etc/ssl/your_domain_name.pem; (or bundle.crt)\nssl_certificate_key    /etc/ssl/your_domain_name.key;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis configuration tells Nginx to listen on port 443 (SSL) and it specifies the directories for your SSL keys.\u003c/p\u003e\n\u003cp\u003e“ssl_certificate” is the directory for your main SSL key or the bundle.\u003c/p\u003e\n\u003cp\u003e“ssl_certificate_key” is the directory for the private key.\u003c/p\u003e\n\u003cp\u003eMake sure that the paths specified are correct and your keys are places in the those directories. Of course you can change to any other directory and update the configuration file accordingly.\u003c/p\u003e\n\u003cp\u003eIn the end, your configuration file should look something like this:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eserver {\n        listen 80;\n        listen [::]:80;\n\n        server_name your.domain.com;\n        access_log /var/log/nginx/nginx.vhost.access.log;\n        error_log /var/log/nginx/nginx.vhost.error.log;\n        location / {\n        root   /home/www/public_html/your.domain.com/public/;\n        index  index.html;\n        }\n}\n\nserver {\n        listen 443;\n        listen [::]:443;\n\n        ssl    on;\n        ssl_certificate    /etc/ssl/your_domain_name.pem; (or bundle.crt)\n        ssl_certificate_key    /etc/ssl/your_domain_name.key;\n\n        server_name your.domain.com;\n        access_log /var/log/nginx/nginx.vhost.access.log;\n        error_log /var/log/nginx/nginx.vhost.error.log;\n        location / {\n        root   /home/www/public_html/your.domain.com/public/;\n        index  index.html;\n        }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"check-syntax\"\u003eCheck syntax\u003c/h2\u003e\n\u003cp\u003eAs you can see, with this configuration, your website is accessible through both port 80 and 443. In case you want your website to be available ONLY through HTTPS , you can remove the block with port 80, but that’s generally not recommended.\u003c/p\u003e\n\u003cp\u003eNow make sure that port 443 is open on your firewall. Then run this command to check Nginx syntax and configuration:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003enginx -t\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf it’s successful, then go ahead and restart Nginx for the changes to take effect.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservice nginx restart\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAnd that’s it. you’re pretty much done. We have learned how to install ssl on nginx. The website should be accessible through HTTPS.\u003c/p\u003e\n\u003cp\u003eIf you would like to learn more about SSL/TLS in general, you can \u003ca href=\"https://workingtitle.pro/index.php/2020/06/23/whats-the-difference-between-ssl-and-tls/\"\u003eread\u003c/a\u003e this.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/install-and-configure-ssl-on-nginx/",
        "date_published": "15066-15-09T60:1515:00+00:00",
        "date_modified": "15066-15-09T60:1515:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "c6b0afeb94235c6c6cf106ad05d01c4940e310b6",
        "title": "IPSEC/L2tp VPN",
        "summary": "",
        "content_text": "For a long time I was looking for a personal VPN solution. I was too lazy to work on setting up a VPN server from scratch so I had to try out many different options online. I tried OpenVPN for a while and used free online servers but that wasn’t good enough. Slow speeds and difficulty in connecting to some servers were the biggest issue.\nI’ve tried using OpenVPN and configuring my own server, but the setup and installation process is a little bit too convoluted and I didn’t find it very user-friendly.\nIPsec/L2TP The best solution I’ve found has been this IPSec/L2TP on github. Thanks to hwdsl2 he saved me a lot of trouble. The setup processes on the server couldn’t be easier.\nOn the client side, you’ll have to do a one-time registry edit but that’s not too bad. The only downside I can think of is that it allows for only 1 connection per IP. So you can’t have multiple devices that share a public IP connect to the server. Other than that, it works like a charm.\n",
        "content_html": "\u003cp\u003eFor a long time I was looking for a personal VPN solution. I was too lazy to work on setting up a VPN server from scratch so I had to try out many different options online. I tried OpenVPN for a while and used free online servers but that wasn’t good enough. Slow speeds and difficulty in connecting to some servers were the biggest issue.\u003c/p\u003e\n\u003cp\u003eI’ve tried using OpenVPN and configuring my own server, but the setup and installation process is a little bit too convoluted and I didn’t find it very user-friendly.\u003c/p\u003e\n\u003ch2 id=\"ipsecl2tp\"\u003eIPsec/L2TP\u003c/h2\u003e\n\u003cp\u003eThe best solution I’ve found has been \u003ca href=\"https://github.com/hwdsl2/setup-ipsec-vpn\"\u003ethis IPSec/L2TP on github\u003c/a\u003e. Thanks to hwdsl2 he saved me a lot of trouble. The setup processes on the server couldn’t be easier.\u003c/p\u003e\n\u003cp\u003eOn the client side, you’ll have to do a one-time registry edit but that’s not too bad. The only downside I can think of is that it allows for only 1 connection per IP. So you can’t have multiple devices that share a public IP connect to the server. Other than that, it works like a charm.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/ipsec-l2tp-vpn/",
        "date_published": "4066-04-09T60:44:00+00:00",
        "date_modified": "4066-04-09T60:44:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      },
      
      {
        "id": "586ca3391e81c1fe30d85d4df766f4b8e7a8dd25",
        "title": "Dual proxy server with Squid",
        "summary": "",
        "content_text": "Due to some internet restriction that I was experiencing in 2019 , I was forced to find a way to circumvent the disruption to the internet connection, so this is what I did using Squid proxy Server\nI had a VPS set up ready to go inside the country, and I had another identical VPS set up in Germany. You’re going to need very little resource for this. A dual core CPU with 2GB of ram on the VPS should be plenty.\nTo have reliable internet connection, I had to set up a squid proxy server on the VPS located within the country. I, while using my connection through the ISP had no connection to any IPs outside the country, but the VPS which was located in a data center did. This allowed me to find a way to the outside.\nAfter this, I set also set up another squid proxy on the server located in Germany. The goal was to connect these two proxy servers together. So I could connect to the proxy server inside the country, where it could forward my request to the other proxy server in Germany.\nIf you don’t know about Squid, you can read about it and download it from their website. It’s a fairly easy installation process.\nAfter installing Squid on both servers, here’s what I did with the configuration on the first VPS (within the country).\nFirst Server configuration for squid proxy The following is the configuration for the internal squid proxy server. Which is the server within the country that is easily accessible but has internet restrictions.\n#Remember to replace the IPs! #0.0.0.0 is the first sever inside the country and 1.1.1.1 is the 2nd VPS located in Germany acl local-servers dstdomain 0.0.0.0 cache_peer 1.1.1.1 parent 1992 0 no-query default cache_peer_domain 1.1.1.1 !0.0.0.0 never_direct deny local-servers never_direct allow all Please remember to change the IPs mentioned here with your server IP. 0.0.0.0 is the first server (inside country) and 1.1.1.1 is the second server (outside the country).\nThe rest are just default configurations for Squid which you can read about in their documentation. You can add the following lines to the main configuration file.\n# Example rule allowing access from your local networks. # Adapt to list your (internal) IP networks from where browsing # should be allowed acl localnet src 10.0.0.0/8\t# RFC1918 possible internal network acl localnet src 172.16.0.0/12\t# RFC1918 possible internal network acl localnet src 192.168.0.0/16\t# RFC1918 possible internal network acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80\t# http acl Safe_ports port 21\t# ftp acl Safe_ports port 443\t# https acl Safe_ports port 70\t# gopher acl Safe_ports port 210\t# wais acl Safe_ports port 1025-65535\t# unregistered ports acl Safe_ports port 280\t# http-mgmt acl Safe_ports port 488\t# gss-http acl Safe_ports port 591\t# filemaker acl Safe_ports port 777\t# multiling http acl CONNECT method CONNECT auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd auth_param basic children 5 auth_param basic realm Squid Basic Authentication auth_param basic credentialsttl 2 hours acl auth_users proxy_auth REQUIRED http_access allow auth_users pinger_enable off half_closed_clients off quick_abort_min 0 KB quick_abort_max 0 KB quick_abort_pct 95 client_persistent_connections off server_persistent_connections off ################ # # Recommended minimum Access Permission configuration: # # Deny requests to certain unsafe ports http_access deny !Safe_ports # Deny CONNECT to other than secure SSL ports http_access deny CONNECT !SSL_ports # Only allow cachemgr access from localhost http_access allow localhost manager http_access deny manager # We strongly recommend the following be uncommented to protect innocent # web applications running on the proxy server who think the only # one who can access services on \u0026#34;localhost\u0026#34; is a local user #http_access deny to_localhost # # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS # # Example rule allowing access from your local networks. # Adapt localnet in the ACL section to list your (internal) IP networks # from where browsing should be allowed http_access allow localnet http_access allow localhost # And finally deny all other access to this proxy http_access deny all # Squid normally listens to port 3128 http_port 8081 # Uncomment and adjust the following to add a disk cache directory. #cache_dir ufs /var/spool/squid 100 16 256 # Leave coredumps in the first cache dir coredump_dir /var/spool/squid # # Add any of your own refresh_pattern entries above these. # refresh_pattern ^ftp:\t1440\t20%\t10080 refresh_pattern ^gopher:\t1440\t0%\t1440 refresh_pattern -i (/cgi-bin/|\\?) 0\t0%\t0 refresh_pattern .\t0\t20%\t4320 Second Server configuration The configs for the second proxy server located outside the country is much easier. We just have to tell Squid to accept requests from our other server. You can add the following lines to the squid configuration:\nacl child_proxy src 0.0.0.0/32 http_access allow child_proxy 0.0.0.0 is the IP of the first server.\nAfter doing this you should be set. Check your connections and make sure all needed ports are open on your firewalls on both servers.\n",
        "content_html": "\u003cp\u003eDue to some internet restriction that I was experiencing in 2019 , I was forced to find a way to circumvent the disruption to the internet connection, so this is what I did using Squid proxy Server\u003c/p\u003e\n\u003cp\u003eI had a VPS set up ready to go inside the country, and I had another identical VPS set up in Germany. You’re going to need very little resource for this. A dual core CPU with 2GB of ram on the VPS should be plenty.\u003c/p\u003e\n\u003cp\u003eTo have reliable internet connection, I had to set up a squid proxy server on the VPS located within the country. I, while using my connection through the ISP had no connection to any IPs outside the country, but the VPS which was located in a data center did. This allowed me to find a way to the outside.\u003c/p\u003e\n\u003cp\u003eAfter this, I set also set up another squid proxy on the server located in Germany. The goal was to connect these two proxy servers together. So I could connect to the proxy server inside the country, where it could forward my request to the other proxy server in Germany.\u003c/p\u003e\n\u003cp\u003eIf you don’t know about Squid, you can read about it and download it \u003ca href=\"http://www.squid-cache.org/\"\u003efrom their website\u003c/a\u003e. It’s a fairly easy installation process.\u003c/p\u003e\n\u003cp\u003eAfter installing Squid on both servers, here’s what I did with the configuration on the first VPS (within the country).\u003c/p\u003e\n\u003ch2 id=\"first-server-configuration-for-squid-proxy\"\u003eFirst Server configuration for squid proxy\u003c/h2\u003e\n\u003cp\u003eThe following is the configuration for the internal squid proxy server. Which is the server within the country that is easily accessible but has internet restrictions.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#Remember to replace the IPs!\n#0.0.0.0 is the first sever inside the country and 1.1.1.1 is the 2nd VPS located in Germany\n\nacl local-servers dstdomain 0.0.0.0 \ncache_peer 1.1.1.1 parent 1992 0 no-query default \ncache_peer_domain 1.1.1.1 !0.0.0.0\nnever_direct deny local-servers\nnever_direct allow all\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePlease remember to change the IPs mentioned here with your server IP. 0.0.0.0 is the first server (inside country) and 1.1.1.1 is the second server (outside the country).\u003c/p\u003e\n\u003cp\u003eThe rest are just default configurations for Squid which you can read about in their documentation. You can add the following lines to the main configuration file.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e# Example rule allowing access from your local networks.\n# Adapt to list your (internal) IP networks from where browsing\n# should be allowed\nacl localnet src 10.0.0.0/8\t# RFC1918 possible internal network\nacl localnet src 172.16.0.0/12\t# RFC1918 possible internal network\nacl localnet src 192.168.0.0/16\t# RFC1918 possible internal network\nacl localnet src fc00::/7       # RFC 4193 local private network range\nacl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines\n\n\n\nacl SSL_ports port 443\nacl Safe_ports port 80\t\t# http\nacl Safe_ports port 21\t\t# ftp\nacl Safe_ports port 443\t\t# https\nacl Safe_ports port 70\t\t# gopher\nacl Safe_ports port 210\t\t# wais\nacl Safe_ports port 1025-65535\t# unregistered ports\nacl Safe_ports port 280\t\t# http-mgmt\nacl Safe_ports port 488\t\t# gss-http\nacl Safe_ports port 591\t\t# filemaker\nacl Safe_ports port 777\t\t# multiling http\nacl CONNECT method CONNECT\nauth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd\nauth_param basic children 5\nauth_param basic realm Squid Basic Authentication\nauth_param basic credentialsttl 2 hours\nacl auth_users proxy_auth REQUIRED\nhttp_access allow auth_users\n\npinger_enable off\nhalf_closed_clients off\nquick_abort_min 0 KB\nquick_abort_max 0 KB\nquick_abort_pct 95\nclient_persistent_connections off\nserver_persistent_connections off\n################\n\n#\n# Recommended minimum Access Permission configuration:\n#\n# Deny requests to certain unsafe ports\nhttp_access deny !Safe_ports\n\n# Deny CONNECT to other than secure SSL ports\nhttp_access deny CONNECT !SSL_ports\n\n# Only allow cachemgr access from localhost\nhttp_access allow localhost manager\nhttp_access deny manager\n\n# We strongly recommend the following be uncommented to protect innocent\n# web applications running on the proxy server who think the only\n# one who can access services on \u0026#34;localhost\u0026#34; is a local user\n#http_access deny to_localhost\n\n#\n# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS\n#\n\n# Example rule allowing access from your local networks.\n# Adapt localnet in the ACL section to list your (internal) IP networks\n# from where browsing should be allowed\nhttp_access allow localnet\nhttp_access allow localhost\n\n# And finally deny all other access to this proxy\nhttp_access deny all\n\n# Squid normally listens to port 3128\nhttp_port 8081\n\n# Uncomment and adjust the following to add a disk cache directory.\n#cache_dir ufs /var/spool/squid 100 16 256\n\n# Leave coredumps in the first cache dir\ncoredump_dir /var/spool/squid\n\n#\n# Add any of your own refresh_pattern entries above these.\n#\nrefresh_pattern ^ftp:\t\t1440\t20%\t10080\nrefresh_pattern ^gopher:\t1440\t0%\t1440\nrefresh_pattern -i (/cgi-bin/|\\?) 0\t0%\t0\nrefresh_pattern .\t\t0\t20%\t4320\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"second-server-configuration\"\u003eSecond Server configuration\u003c/h2\u003e\n\u003cp\u003eThe configs for the second proxy server located outside the country is much easier. We just have to tell Squid to accept requests from our other server. You can add the following lines to the squid configuration:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eacl child_proxy src 0.0.0.0/32\nhttp_access allow child_proxy\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e0.0.0.0 is the IP of the first server.\u003c/p\u003e\n\u003cp\u003eAfter doing this you should be set. Check your connections and make sure all needed ports are open on your firewalls on both servers.\u003c/p\u003e\n",
        "url": "https://workingtitle.pro/posts/dual-proxy-server-with-squid/",
        "date_published": "31056-31-09T50:3131:00+00:00",
        "date_modified": "31056-31-09T50:3131:00+00:00",
        "author": {
          "name": "Calvin Tran",
          "url": "https://canhtran.me/"
        }
      }
      
    ]
  }