<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>$ root@workingtitle.pro</title>
    <description>A minimal hugo theme focus on content</description>
    <link>https://workingtitle.pro</link>
    
    <language>en</language>
    <copyright>Copyright 252525, Calvin Tran</copyright>
    <lastBuildDate>Sat, 19 Aug 2023 11:14:50 +0330</lastBuildDate>
    <generator>Hugo - gohugo.io</generator>
    <docs>http://cyber.harvard.edu/rss/rss.html</docs>
    <atom:link href="https://workingtitle.pro/atom.xml" rel="self" type="application/atom+xml"/>
    
    <item>
      <title>Configure Harbor proxy cache for pulling images from Docker Hub</title>
      <link>https://workingtitle.pro/posts/configure-proxy-cache-harbor/</link>
      <description>&lt;p&gt;Due to severe sanctions restrictions, I&amp;rsquo;ve been having lots of trouble pulling images from Docker Hub. A great method of getting around this issue is to setup proxy cache on Harbor image registry. It can pull images from Docker Hub and cache them so for the next use, you&amp;rsquo;ll end up pulling from your local repo instead of Docker Hub.&lt;/p&gt;
&lt;p&gt;In order to do this, I needed to upgrade Harbor to the latest version. You can do that by following the &lt;a href=&#34;https://goharbor.io/docs/2.8.0/administration/upgrade/&#34;&gt;official docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After you&amp;rsquo;ve done that, you can follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;On the Harbor dashboard navigate to &lt;strong&gt;Administration &amp;gt; Registries &amp;gt; New Endpoint&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select &lt;strong&gt;Docker Hub&lt;/strong&gt; as the provider, give it a name and then test your connection to Docker Hub.
&lt;img
  src=&#34;https://workingtitle.pro/images/new-endpoint-harbor.png&#34;
  alt=&#34;new-endpoint-harbor&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click on &lt;strong&gt;Test connection&lt;/strong&gt; to check your connection. In my case, I had to configure an HTTP proxy in &lt;code&gt;harbor.yaml&lt;/code&gt; file since my server does not have direct access to internet. &lt;em&gt;(Side note to all my Iranian readers: Use &amp;ldquo;shecan&amp;rdquo; as your DNS)&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After saving, you should have a working endpoint like this:
&lt;img
  src=&#34;https://workingtitle.pro/images/endpoint-configured-harbor.png&#34;
  alt=&#34;endpoint-configured-harbor&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Navigate to &lt;strong&gt;Projects &amp;gt; New Project&lt;/strong&gt; and create a project named &lt;code&gt;proxy_cache&lt;/code&gt;. Make sure to enable the proxy cache option and point it to the registry you just created. As so:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/proxy-cache-project-harbor.png&#34;
  alt=&#34;proxy-cache-project-harbor&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;p&gt;And we&amp;rsquo;re done!&lt;/p&gt;
&lt;p&gt;Now when you want to pull an image, you should do so in this format:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker pull yourRepoAddress.com/proxy_cache/&amp;lt;image_name&amp;gt;:&amp;lt;image_tag&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In my example, it looks like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker pull reg.zcore.local/proxy_cache/nginx:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the image is already present, it will download it from the cache. Otherwise, it will pull it from Docker Hub. It will also always check and if there is a new version available on Docker Hub, it will try to download that.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/configure-proxy-cache-harbor/</guid>
      <pubDate>Sat, 19 Aug 2023 11:14:50 +0330</pubDate>
    </item>
    
    <item>
      <title>Graylog on Docker — Part 3: Graylog! </title>
      <link>https://workingtitle.pro/posts/graylog-on-docker-part-3/</link>
      <description>&lt;p&gt;Now that we have a working &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-1/&#34;&gt;Elasticsearch cluster&lt;/a&gt; and a &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-2/&#34;&gt;MongoDB replica set&lt;/a&gt;, we can move to the final piece of the puzzle — the Graylog cluster!&lt;/p&gt;
&lt;p&gt;We will deploy 3 Graylog containers, one of which be will denoted as &amp;ldquo;master&amp;rdquo;, using the &lt;code&gt;is_master = true&lt;/code&gt; parameter in the configuration file. The other two containers will be worker nodes and will have identical configurations.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s our master configuration file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;is_master = true
node_id_file = /usr/share/graylog/node-id
password_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM
root_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e
root_timezone = Asia/Tehran
http_bind_address = 0.0.0.0:9000
elasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203
rotation_strategy = count
elasticsearch_max_docs_per_index = 20000000
elasticsearch_max_number_of_indices = 20
retention_strategy = delete
elasticsearch_shards = 4
elasticsearch_replicas = 3
elasticsearch_index_prefix = graylog
allow_leading_wildcard_searches = false
allow_highlighting = false
elasticsearch_analyzer = standard
output_batch_size = 500
output_flush_interval = 1
output_fault_count_threshold = 5
output_fault_penalty_seconds = 30
processbuffer_processors = 5
outputbuffer_processors = 3
processor_wait_strategy = blocking
ring_size = 65536
inputbuffer_ring_size = 65536
inputbuffer_processors = 2
inputbuffer_wait_strategy = blocking
message_journal_enabled = true
lb_recognition_period_seconds = 3
mongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs
mongodb_max_connections = 1000
mongodb_threads_allowed_to_block_multiplier = 5
proxied_requests_thread_pool_size = 32
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And this will be our slave configuration file, which is identical to master with the difference that &lt;code&gt;is_master&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;is_master = false
node_id_file = /usr/share/graylog/node-id
password_secret = YmojUZtpNEXM9c9ztbrCrfKEcYHhHj3RmRADpR7kYwHE2Tybg5fFWYAgdAsPvivJC2qkjCJonDqmnRiFeRsQM
root_password_sha2 = 4faeec746f8ea72b8d89c91c8122acb828432f8c145bff35c4f3466477d0ec6e
root_timezone = Asia/Tehran
http_bind_address = 0.0.0.0:9000
elasticsearch_hosts = http://es01.example.net:9201,http://es02.example.net:9202,http://es03.example.net:9203
rotation_strategy = count
elasticsearch_max_docs_per_index = 20000000
elasticsearch_max_number_of_indices = 20
retention_strategy = delete
elasticsearch_shards = 4
elasticsearch_replicas = 3
elasticsearch_index_prefix = graylog
allow_leading_wildcard_searches = false
allow_highlighting = false
elasticsearch_analyzer = standard
output_batch_size = 500
output_flush_interval = 1
output_fault_count_threshold = 5
output_fault_penalty_seconds = 30
processbuffer_processors = 5
outputbuffer_processors = 3
processor_wait_strategy = blocking
ring_size = 65536
inputbuffer_ring_size = 65536
inputbuffer_processors = 2
inputbuffer_wait_strategy = blocking
message_journal_enabled = true
lb_recognition_period_seconds = 3
mongodb_uri = mongodb://mongo-cluster:27017,mongo-cluster2:27018,mongo-cluster3:27019/graylog?replicaSet=dbrs
mongodb_max_connections = 1000
mongodb_threads_allowed_to_block_multiplier = 5
proxied_requests_thread_pool_size = 32
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then we have our final docker compose file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;version: &amp;#34;3.8&amp;#34;

services:
  graylog-1:
    container_name: graylog-1-master
    image: graylog:5.0.6    
    volumes: 
      - ./graylog-config/master/:/usr/share/graylog/data/config/
    networks:
      - my-overlay-2
    ports:
      - 9000:9000 # Graylog web interface and REST API
      - 1514:1514 # Syslog TCP
      - 1514:1514/udp # Syslog UDP
      - 12201:12201 # GELF TCP
      - 12201:12201/udp # GELF UDP
      - 5045:5044 #Logstash port
    extra_hosts:
      - &amp;#34;es01.example.net:172.17.93.170&amp;#34;
      - &amp;#34;es02.example.net:172.17.93.171&amp;#34;
      - &amp;#34;es03.example.net:172.17.93.172&amp;#34;
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.type == master
      replicas: 1
    entrypoint: [ &amp;#34;/docker-entrypoint.sh&amp;#34; ]

  graylog-2:
    container_name: graylog-2
    image: graylog:5.0.6
    volumes:
      - /opt/graylog-config/slave/:/usr/share/graylog/data/config/
    networks:
      - my-overlay-2
    ports:
      - 9001:9000 # Graylog web interface and REST API
      - 1515:1514 # Syslog TCP
      - 1515:1514/udp # Syslog UDP
      - 12202:12201 # GELF TCP
      - 12202:12201/udp # GELF UDP
      - 5044:5044 #Logstash port
    extra_hosts: 
      - &amp;#34;es01.example.net:172.17.93.170&amp;#34; 
      - &amp;#34;es02.example.net:172.17.93.171&amp;#34;
      - &amp;#34;es03.example.net:172.17.93.172&amp;#34;
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.type == worker-1
      replicas: 1
    entrypoint: [ &amp;#34;/docker-entrypoint.sh&amp;#34; ]

  graylog-3:
    container_name: graylog-3
    image: graylog:5.0.6
    volumes:
      - /opt/graylog-config/slave/:/usr/share/graylog/data/config/
    networks:
      - my-overlay-2
    ports:
      - 9002:9000 # Graylog web interface and REST API
      - 1516:1514 # Syslog TCP
      - 1516:1514/udp # Syslog UDP
      - 12203:12201 # GELF TCP
      - 12203:12201/udp # GELF UDP
      - 5046:5044 #Logstash port
    extra_hosts: 
      - &amp;#34;es01.example.net:172.17.93.170&amp;#34; 
      - &amp;#34;es02.example.net:172.17.93.171&amp;#34;
      - &amp;#34;es03.example.net:172.17.93.172&amp;#34;
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.labels.type == worker-2    
      replicas: 1
    entrypoint: [ &amp;#34;/docker-entrypoint.sh&amp;#34; ]
networks:
  my-overlay-2:
    external: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here&amp;rsquo;s the breakdown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we&amp;rsquo;re setting up 3 instances of Graylog, one master and two slaves.&lt;/li&gt;
&lt;li&gt;The master node gets its own configuration file via volumes and the slaves get their own.&lt;/li&gt;
&lt;li&gt;We use placement constraints to place containers on specific nodes, just as we did for the Elasticsearch cluster and MongoDB cluster. This ensures we have the highest level of availability if one of the nodes in our Docker Swarm goes down.&lt;/li&gt;
&lt;li&gt;We use &lt;code&gt;extra_hosts&lt;/code&gt; to specify the IP address of the nodes where Graylog can access elasticsearch. The addresses specified such as &lt;code&gt;es01.example.net&lt;/code&gt; is configured as the elasticsearch node in the Graylog configuration file mentioned before.&lt;/li&gt;
&lt;li&gt;Please note that all of our containers in the cluster (Elastic, Mongo, Graylog) use a single shared overlay network (&lt;code&gt;my-overlay-2&lt;/code&gt;) so they can access each other.&lt;/li&gt;
&lt;/ul&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/graylog-on-docker-part-3/</guid>
      <pubDate>Sun, 30 Jul 2023 17:31:00 +0330</pubDate>
    </item>
    
    <item>
      <title>Graylog on Docker — Part 2: MongoDB  </title>
      <link>https://workingtitle.pro/posts/graylog-on-docker-part-2/</link>
      <description>&lt;p&gt;Now that we&amp;rsquo;ve setup our Elasticsearch cluster in &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-1/&#34;&gt;part one&lt;/a&gt;, we can move to the second stage which is setting up a MongoDB replica set.&lt;/p&gt;
&lt;h2 id=&#34;configure-mongodb-replica-set&#34;&gt;Configure MongoDB replica set&lt;/h2&gt;
&lt;p&gt;On our master machine (docker swarm), we will be add a new docker compose which will be used for the MongoDB deployment.&lt;/p&gt;
&lt;h3 id=&#34;mongoyaml&#34;&gt;mongo.yaml&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;version: &amp;#34;3.8&amp;#34;

services:
  mongo-1:
    image: mongo
    volumes:
      - /data/mongo/mongo1:/data/db
    command: &amp;#39;mongod --oplogSize 128 --replSet dbrs&amp;#39;
    networks:
      - my-overlay-2
    ports:
      - 27017:27017
    deploy:
      placement:
        constraints:
          - node.labels.type == master
      replicas: 1
      restart_policy:
        condition: on-failure
  mongo-2:
    image: mongo
    volumes:
      - /data/mongo/mongo2:/data/db
    command: &amp;#39;mongod --oplogSize 128 --replSet dbrs&amp;#39;
    networks:
      - my-overlay-2
    ports:
      - 27018:27017
    deploy:
      placement:
        constraints:
          - node.labels.type == worker-1
      replicas: 1
      restart_policy:
        condition: on-failure
  mongo-3:
    image: mongo
    volumes:
      - /data/mongo/mongo3:/data/db
    command: &amp;#39;mongod --oplogSize 128 --replSet dbrs&amp;#39;
    networks:
      - my-overlay-2
    ports:
      - 27019:27017
    deploy:
      placement:
        constraints:
          - node.labels.type == worker-2
      replicas: 1
      restart_policy:
        condition: on-failure
volumes:
  mongodb:
    driver: &amp;#34;local&amp;#34;
networks:
  my-overlay-2:
    external: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let&amp;rsquo;s walk through it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We create 3 containers &lt;code&gt;mongo-1&lt;/code&gt;,&lt;code&gt;mongo-2&lt;/code&gt; and &lt;code&gt;mongo-3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Each container is always placed on a different node using &lt;code&gt;constraints&lt;/code&gt;. I&amp;rsquo;ve configured my master node with the &lt;code&gt;master&lt;/code&gt; label, and each of the worker nodes with a &lt;code&gt;worker&lt;/code&gt; label.&lt;/li&gt;
&lt;li&gt;MongoDB files are stored under the &lt;code&gt;/data/mongo&lt;/code&gt; directory on each node. I&amp;rsquo;ve used NFS to mount this directory on a different server for larger storage but you can manually create these directories or each node, or change them entirely based on your needs.&lt;/li&gt;
&lt;li&gt;The containers use the same network &lt;code&gt;my-overlay-2&lt;/code&gt; as the rest of the containers described on &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-1/&#34;&gt;Part 1&lt;/a&gt;. This is crucial since we need all the components of the Graylog cluster (Elasticsearch, MongoDB, Graylog) to talk to each other.&lt;/li&gt;
&lt;li&gt;The command &lt;code&gt;mongod --oplogSize 128 --replSet dbrs&lt;/code&gt; tells our mongo instances that we want to initiate a replica set called &lt;code&gt;dbrs&lt;/code&gt;. You can change this to your desired name.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve used different host ports for each container (&lt;code&gt;27017&lt;/code&gt;, &lt;code&gt;27018&lt;/code&gt;, &lt;code&gt;27019&lt;/code&gt;) since initially I didn&amp;rsquo;t plan to place each container on a different node. If you&amp;rsquo;re planning to omit the constrains so the containers can be placed on any node, you should keep this port configuration. Otherwise, you can switch to &lt;code&gt;27017:27017&lt;/code&gt; for all the containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;start-the-stack&#34;&gt;Start the Stack&lt;/h2&gt;
&lt;p&gt;That&amp;rsquo;s pretty much it. Once you&amp;rsquo;ve made the changes to the compose file, save it somewhere on the master node and run it:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker stack deploy -c mongo.yaml mongo
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;initialize-the-replica-set&#34;&gt;Initialize the replica set&lt;/h2&gt;
&lt;p&gt;Once your containers are up and running, you should check the logs to make sure everything is okay. You can do so by this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker logs -f &amp;lt;container_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If everything is done correctly, you should see something like &lt;code&gt;waiting to Initialize ...&lt;/code&gt; in the logs.  If that&amp;rsquo;s the case, you need to run this command from a machine that has access to your cluster:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mongosh --host &amp;#39;172.17.93.171:27017&amp;#39; --eval &amp;#39;rs.initiate({ _id: &amp;#34;dbrs&amp;#34;, members: [{ _id: 0, host : &amp;#34;172.17.93.171:27017&amp;#34; }, { _id: 1, host : &amp;#34;172.17.93.172:27018&amp;#34; }, { _id: 2, host : &amp;#34;172.17.93.170:27019&amp;#34; }]})&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;You should have &lt;code&gt;mongosh&lt;/code&gt; installed on the node that&amp;rsquo;s running this.&lt;/li&gt;
&lt;li&gt;Replace the IPs with your docker swarm node IPs.&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;ve changed the replica set name in the docker-compose file, make sure to change it from &lt;code&gt;dbrs&lt;/code&gt; to your name.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once this command is executed, you should see a message regarding the successful replica set initialization.&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s it! You now have a working MongoDB replica set on docker swarm! 🥳🥳🥳&lt;/p&gt;
&lt;p&gt;In the next post, we&amp;rsquo;re going to finish up the cluster by deploying Graylog.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/graylog-on-docker-part-2/</guid>
      <pubDate>Sun, 09 Jul 2023 17:38:53 +0330</pubDate>
    </item>
    
    <item>
      <title>Graylog Deflector Problem</title>
      <link>https://workingtitle.pro/posts/graylog-deflector-problem/</link>
      <description>&lt;p&gt;So I&amp;rsquo;ve been setting up and testing a &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-1/&#34;&gt;Graylog multi-node&lt;/a&gt; setup, and I&amp;rsquo;ve come across an annoying problem. Sometimes for some unknown reason as Graylog starts, it creates an index called &lt;code&gt;graylog_deflector&lt;/code&gt; which is &lt;em&gt;supposed&lt;/em&gt; to point to the correct index; an alias of some sorts.&lt;/p&gt;
&lt;p&gt;But as it happens, &lt;a href=&#34;https://community.graylog.org/t/graylog-deflector-exists-as-an-indexer-and-is-not-an-alias/7413&#34;&gt;Graylog sometimes messes this up&lt;/a&gt; and ends up creating an actual index called &lt;code&gt;graylog_deflector&lt;/code&gt;. So when I would log into my Graylog UI, I&amp;rsquo;d see this error:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Elasticsearch exception [type=index_not_found_exception, reason=no such index []]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Graylog can&amp;rsquo;t create the index needed so it starts to complain.&lt;/p&gt;
&lt;p&gt;The solution is just to delete the &lt;code&gt;graylog_deflector&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First, stop your Graylog server instance.&lt;/li&gt;
&lt;li&gt;Delete the index (replace the IP and port with your own)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -X DELETE &amp;#34;172.17.93.170:9201/graylog_deflector?pretty&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Start your Graylog server again.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now when you log in, Graylog shouldn&amp;rsquo;t complain about the index again. If your issue isn&amp;rsquo;t solved, comment here and let me know.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/graylog-deflector-problem/</guid>
      <pubDate>Sun, 09 Jul 2023 16:31:24 +0330</pubDate>
    </item>
    
    <item>
      <title>Graylog on Docker — Part 1: Elasticsearch </title>
      <link>https://workingtitle.pro/posts/graylog-on-docker-part-1/</link>
      <description>&lt;p&gt;My goal is to setup a highly-available Graylog instance using multiple containers on 3 Virtual Machines.&lt;/p&gt;
&lt;p&gt;Our setup needs the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 Linux VMs running the latest version of docker&lt;/li&gt;
&lt;li&gt;Docker swarm configured &amp;amp; initiated on all servers (1 master, 2 workers)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What we&amp;rsquo;re going to do is deploy 3 containers for each component of the Graylog ecosystem. Each one of the these containers will be placed on one of our 3 VMs, giving us a high level of availability and redundancy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 Elasticsearch containers running as a elastic cluster&lt;/li&gt;
&lt;li&gt;3 MongoDB containers working as a ReplicaSet&lt;/li&gt;
&lt;li&gt;3 Graylog containers working in a master-worker system&lt;/li&gt;
&lt;li&gt;3 HAProxy containers responsible for load balancing the requests to the graylog containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;rsquo;ve tried to follow the rough design described by Graylog &lt;a href=&#34;https://go2docs.graylog.org/5-0/setting_up_graylog/multi-node_setup.html&#34;&gt;in their documentation&lt;/a&gt;, but I&amp;rsquo;ve taken some liberties with how I&amp;rsquo;ve approached it.&lt;/p&gt;
&lt;p&gt;Since Graylog has to be setup in a specific order (Elasticsearch 🠒 MongoDB 🠒 Graylog), we&amp;rsquo;re going to start part 1 with the Elasticsearch setup.&lt;/p&gt;
&lt;h2 id=&#34;elasticsearch-cluster-on-docker&#34;&gt;Elasticsearch cluster on Docker&lt;/h2&gt;
&lt;p&gt;Keep in mind that before using this docker compose file, you should have already configured your Docker swarm nodes &amp;amp; setup its networking correctly so all the nodes can connect to each other.&lt;/p&gt;
&lt;h3 id=&#34;containers&#34;&gt;Containers&lt;/h3&gt;
&lt;p&gt;This docker compose file starts three instances of Elasticsearch called &lt;code&gt;es01&lt;/code&gt;, &lt;code&gt;es02&lt;/code&gt; and &lt;code&gt;es03&lt;/code&gt; and places them on different nodes in the swarm. Once the containers are created, it initiates the elasticsearch cluster called &lt;code&gt;es-docker-cluster&lt;/code&gt;. The value for &lt;code&gt;discovery.seed_hosts&lt;/code&gt; should always point to other containers in the cluster.&lt;/p&gt;
&lt;h3 id=&#34;volumes&#34;&gt;Volumes&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve setup NFS storage on each of the VMs and mounted it to &lt;code&gt;/data/elastic/&lt;/code&gt;. In this directory there are 3 subdirectories for each of the elasticsearch instances, so we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/data/elastic/es01&lt;/li&gt;
&lt;li&gt;/data/elastic/es02&lt;/li&gt;
&lt;li&gt;/data/elastic/es03&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should configure NFS or GlusterFS and specify your volumes for each container.&lt;/p&gt;
&lt;h3 id=&#34;networking&#34;&gt;Networking&lt;/h3&gt;
&lt;p&gt;The network I&amp;rsquo;ve used is a simple &lt;code&gt;overlay&lt;/code&gt; network which should be setup and configured before the cluster initialization. The &lt;code&gt;overlay&lt;/code&gt; network is part of Docker Swarm and it allows different nodes in the Swarm to talk to each other.&lt;/p&gt;
&lt;h3 id=&#34;constraints&#34;&gt;Constraints&lt;/h3&gt;
&lt;p&gt;Using &lt;code&gt;node.role&lt;/code&gt; constraint, we can specify exactly where the container should be placed. This is necessary for ensuring maximum availability and redundancy in case of one or two of the VMs crashing or going offline.&lt;/p&gt;
&lt;h2 id=&#34;starting-the-elasticsearch-cluster&#34;&gt;Starting the Elasticsearch cluster&lt;/h2&gt;
&lt;p&gt;Once everything is setup, copy the docker compose file below to a directory on your Docker Swarm master node and then start the cluster using this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker stack deploy -c elasticsearch.yaml elastic
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;elasticsearchyaml&#34;&gt;elasticsearch.yaml&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;version: &amp;#34;3.8&amp;#34;
services:
  es01:
    image: elasticsearch:7.5.2
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34;
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /data/elastic/es01:/usr/share/elasticsearch/data
    ports:
      - 9201:9200
    networks:
      - my-overlay-2
    deploy:
      placement:
        constraints: [node.role == manager]

  es02:
    image: elasticsearch:7.5.2
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34;
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /data/elastic/es02:/usr/share/elasticsearch/data
    ports:
      - 9202:9200
    networks:
      - my-overlay-2
    deploy:
      placement:
        constraints: [node.role == worker]

  es03:
    image: elasticsearch:7.5.2
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - &amp;#34;ES_JAVA_OPTS=-Xms512m -Xmx512m&amp;#34;
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /data/elastic/es03:/usr/share/elasticsearch/data
    ports:
      - 9203:9200
    networks:
      - my-overlay-2
    deploy:
      placement:
        constraints: [node.role == worker]


networks:
  my-overlay-2:
    external: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In &lt;a href=&#34;https://workingtitle.pro/posts/graylog-on-docker-part-2/&#34;&gt;Part 2&lt;/a&gt;, we&amp;rsquo;re going to be deploying a MongoDB replica set on Docker Swarm.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/graylog-on-docker-part-1/</guid>
      <pubDate>Sun, 02 Jul 2023 15:08:08 +0330</pubDate>
    </item>
    
    <item>
      <title>Permission Denied Problem with Tcpdump rotation</title>
      <link>https://workingtitle.pro/posts/rotate-tcpdump-capture-with--g-and--w-/</link>
      <description>&lt;p&gt;I needed to start a &lt;code&gt;tcpdump&lt;/code&gt; process which rotates the PCAP once every hour. Using the &lt;code&gt;-G&lt;/code&gt; option we can specify the number of seconds the process should run before rotation (in my case its 3600), and using &lt;code&gt;-W&lt;/code&gt; we can tell it how many PCAP files should be retained. So if I need the PCAPs of the last 2 days, I would use &lt;code&gt;-W 48&lt;/code&gt; since I&amp;rsquo;m rotating them once every hour.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s my command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tcpdump -G 3600 -W 48 -i ens1f1 -w /data/dp-pcap/srv12-ens1f1-%Y-%m-%d_%H.%M.%S.pcap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It&amp;rsquo;s all pretty straightforward, and it &lt;em&gt;should&lt;/em&gt; work. As you start the process, it works as expected but when its time to rotate the PCAP, it will throw a &lt;code&gt;permission denied&lt;/code&gt; error and terminates the processes. So what happens?&lt;/p&gt;
&lt;p&gt;When you first start capture, tcpdump starts the process with owner that you&amp;rsquo;re logged in with (in my case root), but once its time for rotation, it tries to write the output file to the directory using &lt;code&gt;tcpdump&lt;/code&gt; as the owner. Since it doesn&amp;rsquo;t have access to your write directory, it fails.&lt;/p&gt;
&lt;p&gt;Yeah&amp;hellip;it&amp;rsquo;s not fun to find out about this in a production environment where PCAPs are critical.&lt;/p&gt;
&lt;p&gt;So how to fix it?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Set world-write permissions to the directory you&amp;rsquo;re saving the files to. (in my case /data/dp-pcap)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Change the owner of the directory to &lt;code&gt;tcpdump&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I got mad so I did &lt;strong&gt;option 3&lt;/strong&gt; which was &lt;code&gt;chmod 777 dp-pcap/&lt;/code&gt;. Would not recommend.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/rotate-tcpdump-capture-with--g-and--w-/</guid>
      <pubDate>Thu, 27 Apr 2023 14:11:38 +0330</pubDate>
    </item>
    
    <item>
      <title>Create a MongoDB Replica Set Using Docker Compose</title>
      <link>https://workingtitle.pro/posts/create-a-mongodb-replicaset-using-docker-compose/</link>
      <description>&lt;p&gt;I&amp;rsquo;ve been trying to setup a Graylog multi-node cluster for testing purposes and for that I needed to create a mongoDB replica set. Using docker seemed like the most logical choice so here&amp;rsquo;s how I did it using Docker Compose.&lt;/p&gt;
&lt;p&gt;I created 3 MongoDB containers, exposed the relevant ports and started the &lt;code&gt;mongod&lt;/code&gt; process using the &lt;code&gt;--replSet&lt;/code&gt; option and then specifying my replica set name such as &lt;code&gt;rs01&lt;/code&gt;. Then you create your volumes for each MongoDB container.&lt;/p&gt;
&lt;p&gt;After that I use a bash script &lt;code&gt;rs-init.sh&lt;/code&gt; (placed under the scripts folder at the root of project) to initiate the replica set on &lt;code&gt;mongo1&lt;/code&gt; container.&lt;/p&gt;
&lt;p&gt;Finally we use a another bash script &lt;code&gt;StartReplicaSet.sh&lt;/code&gt; to stop and start the containers using docker compose.&lt;/p&gt;
&lt;p&gt;So here&amp;rsquo;s my full docker compose file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;version: &amp;#39;3.8&amp;#39;

services:
  mongo1:
    container_name: mongo1
    image: mongo
    volumes:
      - ./scripts/rs-init.sh:/scripts/rs-init.sh
      - /opt/mongo1/:/data/db
    networks:
      - mongo-network
    ports:
      - 27017:27017
    depends_on:
      - mongo2
      - mongo3
    links:
      - mongo2
      - mongo3
    restart: always
    entrypoint: [ &amp;#34;/usr/bin/mongod&amp;#34;, &amp;#34;--bind_ip_all&amp;#34;, &amp;#34;--replSet&amp;#34;, &amp;#34;rs01&amp;#34; ]

  mongo2:
    container_name: mongo2
    image: mongo
    volumes:
      - /opt/mongo2/:/data/db
    networks:
      - mongo-network
    ports:
      - 27018:27017
    restart: always
    entrypoint: [ &amp;#34;/usr/bin/mongod&amp;#34;, &amp;#34;--bind_ip_all&amp;#34;, &amp;#34;--replSet&amp;#34;, &amp;#34;rs01&amp;#34; ]

  mongo3:
    container_name: mongo3
    image: mongo
    volumes:
      - /opt/mongo3/:/data/db
    networks:
      - mongo-network
    ports:
      - 27019:27017
    restart: always
    entrypoint: [ &amp;#34;/usr/bin/mongod&amp;#34;, &amp;#34;--bind_ip_all&amp;#34;, &amp;#34;--replSet&amp;#34;, &amp;#34;rs01&amp;#34; ]

networks:
  mongo-network:
    name: mongo-network
    driver: bridge
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;rs-init.sh&lt;/code&gt;: This is the bash script which initiates the replica set:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DELAY=25

mongosh &amp;lt;&amp;lt;EOF
var config = {
    &amp;#34;_id&amp;#34;: &amp;#34;rs01&amp;#34;,
    &amp;#34;version&amp;#34;: 1,
    &amp;#34;members&amp;#34;: [
        {
            &amp;#34;_id&amp;#34;: 1,
            &amp;#34;host&amp;#34;: &amp;#34;mongo1:27017&amp;#34;,
            &amp;#34;priority&amp;#34;: 2
        },
        {
            &amp;#34;_id&amp;#34;: 2,
            &amp;#34;host&amp;#34;: &amp;#34;mongo2:27017&amp;#34;,
            &amp;#34;priority&amp;#34;: 1
        },
        {
            &amp;#34;_id&amp;#34;: 3,
            &amp;#34;host&amp;#34;: &amp;#34;mongo3:27017&amp;#34;,
            &amp;#34;priority&amp;#34;: 1
        }
    ]
};
rs.initiate(config, { force: true });
EOF

echo &amp;#34;====&amp;gt; Waiting for ${DELAY} seconds for replica set configuration to be applied&amp;#34;

sleep $DELAY
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;StartReplicaSet.sh&lt;/code&gt;: This is the final script that starts the replica set:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#!/bin/bash

DELAY=10

docker compose down
docker compose up -d

echo &amp;#34;====&amp;gt; Waiting for ${DELAY} seconds for containers to go up&amp;#34;
sleep $DELAY

docker exec mongo1 sh -c &amp;#39;chmod +x /scripts/rs-init.sh&amp;#39;
docker exec mongo1 /scripts/rs-init.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let&amp;rsquo;s walk through each step:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We start by executing the bash script &lt;code&gt;StartReplicaSet.sh&lt;/code&gt; which first stops previous containers and then starts the containers using &lt;code&gt;docker compose&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;It then waits for containers to start and get ready by using sleep in seconds. (specified by the &lt;code&gt;DELAY&lt;/code&gt; variable)&lt;/li&gt;
&lt;li&gt;After the containers are up, we make the &lt;code&gt;rs-init.sh&lt;/code&gt; script which is one mongo1 container executable and run it.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rs-init.sh&lt;/code&gt; on mongo1 then initiates the replica set, electing mongo1 as the primary since we&amp;rsquo;ve specified the priority&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And that&amp;rsquo;s pretty much it. The cluster should then be up and running. One last thing to note is that if you&amp;rsquo;re using an older version of MongoDB you might have to change the command &lt;code&gt;mongosh&lt;/code&gt; to &lt;code&gt;mongo&lt;/code&gt;&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/create-a-mongodb-replicaset-using-docker-compose/</guid>
      <pubDate>Sat, 15 Apr 2023 14:27:25 +0330</pubDate>
    </item>
    
    <item>
      <title>Install IPA Client on Debian 11</title>
      <link>https://workingtitle.pro/posts/install-ipa-client-on-debian/</link>
      <description>&lt;p&gt;Couple of months ago I was setting up an IPA server for our infrastructure using CentOS. That went pretty smoothly but when I started joining clients to the IPA server, I quickly found out that the &lt;a href=&#34;https://groups.google.com/g/linux.debian.project/c/0tZoaWBLtlg&#34;&gt;IPA client package was missing from Debian 11&lt;/a&gt; repositories due to some bugs. This was clearly not ideal since most of our infrastructure runs on Debian 11.&lt;/p&gt;
&lt;p&gt;My second solution was to use the &lt;a href=&#34;https://packages.debian.org/bullseye-backports/freeipa-client&#34;&gt;backports package&lt;/a&gt; from Debian, but that also failed on my machines due to some missing libraries that were present in Debian 10 but were removed from Debian 11.&lt;/p&gt;
&lt;p&gt;Next, I tried to manually install and configure each service (SSSD, LDAP, NSS, Kerberos, etc) but to no avail. I feel like doing the configuration manually is prone to a lot of mistakes. So then I decided to write a bash script for it. Using the script everything is configured manually on the client machine. It needs to have access to the IPA server via ssh public key so it can copy some data from it such as CA certificate keys. It most definitely has many bugs, but it seems to be working for me now and I&amp;rsquo;ve joined 40+ Debian 11 nodes using this script.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m sharing the link here in the hopes that some poor soul stuck on this issue finds it and can use it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ali-foroughi/ipa-client-install&#34;&gt;https://github.com/ali-foroughi/ipa-client-install&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you do end up using it, please let me know how it went. 😅&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s hoping that Debian finally gets their act together and fixes the package 🥂&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/install-ipa-client-on-debian/</guid>
      <pubDate>Wed, 05 Apr 2023 16:27:43 +0330</pubDate>
    </item>
    
    <item>
      <title>Updating Timezone Using Ansible</title>
      <link>https://workingtitle.pro/posts/updating-timezone-using-ansible/</link>
      <description>&lt;p&gt;Recently Iran has stopped observing day-light savings (DST) time, which has caused various problems for everyone in the IT field. All of our servers were showing the wrong time since the tzdata package wasn&amp;rsquo;t updated. I wrote this Ansible playbook to upgrade everything in an instance.&lt;/p&gt;
&lt;p&gt;This playbook is for upgrading the tzdata package on Debain-based OS, using APT package mananger.
If you don&amp;rsquo;t need an HTTP proxy for APT, simply remove the tasks related to it.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;- name: Upgrade tzdata package for correcting timezone
  hosts: all
  become: yes
  tasks:
    - name: Create a directory for apt proxy
      ansible.builtin.file:
        path: /etc/apt/apt.conf.d/
        state: directory
        mode: &amp;#39;0744&amp;#39;

    - name: Create a proxy file if it doesn&amp;#39;t exist
      ansible.builtin.file:
        path: /etc/apt/apt.conf.d/10proxy
        state: touch
        mode: &amp;#39;0744&amp;#39;

    - name: Edit /etc/apt/apt.conf.d/10proxy and insert http proxy IP
      lineinfile:
        path: /etc/apt/apt.conf.d/10proxy
        state: present
        line: Acquire::http { Proxy &amp;#34;http://172.17.93.162:3142&amp;#34;; }

    - name: Edit /etc/apt/apt.conf.d/10proxy and insert https proxy IP
      lineinfile:
        path: /etc/apt/apt.conf.d/10proxy
        state: present
        line: Acquire::https { Proxy &amp;#34;http://172.17.93.162:3142&amp;#34;; }

    - name: Upgrade tzdata
      ansible.builtin.apt:
        name: tzdata
        state: latest
        update_cache: yes
        update_cache_retries: 2
        only_upgrade: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;###UPDATE###&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;So it turns out some services like Syslog need to be restarted so they can read the correct time from the system. If you&amp;rsquo;re able to, I&amp;rsquo;d suggest to just go ahead and restart the server. Otherwise, you can just restart Syslog:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl restart syslog.socket
&lt;/code&gt;&lt;/pre&gt;</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/updating-timezone-using-ansible/</guid>
      <pubDate>Mon, 03 Apr 2023 13:55:59 +0330</pubDate>
    </item>
    
    <item>
      <title>Create a Highly Available Kubernetes Cluster From Scratch</title>
      <link>https://workingtitle.pro/posts/create-a-highly-available-kubernetes-cluster-from-scratch/</link>
      <description>&lt;p&gt;In this guide we’re looking to create a highly available Kubernetes cluster with multiple control plane nodes, loadbalancers and worker nodes.&lt;/p&gt;
&lt;p&gt;The architecture of this Kubernetes cluster ensures a good level of availability and reliability for use in a production environment, but it is by no means fail-safe.&lt;/p&gt;
&lt;p&gt;I’ve followed the recommendations from Kubernetes documentations which you can find &lt;a href=&#34;https://kubernetes.io/docs/home/&#34;&gt;here&lt;/a&gt;. All I’ve done is to present them in a curated manner.&lt;/p&gt;
&lt;h2 id=&#34;what-youll-need&#34;&gt;What you’ll need&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 Virtual machines for master nodes  running Debian or CentOS  with at least 2 GB of RAM and 2 CPU cores&lt;/li&gt;
&lt;li&gt;2 worker nodes running Debian or CentOs. It can be either VM’s or bare-metal servers. Use full bare metal servers if you have heavy workloads&lt;/li&gt;
&lt;li&gt;At least 2 virtual machines running Debian or CentOs for load balancing&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/kuber-arch.png&#34;
  alt=&#34;kuber-arch&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 separate master nodes (control planes) for redundancy&lt;/li&gt;
&lt;li&gt;the master nodes are connected via loadbalancer&lt;/li&gt;
&lt;li&gt;we’ll have at least 2 load balancing instance where they negotiate a virtual IP between the instances&lt;/li&gt;
&lt;li&gt;worker nodes connect to the loadbalancer and the loadbalancer distributes request between control plane nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;setting-up-the-load-balancers&#34;&gt;Setting up the Load balancers&lt;/h2&gt;
&lt;p&gt;We’ll be using HA proxy and Keepalived for the load balancing solution. I’ve followed &lt;a href=&#34;https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing&#34;&gt;this guide&lt;/a&gt; for reference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install HAProxy and Keepalived on both load balancing server&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt; apt install haproxy
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt install keepalived
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Edit /etc/keepalived/keepalived.conf and make the configurations&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;! /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}

vrrp_script check_apiserver {
  script &amp;#34;/etc/keepalived/check_apiserver.sh&amp;#34;
  interval 3
  weight -2
  fall 10
  rise 2
}

vrrp_instance VI_1 {
    state ${STATE}
    interface ${INTERFACE}
    virtual_router_id ${ROUTER_ID}
    priority ${PRIORITY}
    authentication {
        auth_type PASS
        auth_pass ${AUTH_PASS}
    }

    virtual_ipaddress {
        ${APISERVER_VIP}
    }

    track_script {
        check_apiserver
    }

}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Add the script for health checking &lt;code&gt;/etc/keepalived/check_apiserver.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#!/bin/sh
errorExit() {
    echo &amp;#34;*** $*&amp;#34; 1&amp;gt;&amp;amp;2
    exit 1
}

curl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit &amp;#34;Error GET https://localhost:${APISERVER_DEST_PORT}/&amp;#34;

if ip addr | grep -q ${APISERVER_VIP}; then

    curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit &amp;#34;Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/&amp;#34;

fi
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Edit &lt;code&gt;/etc/haproxy/haproxy.cfg&lt;/code&gt; and make the configurations based on the guide&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log /dev/log local0
    log /dev/log local1 notice
    daemon
#---------------------------------------------------------------------
# common defaults that all the &amp;#39;listen&amp;#39; and &amp;#39;backend&amp;#39; sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 1
    timeout http-request    10s
    timeout queue           20s
    timeout connect         5s
    timeout client          20s
    timeout server          20s
    timeout http-keep-alive 10s
    timeout check           10s
#---------------------------------------------------------------------
# apiserver frontend which proxys to the control plane nodes
#---------------------------------------------------------------------
frontend apiserver
    bind *:${APISERVER_DEST_PORT}
    mode tcp
    option tcplog
    default_backend apiserver
#---------------------------------------------------------------------
# round robin balancing for apiserver
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
        server ${HOST1_ID} ${HOST1_ADDRESS}:${APISERVER_SRC_PORT} check
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;The configuration on both servers can be identical except two parts in the keepalived configuration:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;state MASTER
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;state BACKUP
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;code&gt;MASTER&lt;/code&gt; state should be on the main load balancer node and the &lt;code&gt;BACKUP&lt;/code&gt;  state should be used on all others. You can have many &lt;code&gt;BACKUP&lt;/code&gt;  nodes with the same state.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;priority ${PRIORITY}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Should be &lt;b&gt;LOWER&lt;/b&gt; on the &lt;code&gt;MASTER&lt;/code&gt; server. For example you can configure priority 100 on the &lt;code&gt;MASTER&lt;/code&gt; server, priority 101 on the first &lt;code&gt;BACKUP&lt;/code&gt; server, priority 102 on the second &lt;code&gt;BACKUP&lt;/code&gt; server and so on.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;option httpchk GET /healthz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This option should probably be changed to /livez. Check your kube-apiserver configuration file and match it to this value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Once the configuration on both servers is done restart the services&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service haproxy restart
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service keepalived restart
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;setting-up-master-nodes&#34;&gt;Setting up master nodes&lt;/h2&gt;
&lt;br&gt;
&lt;h3 id=&#34;first-master-node-main-control-plane&#34;&gt;First master node (main control plane)&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;IMPORTANT NOTE:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;On Debian machines, you need to edit &lt;code&gt;/etc/default/grub&lt;/code&gt; and set &lt;code&gt;systemd.unified_cgroup_hierarchy=0&lt;/code&gt; as the value for &lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT&lt;/code&gt; as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;systemd.unified_cgroup_hierarchy=0&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then update grub:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;update-grub
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and reboot the server.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;yum update&lt;/code&gt; OR &lt;code&gt;apt update&lt;/code&gt; &amp;amp;&amp;amp; apt upgrade&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable SElinux (for CentOS)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nano  /etc/selinux/config
.
.
.
SELINUX=disabled
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Letting iptables see bridged traﬃc&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sysctl --system
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Set all hostnames in &lt;code&gt;/etc/hosts&lt;/code&gt; if you’re not using a DNS serve&lt;/li&gt;
&lt;li&gt;Turn off swap&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;swapoff -a
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Open /etc/fstab and comment out the section related to swap&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Install Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&#34;&gt;Install kubeadm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open ports with firewalld&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo firewall-cmd --zone=public --permanent --add-port=6443/tcp
sudo firewall-cmd --zone=public --permanent --add-port=2379-2381/tcp
sudo firewall-cmd --zone=public --permanent --add-port=10250/tcp
sudo firewall-cmd --zone=public --permanent --add-port=10257/tcp
sudo firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Configure native cgroups driver&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt;EOF{
  &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
  &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
  &amp;#34;log-opts&amp;#34;: {
    &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;
  },
  &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;,
  &amp;#34;storage-opts&amp;#34;: [
    &amp;#34;overlay2.override_kernel_check=true&amp;#34;
  ]
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Then apply the changes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl daemon-reload
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Pull kubeadm images&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeadm config images pull --kubernetes-version v1.24.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can specify the desired version with &lt;code&gt;&amp;ndash;kubernetes-version&lt;/code&gt;. It’s recommended for all nodes to have the same version so it’s better to manually pull the same version on each master node as to avoid confusion.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After successfully pulling the images, initialize the master node via this command:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeadm init  --apiserver-advertise-address=CLUSTER-ENDPOINT --control-plane-endpoint=cluster-endpoint --pod-network-cidr=10.244.0.0/16 --upload-certs --kubernetes-version v1.24.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;CLUSTER-ENDPOINT&lt;/code&gt; should point to the virtual IP of the loadbalancer. You can define it in &lt;code&gt;/etc/hosts&lt;/code&gt; if you’re not using a DNS server.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply flannel for cluster networking&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Check the status of pods via:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl  get pods -A
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Everything should be running normally.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Save the output from the kubeadm init command so it can be used for starting other master and worker nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;second-and-third-master-nodes&#34;&gt;Second and Third Master nodes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Follow all of the steps mentioned in the previous section and pull the kubeadm images with:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeadm config images pull --kubernetes-version v1.24.0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Join the second and third master nodes to the cluster via the output from the first master node.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Kubectl join … 
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;adding-worker-nodes-to-the-cluster&#34;&gt;Adding worker nodes to the cluster&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Login to your worker node(s)&lt;/li&gt;
&lt;li&gt;Follow the same steps from the master node installation and pull the kubeadm images with:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubeadm config images pull --kubernetes-version v1.24.0
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Open the following ports with iptables or other firewall&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;iptables -A INPUT -p tcp --dport 10250 -j ACCEPT
iptables -A INPUT -p tcp --dport 30000:35000 -j ACCEPT
iptables -A INPUT -p tcp --dport 10248 -j ACCEPT
iptables-save &amp;gt; /etc/iptables/rules.v4
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make sure DNS names are configured in &lt;code&gt;/etc/hosts&lt;/code&gt; and the nameservers are set in &lt;code&gt;/etc/resolv.conf&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use kubeadm join command with the token acquired from the first master node to join the server into the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, log into one of your master node (control plane) and run the following command to see all the joined nodes:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get nodes -A
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should see an output of all your nodes. (master + worker nodes)&lt;/p&gt;
&lt;p&gt;Hopefully this article helped you in learning how to create a highly available Kubernetes cluster.&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/&#34;&gt;Create a highly available Kubernetes cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/&#34;&gt;Creating clusters with kubeadm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/&#34;&gt;Installing kubeadm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/&#34;&gt;kubelet configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/52645473/coredns-fails-to-run-in-kubernetes-cluster&#34;&gt;Issues with coreDNS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://serverfault.com/questions/1055263/kube-apiserver-exits-while-control-plane-joining-the-ha-cluster&#34;&gt;edit KUBELET_NETWORK_ARGS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://linuxize.com/post/how-to-disable-selinux-on-centos-7/&#34;&gt;Disable selinux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://serverfault.com/questions/288648/disable-the-public-key-check-for-rpm-installation&#34;&gt;Disable GPG checking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/43794169/docker-change-cgroup-driver-to-systemd&#34;&gt;Define cgroups driver systemd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discuss.kubernetes.io/t/why-does-etcd-fail-with-debian-bullseye-kernel/19696&#34;&gt;kube-schedular fails on debain&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/create-a-highly-available-kubernetes-cluster-from-scratch/</guid>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>How to Recover a MySQL Database With frm and .ibd Files</title>
      <link>https://workingtitle.pro/posts/how-to-recover-a-mysql-database-with-frm-and-ibd-files/</link>
      <description>&lt;p&gt;In order to recover a MySQL database, you need to have access to the .frm and .ibd files. I won’t get into how to acquire them, but if you have them, you can follow these steps to recover your database. (at least partially)&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;what-youll-need&#34;&gt;What you’ll need&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;.frm and .ibd files related to your database&lt;/li&gt;
&lt;li&gt;A server running the same version of MySQL as the database you want to restore&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;recover-database-structure&#34;&gt;Recover database structure&lt;/h2&gt;
&lt;p&gt;First thing we need to do is recover the database structure by using &lt;code&gt;mysqlfrm&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://helpmanual.io/help/mysqlfrm/&#34;&gt;mysqlfrm&lt;/a&gt; is a utility use to read .frm files and create the database structure based on those files. Here’s the steps:&lt;/p&gt;
&lt;p&gt;Run mysqlfrm under the diagnostic mode to get the table structure.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mysqlfrm --diagnostic TABLE_NAME.frm &amp;gt; TABLE_NAME.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This table will create the database structure (the mysql CREATE comands) and store them in a text file for reference. If you look into the txt file you’ll see an output like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE TABLE `wp_users` (
  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, 
  `user_login` varchar(128) NOT NULL, 
  `user_pass` varchar(128) NOT NULL, 
  `user_nicename` varchar(128) NOT NULL, 
  `user_email` varchar(64) NOT NULL, 
  `user_url` varchar(128) NOT NULL, 
  `user_registered` datetime NOT NULL, 
  `user_activation_key` varchar(1020) NOT NULL, 
  `user_status` int(11) NOT NULL, 
  `display_name` varchar(1000) NOT NULL, 
PRIMARY KEY `PRIMARY` (`ID`),
KEY `user_login_key` (`user_login`),
KEY `user_nicename` (`user_nicename`),
KEY `user_email` (`user_email`)
) ENGINE=InnoDB
  ROW_FORMAT=compact;
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;create-the-database-on-mysql&#34;&gt;Create the database on MySQL&lt;/h2&gt;
&lt;p&gt;Log into your MySQL server and create the MySQL database where you want the files to be restored. Take note that your MySQL query for creating the database has to include the default character set and collate. Here’s how I made my database:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE DATABASE my_database DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;create-table-and-import-the-ibd-file&#34;&gt;Create table and import the .ibd file&lt;/h2&gt;
&lt;p&gt;Now we need to create the table using the .frm file that we extracted. Select the database and run your CREATE command that was included in the &lt;code&gt;TABLE_NAME.txt&lt;/code&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mysql &amp;gt; USE my_database;
mysql &amp;gt; CREATE TABLE `wp_users` (
  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT, 
  `user_login` varchar(128) NOT NULL, 
  `user_pass` varchar(128) NOT NULL, 
  `user_nicename` varchar(128) NOT NULL, 
  `user_email` varchar(64) NOT NULL, 
  `user_url` varchar(128) NOT NULL, 
  `user_registered` datetime NOT NULL, 
  `user_activation_key` varchar(1020) NOT NULL, 
  `user_status` int(11) NOT NULL, 
  `display_name` varchar(1000) NOT NULL, 
PRIMARY KEY `PRIMARY` (`ID`),
KEY `user_login_key` (`user_login`),
KEY `user_nicename` (`user_nicename`),
KEY `user_email` (`user_email`)
) ENGINE=InnoDB
  ROW_FORMAT=compact;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now we need to drop the .ibd file that’s created with the above command and replace it with the .ibd file we already have.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ALTER TABLE table_name DISCARD TABLESPACE;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Copy your own .ibd file to the directory where the database resides. On an ubuntu/debian server that directory would be:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/var/lib/mysql
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;now we need to import the new .ibd file for the table we just created:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ALTER TABLE table_name IMPORT TABLESPACE;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should see a message that the query was successfully executed.&lt;/p&gt;
&lt;p&gt;Repeat the same steps for every single table. you could write a bash script to automatically run the same tasks for each table.
&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;export-the-restored-database&#34;&gt;Export the restored database&lt;/h2&gt;
&lt;p&gt;After following the steps for each of the tables, your database should be back to its original state. (more or less)&lt;/p&gt;
&lt;p&gt;There might be some data corruption, but you’ll likely get back most of the content.&lt;/p&gt;
&lt;p&gt;Run the following command to export your MySQL database:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mysqldump my_database &amp;gt; database_name_export.sql
&lt;/code&gt;&lt;/pre&gt;</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/how-to-recover-a-mysql-database-with-frm-and-ibd-files/</guid>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Linux Log Management With Graylog</title>
      <link>https://workingtitle.pro/posts/linux-log-management-with-graylog/</link>
      <description>&lt;p&gt;I’ve been searching the web for a free and open-source log monitoring solution for Linux for a while now. I’ve tried everything from Nagios to Kibana. However I found out the best solution for me is Graylog. I basically just want to manage web server and mail server logs and get alerts if certain conditions are met. So Let’s take a look at Linux log management with Graylog.&lt;/p&gt;
&lt;p&gt;Although the official Graylog documentation is great, I’ve found out it has some shortcomings when it comes to explaining how to actually transfer the logs from your own servers to the intended master Graylog server, especially if you’ve never done this before (like me!). If you’re not familiar with Syslog for example, you’re gonna have a hard time configuration just by following the official docs. That’s why I wanted to document how I approached it which is very bare bones and simple.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;So here’s what we’re going to do. We’re planning to transfer all of our logs from a server to a Graylog node, using Rsyslog as the transfer protocol.&lt;/p&gt;
&lt;p&gt;For setting up a Graylog node we’re going to use a Ubuntu 20.04 virtual machine with 2 Gigabytes of ram and 2 CPU cores. Keep in mind that this is just for testing purposes. In a production environment, you’re going to need a much more powerful server, most likely dedicated.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;installing-graylog-on-ubuntu-2004&#34;&gt;Installing Graylog on Ubuntu 20.04&lt;/h2&gt;
&lt;p&gt;First we’re going to start by installing the prerequisite packages. Run the following commands:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
sudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you get a “package not found” error, you need to add the universe repositry as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo add-apt-repository universe
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade
sudo apt-get install apt-transport-https openjdk-8-jre-headless uuid-runtime pwgen
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;install-mongodb&#34;&gt;Install MongoDB&lt;/h2&gt;
&lt;p&gt;Next step is to install MongoDB via the following commands:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 9DA31620334BD75D9DCB49F368818C72E52529D4
echo &amp;#34;deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse&amp;#34; | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list
sudo apt-get update
sudo apt-get install -y mongodb-org
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After the installation is complete, enable it and verify its running correctly.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo systemctl enable mongod.service
sudo systemctl restart mongod.service
sudo systemctl --type=service --state=active | grep mongod
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;install-elasticsearch&#34;&gt;Install Elasticsearch&lt;/h2&gt;
&lt;p&gt;Graylog uses Elasticsearch 7 to manage the logs. You can install it via the following commands.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;wget -q https://artifacts.elastic.co/GPG-KEY-elasticsearch -O myKey
sudo apt-key add myKey
echo &amp;#34;deb https://artifacts.elastic.co/packages/oss-7.x/apt stable main&amp;#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install elasticsearch-oss
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then make the following changes to the to the configuration file.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ sudo tee -a /etc/elasticsearch/elasticsearch.yml &amp;gt; /dev/null &amp;lt;&amp;lt;EOT
cluster.name: graylog
action.auto_create_index: false
EOT
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;install-graylog&#34;&gt;Install Graylog&lt;/h2&gt;
&lt;p&gt;Finally, we can add the Graylog repository and install it using these commands.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ wget https://packages.graylog2.org/repo/packages/graylog-4.0-repository_latest.deb
$ sudo dpkg -i graylog-4.0-repository_latest.deb
$ sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install graylog-server graylog-enterprise-plugins graylog-integrations-plugins graylog-enterprise-integrations-plugins
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;edit-the-configuration-file&#34;&gt;Edit the configuration file&lt;/h2&gt;
&lt;p&gt;There are a couple of changes we need to make to the Graylog configuration file before we can start it. Open the config file using your favorite text editor:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo nano /etc/graylog/server/server.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here are the changes we need to make:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find the password_secret parameter. You can create a password for this section with this command: pwgen -N 1 -s 96
    This parameter is used when creating a cluster for Graylog so make sure you save the password.&lt;/li&gt;
&lt;li&gt;root_password_sha2 has your desired password and save it here. Use this command to hash your password:
    echo -n yourpassword | shasum -a 256
    Definitely save this password. You will need it to log into the control panel.&lt;/li&gt;
&lt;li&gt;I would suggest specifying the time zone as well with the root_timezone&lt;/li&gt;
&lt;li&gt;You can also choose the username for the panel with the root_username directive, but this is not required. The default username is admin&lt;/li&gt;
&lt;li&gt;If you want to have access to the control panel from other locations, you need to change the http_bind_address. Change it from 127.0.0.0 to the IP address of your server&lt;/li&gt;
&lt;h2 id=&#34;start-graylog&#34;&gt;Start Graylog&lt;/h2&gt;
&lt;p&gt;Finally, we’re ready to start the Graylog server. Run the following commands.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ sudo systemctl daemon-reload
$ sudo systemctl enable graylog-server.service
$ sudo systemctl start graylog-server.service
$ sudo systemctl --type=service --state=active | grep graylog
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;server-firewall-configuration&#34;&gt;Server Firewall configuration&lt;/h2&gt;
&lt;p&gt;There are at least two ports that need to be opened with this configuration. Depending on what protocol you use for transfer of logs , you might need to open the corresponding ports.&lt;/p&gt;
&lt;p&gt;On Ubuntu with the UFW firewall, you can open ports very easily like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ufw allow 9000/tcp
ufw allow 514/tcp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Port 514 is the Rsyslog port which will be specified in the configuration. Port 9000 is the web interface for Graylog which we will get to later. If you’re using UDP for the transfer of the files, change the rules to UDP instead of TCP&lt;/p&gt;
&lt;p&gt;Reload the firewall to apply the rules:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ufw reload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then check the configuration to make sure&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ ufw status

To                         Action      From
--                         ------      ----
9000/tcp                   ALLOW       Anywhere                  
22/tcp                     ALLOW       Anywhere                  
514/tcp                    ALLOW       Anywhere                  
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;client-configuration-with-rsyslog&#34;&gt;Client configuration with Rsyslog&lt;/h2&gt;
&lt;p&gt;Rsyslog is a protocol for managing logs on Linux servers, and its installed by default on most distributions including Ubuntu. Using Rsyslog, we can transfer all or some of logs of a server to another server like Graylog so we can perform analysis.&lt;/p&gt;
&lt;p&gt;Here’s how I configured a CentOS server to send all of its logs to the Graylog server we just configured.&lt;/p&gt;
&lt;p&gt;First, navigate to the Rsyslog configuration directory:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd /etc/rsyslog.d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Create a new configuration file and open it.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nano 01-client.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and copy the content below inside the file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## GTLS driver
$DefaultNetstreamDriver gtls
# Certificates
$DefaultNetstreamDriverCAFile /etc/ssl/rsyslog/CA.pem
$DefaultNetstreamDriverCertFile /etc/ssl/rsyslog/client-cert.pem
$DefaultNetstreamDriverKeyFile /etc/ssl/rsyslog/client-key.pem
# Auth mode
$ActionSendStreamDriverAuthMode x509/name
$ActionSendStreamDriverPermittedPeer [server-hostname]
# Only use TLS
$ActionSendStreamDriverMode 1
# Forward everything to destination server
*.* @@[server-IP]:514
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The key setting here is the SSL keys. It is highly encouraged to use SSL to transfer the logs across the network. If you don’t use SSL, the log will transfer in a clear-text format and can be readable by anyone who has access to either nodes, or has access to your network in transit. (e.g your ISP).&lt;/p&gt;
&lt;p&gt;Issue two SSL certificates for your hostname. One for your server and one for client. For example server.yourdomain.com and client.yourdomain.com. If your server and client on part of the same domain, you could use a single Wild Card certificate for both.&lt;/p&gt;
&lt;h2 id=&#34;start-rsyslog&#34;&gt;Start Rsyslog&lt;/h2&gt;
&lt;p&gt;After your save the configuration file, make sure to install the &lt;code&gt;rsyslog-gnutls&lt;/code&gt; package as well.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt install rsyslog-gnutls
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally restart Rsyslog to apply the configuration.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl restart rsyslog.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now to make sure everything is running smoothly, check the status with this command. If there are any errors, you should see them.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;journalctl -f -u rsyslog
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;graylog-web-interface-configuration&#34;&gt;Graylog web interface configuration&lt;/h2&gt;
&lt;p&gt;Now that everything is setup and running smoothly (hopefully), we can begin receiving logs in the Graylog interface.&lt;/p&gt;
&lt;p&gt;Navigate to the web interface by opening the server IP via port 9000 in the URL bar of your browser:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http://127.0.0.1:9000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you’ve defined your server IP in the configuration, replace it with 127.0.0.1&lt;/p&gt;
&lt;p&gt;If everything goes will you will be greeted with this page:
&lt;img
  src=&#34;https://workingtitle.pro/images/graylog-web-interface.png&#34;
  alt=&#34;graylog-web-interface&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;p&gt;Enter admin as the default username and the password you created in the configuration file under the root_password directive.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;adding-an-input-with-rsyslog&#34;&gt;Adding an Input with Rsyslog&lt;/h2&gt;
&lt;p&gt;So far we’ve configured our client to send its log to the Graylog server, but we need to add that as an input in order to process it within Graylog.&lt;/p&gt;
&lt;p&gt;After logging in the Graylog interface, go to the System tab and click on Inputs.&lt;/p&gt;
&lt;p&gt;From the drop down menu, click &amp;ldquo;Syslog TCP&amp;rdquo; and then click the Launch &amp;ldquo;New Input&amp;rdquo; button.&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/graylog-new-input.png&#34;
  alt=&#34;graylog-new-input&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first field is your Graylog node. Since we only have a single Graylog server, you don’t need to select anything.&lt;/li&gt;
&lt;li&gt;In the Title filed just select a name for your input – for example Rsyslog.&lt;/li&gt;
&lt;li&gt;You can leave the IP on 0.0.0.0 sine we want to listen to Rsyslog inputs over the public IP.&lt;/li&gt;
&lt;li&gt;The default port for Rsyslog is 514 so leave this field unchanged.&lt;/li&gt;
&lt;li&gt;Important Note: In order to use port 514, you need to run Gray log as the root user which is not recommended. If Graylog isn’t running as root, port 514 won’t likely work. You can change it to something else for example 5514. However. make sure that you specify the new port in your client Rsyslog configuration file.&lt;/li&gt;
&lt;li&gt;Leave the rest of the fields at default setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/graylog-new-input-part2.png&#34;
  alt=&#34;graylog-new-input-part2&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TLS cert file: The path to the certificate file for your server.&lt;/li&gt;
&lt;li&gt;TLS private key: The path to the private key.&lt;/li&gt;
&lt;li&gt;The TLS client Auth Trusted Certs: The path to the CA certificate. Make sure to include the root certificate in this file as well because Graylog can’t retrieve it from the web.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The rest of the configuration is up to you but only these mentioned parameters are crucial and needed.&lt;/p&gt;
&lt;p&gt;You’re not required to use *&lt;strong&gt;&lt;strong&gt;LINK&lt;/strong&gt;&lt;/strong&gt; SSL/TLS certificate, but if you don’t, all of your logs will be transferred in clear-text format which is far from ideal.&lt;/p&gt;
&lt;p&gt;After adding the input you should see this section with the input correctly running:&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/graylog-local-input.png&#34;
  alt=&#34;graylog-local-input&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;p&gt;Now navigate to the homepage of Graylog and you should see the logs coming in:&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;https://workingtitle.pro/images/graylog-dashboard.png&#34;
  alt=&#34;graylog-dashboard&#34;
  loading=&#34;lazy&#34;
  decoding=&#34;async&#34;
  class=&#34;full-width&#34;
/&gt;

&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So that’s all! We now have full graylog node running and recieving inputs from Rsyslog on the node. You can continue to add new servers. Just forward the logs using the same method to the Ryslog port from the client.&lt;/p&gt;
&lt;p&gt;Based on personal experience I’ve found Linux log managment with Graylog to be the best for my intentions and the easiest.&lt;/p&gt;
&lt;p&gt;There is range of graphs and alert systems that you can create in Graylog which is well beyond the scope of this post. I suggest checking out the &lt;a href=&#34;https://docs.graylog.org/en/4.0/&#34;&gt;official docs&lt;/a&gt; to get familiar with it, or just play around with the settings!&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/linux-log-management-with-graylog/</guid>
      <pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Homelab: LXC/LXD Hypervisor and using Linux Containers</title>
      <link>https://workingtitle.pro/posts/homelab-lxc-lxd-hypervisor-and-using-linux-containers/</link>
      <description>&lt;p&gt;I’m planning to set up an entire network on a home server for testing and practice purposes, using Linux containers. I’m looking to setup the follow services (more might be added in the future):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LXC/LXD host machine&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cobbler.github.io/&#34;&gt;Cobbler&lt;/a&gt; server (for automatic VM creation) or maybe &lt;a href=&#34;https://www.proxmox.com/en/&#34;&gt;Proxmox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DHCP&lt;/li&gt;
&lt;li&gt;DNS&lt;/li&gt;
&lt;li&gt;FTP and file storage&lt;/li&gt;
&lt;li&gt;NFS share&lt;/li&gt;
&lt;li&gt;Postgres database server&lt;/li&gt;
&lt;li&gt;Apache web server&lt;/li&gt;
&lt;li&gt;NGINX load balancing&lt;/li&gt;
&lt;li&gt;LDAP server for authentication and access management&lt;/li&gt;
&lt;li&gt;at least 10 clients running various Linux distributions&lt;/li&gt;
&lt;li&gt;Mail server using &lt;a href=&#34;https://www.exim.org/&#34;&gt;Exim&lt;/a&gt; (or &lt;a href=&#34;http://www.postfix.org/&#34;&gt;Postfix&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nagios.org/&#34;&gt;Nagios&lt;/a&gt; server for monitoring&lt;/li&gt;
&lt;li&gt;Syslog server (&lt;a href=&#34;https://www.elastic.co/kibana/&#34;&gt;kibana&lt;/a&gt; and &lt;a href=&#34;https://www.elastic.co/logstash/&#34;&gt;logstash&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;setting-up-lcxlxd-machine&#34;&gt;Setting up LCX/LXD machine&lt;/h2&gt;
&lt;p&gt;The first thing we need to do for setting up this network is to have a host machine for our containers. For all the services I mentioned in the last part we will be using Linux containers instead of full virtual machines. Because this network is only for testing and practice purposes, we don’t need large resource. I’m setting up this network on a host machine with only 8 GB of ram and a modest Intel core i5 CPU. Nothing crazy.&lt;/p&gt;
&lt;p&gt;Use &lt;a href=&#34;https://linuxcontainers.org/lxd/getting-started-cli/&#34;&gt;this&lt;/a&gt; guide to setup and install LXD. It’s recommended to use Ubuntu or Debian as your host server in this case but you could technically use other distributions as well, but in this case since we’re going to be using Proxmox as well, we’re going with the recommended Ubuntu option.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-new-lxd-container&#34;&gt;Creating a new LXD container&lt;/h2&gt;
&lt;p&gt;Using Linux containers is different from virtual machines because they can share resources among them such as kernel to reduce load on the host machine. (Check out &lt;a href=&#34;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-ksm&#34;&gt;KSM&lt;/a&gt;). This is great for our use case because we will be spinning up a lot of containers so we can run all the services on a modest host machine.&lt;/p&gt;
&lt;p&gt;The full guide on how to spin up and use LXD containers are in the official guide, but generally you can start up a container with a command like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;lxc launch ubuntu:20.04 TestServer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ubuntu is the operating system we want and 20.04 is the version of the server.&lt;/p&gt;
&lt;p&gt;If you want to see all the images available, you can run this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;lxc image list images:
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command will show all the images available on official repository. Once you setup a new container, the image will be downloaded and stored locally.&lt;/p&gt;
&lt;p&gt;You can search for all images of a specific distribution like so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;lxc image list images: debian
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Most of the useful commands for LXD/LXC can be found here.
Play around with different commands, create new containers, connect to them, start and stop them to get the hang of it.&lt;/p&gt;
&lt;p&gt;In the next post, we’re going to set up a Proxmox service to control these containers in a easier fashion, but it’s important to familiarize yourself with how they work at a command-line level.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/homelab-lxc-lxd-hypervisor-and-using-linux-containers/</guid>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Personal Cloud Storage Solution With Nextcloud</title>
      <link>https://workingtitle.pro/posts/personal-cloud-storage-solution-with-nextcloud/</link>
      <description>&lt;p&gt;Ever wanted to have your own personal cloud storage space? I did. After I noticed my (free) Google Drive space running out.&lt;/p&gt;
&lt;p&gt;Googling “Personal cloud” brings up bunch of results, such as ownCloud that I tried to get started with but I found it unnecessarily complicated.&lt;/p&gt;
&lt;p&gt;The next best choice for a personal cloud storage is NextCloud, which is a completely free and open-source solution for setting up your own cloud storage server.&lt;/p&gt;
&lt;p&gt;Here’s what you need to setup NextCloud as a personal cloud storage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Linux VPS, preferably running on Ubuntu 18.04 or Redhat 8. For this example I’m using CentOS 7 which is also fine.&lt;/li&gt;
&lt;li&gt;A valid IP&lt;/li&gt;
&lt;li&gt;Optional: DNS Server so you could set it up as a sub directory for your domain. For example: cloud.workingtitle.pro would be the address of your &lt;li&gt;server for the web interface.&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;Apache&lt;/li&gt;
&lt;li&gt;PHP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;getting-started-with-a-personal-cloud-storage&#34;&gt;Getting started with a personal cloud storage&lt;/h2&gt;
&lt;p&gt;Firstly download the installation file from NextCloud’s website. Link found here.&lt;/p&gt;
&lt;p&gt;The first thing you need to do is to go ahead and update your server. You can do so via this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo yum update
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Make sure that you have MySQL ready and have access for creating a database and giving permissions.&lt;/p&gt;
&lt;p&gt;You should also have Apache installed. If you don’t, install it via this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install httpd
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;yum-install-httpd&#34;&gt;yum install httpd&lt;/h2&gt;
&lt;p&gt;The next step is to install PHP on your server if you don’t already have it. To check if you have PHP installed or not run the following command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;php -v
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If PHP is installed, you’ll get output showing the version, if its not your server will be confused.&lt;/p&gt;
&lt;p&gt;NextCloud recommends PHP version 7.3 or 7.4. So if don’t have it, go ahead and install using the following commands.&lt;/p&gt;
&lt;p&gt;First add the EPEL repository to your server:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install epel-release yum-utils -y
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install http://rpms.remirepo.net/enterprise/remi-release-7.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now add PHP 7.4 to your repository:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum-config-manager --enable remi-php74
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And finally install PHP and some of its core modules:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum install php php-common php-opcache php-mcrypt php-cli php-gd php-curl php-mysql -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this PHP should be setup and ready to go.&lt;/p&gt;
&lt;p&gt;Now NextCloud requires bunch of different modules. The full list can be found here:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PHP module ctype
PHP module curl
PHP module dom
PHP module GD
PHP module hash (only on FreeBSD)
PHP module iconv
PHP module JSON
PHP module libxml (Linux package libxml2 must be &amp;gt;=2.7.0)
PHP module mbstring
PHP module openssl
PHP module posix
PHP module session
PHP module SimpleXML
PHP module XMLReader
PHP module XMLWriter
PHP module zip
PHP module zlib
PHP module memcached
PHP module imagick
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;All of these should be included in the source tar.gz file that we install later, but that wasn’t the case for me and I had to install some of these manually.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;apache-configuration&#34;&gt;Apache configuration&lt;/h2&gt;
&lt;p&gt;Create a file named &lt;code&gt;nextcloud.conf&lt;/code&gt; under the directory &lt;code&gt;/etc/httpd/conf.d/&lt;/code&gt; and add the following lines to the file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;VirtualHost *:80&amp;gt;
  DocumentRoot /var/www/nextcloud/
  ServerName  cloud.workingtitle.pro

  &amp;lt;Directory /var/www/nextcloud/&amp;gt;
    Require all granted
    AllowOverride All
    Options FollowSymLinks MultiViews

    &amp;lt;IfModule mod_dav.c&amp;gt;
      Dav off
    &amp;lt;/IfModule&amp;gt;

  &amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;DocumentRoot&lt;/code&gt; is the path fo the NextCloud files and it’s up to you where you want to place it. Just create a directory of your choice and the download the installation files and extract them within that diectory. Then specify the path within this &lt;code&gt;nextcloud.conf&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;You should also change the &lt;code&gt;ServerName&lt;/code&gt; section to the host name you want to set for your server. This is where the DNS server part I talked about comes into play. If you don’t know how to setup a DNS server, you can check out &lt;b&gt;my previous post&lt;/b&gt;.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;installing-ssl&#34;&gt;Installing SSL&lt;/h2&gt;
&lt;p&gt;The next step is to setup SSL for your domain. If you don’t know how, you can check my other guide about &lt;b&gt;setting up SSL on NGINX&lt;/b&gt;.&lt;/p&gt;
&lt;p&gt;An easy and fast way is to use &lt;a href=&#34;https://certbot.eff.org/&#34;&gt;Certbot&lt;/a&gt;. It helps you to setup a Let’s Encrypt SSL on your server that renews itself automatically. It’s very easy to install as it’s only a few clicks. It’s 2020, if you don’t have SSL then you should reevaluate your life as a system admin 🙂&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;installation-wizard&#34;&gt;Installation Wizard&lt;/h2&gt;
&lt;p&gt;Now we’re getting to the final stages. After you have configured Apache and installed SSL, restart it to make sure everything is okay.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl restart httpd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Open the URL in your browser to continue with the web installation.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http://localhost/nextcloud
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Type in the Administrator username and password of your choice. Then click on storage and database option to enter the database info.&lt;/p&gt;
&lt;p&gt;NextClould recommends to user MySQL as the database. So go on your server and and create a new database and user name and enter the information in the fields.&lt;/p&gt;
&lt;p&gt;Using SQLite is not recommended as it’s only intended for testing and educational purposes and not be used in a production environment.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;defining-trusted-domains&#34;&gt;Defining trusted domains&lt;/h2&gt;
&lt;p&gt;By default only the domain name that’s entered in the Apache configuration will be allowed to access NextCloud. If you want to add another domain or subdomain you will have to manually add it to the trusted domains list.&lt;/p&gt;
&lt;p&gt;Open the config.php file. The syntax for the trusted domains is like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;#39;trusted_domains&amp;#39; =&amp;gt;
  array (
   0 =&amp;gt; &amp;#39;localhost&amp;#39;,
   1 =&amp;gt; &amp;#39;cloud.workingtitle.pro&amp;#39;,
   2 =&amp;gt; &amp;#39;newDomain.workingtitle.pro&amp;#39;,
   3 =&amp;gt; &amp;#39;192.168.1.50&amp;#39;,
),
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this configuration you can access Nextclould via localhost address, as well as the hostnames and the IP 192.168.1.50. You can add and subtract from this list as you wish.&lt;/p&gt;
&lt;p&gt;We’re done now. Open up the URL in your browser and you should see the login page.&lt;/p&gt;
&lt;p&gt;For further information regarding user creation and file sharing management, I suggest checking out the &lt;a href=&#34;https://docs.nextcloud.com/server/20/admin_manual/index.html&#34;&gt;Official documentation&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/personal-cloud-storage-solution-with-nextcloud/</guid>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Simple squid proxy server with basic authentication</title>
      <link>https://workingtitle.pro/posts/simple-squid-proxy-server-with-basic-authentication/</link>
      <description>&lt;p&gt;In previous posts I talked about setting up a double-proxy server using Squid. In this guide I’m gonna walk you through setting up a simple proxy server using Squid, and apply a simple authentication method.&lt;/p&gt;
&lt;p&gt;The official &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;Squid&lt;/a&gt; documentation on this issue is very vague and all over the place and I couldn’t find a good and straightforward guide for it.&lt;/p&gt;
&lt;p&gt;So here we go.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;what-you-will-need&#34;&gt;What you will need&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Linux server (VPS). In this example I’m using a CentOS 7 machine but the steps should generally be the same on different distributions.&lt;/li&gt;
&lt;li&gt;A valid IP address&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;installing-squid&#34;&gt;Installing Squid&lt;/h2&gt;
&lt;p&gt;The first step is to install Squid on your machine. Use the following command on CentOS.&lt;/p&gt;
&lt;p&gt;First update your repositories via this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo yum -y update
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then install squid:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum -y install squid
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Start squid and enable it for system startup:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;csystemctl start squid
systemctl enable squid
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;squid-configuration&#34;&gt;Squid configuration&lt;/h2&gt;
&lt;h3 id=&#34;create-a-user-for-squid&#34;&gt;Create a User for Squid&lt;/h3&gt;
&lt;p&gt;The first thing you’ll need to do is to set up a username and password for connecting to the proxy server. The username information for Squid is stored in this file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nano /etc/squid/passwd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Create a new user with this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo htpasswd /etc/squid/passwd [username-here]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After entering the command a prompt shows up for defining new password. Enter your password and make sure to save this password because you will need it.&lt;/p&gt;
&lt;h3 id=&#34;edit-the-configuration-file&#34;&gt;Edit the configuration file&lt;/h3&gt;
&lt;p&gt;Now we need to make the changes to the main squid configuration file. Open the file with your favorite text editor:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nano /etc/squid/squid.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Leave the default configuration in place (you might need them later) but add the following lines to the beginning of the file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 2 hours
acl auth_users proxy_auth REQUIRED
http_access allow auth_users
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;/usr/lib64/squid/basic_ncsa_auth&lt;/code&gt; is the library we’re using for authentication and the accepted user list is found at &lt;code&gt;/etc/squid/passwd&lt;/code&gt; which we defined a user for early on.&lt;/p&gt;
&lt;p&gt;Next add the following line to configuration:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;acl localnet src 0.0.0.0/8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This tells Squid to accept connections from any IP. (After authentication). I added this because I want to be able to connect to the server from any location.&lt;/p&gt;
&lt;p&gt;Another important section is the port configuration. In my case, Squid only seemed to be listening on IPv6 which was not ideal. So in order to change it I had to edit this section:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http_port 3128
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to let Squid know you want it to listen on IPv4 add the IP 0.0.0.0 in front of it. As so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;http_port 0.0.0.0:3128
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this, save the configuration file and restart Squid:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service squid restart
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;firewall-configuration&#34;&gt;Firewall configuration&lt;/h2&gt;
&lt;p&gt;If you have firewall on your server, you’ll have to open the port 3128.&lt;/p&gt;
&lt;p&gt;Use the following command on CentOS:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo firewall-cmd --zone=external --permanent --add-port=3128/tcp
sudo firewall-cmd --zone=external --permanent --add-port=3128/udp
sudo firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you’re not using firewall-cmd, you’ll have to open the port using &lt;code&gt;iptables&lt;/code&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo iptables -A INPUT -p tcp --dport 3128 -j ACCEPT
sudo iptables -A INPUT -p udp --dport 3128 -j ACCEPT
service iptables save
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And that’s all. Everything is set now. Restart Squid and you should be good to go.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service squid restart
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you’re having trouble connecting, make sure to check the Squid logs at &lt;code&gt;/var/log/squid/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you have any questions you can leave a comment on this post or email me at &lt;a href=&#34;mailto:admin@workingtitle.pro&#34;&gt;admin@workingtitle.pro&lt;/a&gt;&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/simple-squid-proxy-server-with-basic-authentication/</guid>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Setting Up a DNS Server on CentOS From Scratch</title>
      <link>https://workingtitle.pro/posts/setting-up-a-dns-server-on-centos-from-scratch/</link>
      <description>&lt;p&gt;Setting up a DNS server with BIND is not a very difficult process, but most of the guides and walk-through I’ve read on the subject tend to make it more complicated than it has to be, or that their information is very outdated. The results are a group of confused system admins who get stuck in an endless loop of troubleshooting their damn DNS server!&lt;/p&gt;
&lt;p&gt;So my goal here is to help you set up your DNS server and get it running with minimal hassle.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;what-you-will-need-to-start&#34;&gt;What you will need to start&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A freshly-installed Linux server ready to go. In this example, I’m going to be using CentOS 7&lt;/li&gt;
&lt;li&gt;A valid public IP address&lt;/li&gt;
&lt;li&gt;A registered domain name, e.g workingtitle.pro. If you don’t already have a domain name, you can register one through &lt;a href=&#34;https://www.godaddy.com/it-it&#34;&gt;GoDaddy&lt;/a&gt; or &lt;a href=&#34;https://www.namecheap.com/&#34;&gt;Namecheap&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;domain-settings&#34;&gt;Domain settings&lt;/h2&gt;
&lt;p&gt;The first thing you’ll need to do is to create DNS records in your domain control panel. Login into your domain registry panel and create the nameserver records as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ns1.yourdomain.com     IP: [your-server-ip]
ns2.yourdomain.com     IP: [your-server-ip]
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;installing-bind-and-initial-configuration&#34;&gt;Installing BIND and initial configuration&lt;/h2&gt;
&lt;p&gt;The first step is to install BIND, which is the software that controls our DNS server. You can do so via this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;yum -y install bind bind-utils
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After the installation process is done, we’ll need to make our changes to the BIND configuration file in order to function correctly. In CentOS BIND is managed by a process called “named“. Open the configuration file with the text editor of your choice:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim /etc/named.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The first section we need to change is this part:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;listen-on port 53 { 127.0.0.1; };
listen-on-v6 port 53 { ::1; };
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However as it stands, the nameserver is only responding to requests from the server itself. (127.0.0.1). Therefore, if any computer on the internet tries to reach this server it will be rejected. We will need to change the value to any as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;listen-on port 53 { any; };
listen-on-v6 port 53 { any; };
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next, we need to allow the nameserver to accept query requests from any IP and to reject recursion. If Recursive DNS is active on a server, it will try to respond to DNS queries for domains that are NOT within your namesever. So technically, someone could ask your server for the IP of &lt;a href=&#34;https://www.google.com&#34;&gt;www.google.com&lt;/a&gt;. Obviously your server is not the authority for that, so it’s standard practice to disable recursion.
Make these changes as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;allow-query { any; };
recursion no;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In the next step we need to define zones for your domain. In the end of the /etc/named.conf file, add the following lines:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;zone &amp;#34;workingtitle.pro&amp;#34; IN {

         type master;
         file &amp;#34;/var/named/workingtitle.pro.db&amp;#34;;
};
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Replace “workingtitle.pro” with your own domain name. This section tells the users that the zone for the domain “workingtitle.pro” exists on this sever and the details can be found at /var/named/workingtitle.pro.db.&lt;/p&gt;
&lt;p&gt;That’s all the changes we need to make to the configuration file. The rest of the settings are optional and you can change them based on your need.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;creating-zone-files&#34;&gt;Creating Zone files&lt;/h2&gt;
&lt;p&gt;The next step is to create zone files for your domains. The zone files contain the DNS records, showing where each resource is located.&lt;/p&gt;
&lt;p&gt;Create the zone file for the domain by editing this file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim /var/named/workingtitle.pro.db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is the same directory we defined in the previous step. Copy and paste the content blew into the file.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$TTL 14400
workingtitle.pro.   IN  SOA     ns1.workingtitle.pro. hostmaster.workingtitle.pro. (
                                                1001    ;Serial
                                                4H      ;Refresh
                                                15M     ;Retry
                                                1W      ;Expire
                                                1D      ;Minimum TTL
                                                )

;Name Server Information
workingtitle.pro.	14400	IN	NS      ns1.workingtitle.pro.
workingtitle.pro.	14400	IN      NS      ns2.workingtitle.pro.

;IP address of Name Server
ns1	IN	A       8.8.8.8	;replace with your server public IP address
ns2	IN	A	8.8.8.8	;replace with your server public IP address

;Mail exchanger
workingtitle.pro.	IN  	MX 	10	mail.workingtitle.pro.

;A - Record HostName To IP Address
workingtitle.pro.     IN  A       8.8.8.8 ;replace with your server public IP address
www	IN	A	8.8.8.8	;replace with your server public IP address
mail	IN	A	8.8.8.8	;replace with your server public IP address

;CNAME record
ftp     IN	CNAME        workingtitle.pro.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let’s take a look at each section individually:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;;Name Server Information
workingtitle.pro.	14400	IN	NS      ns1.workingtitle.pro.
workingtitle.pro.	14400	IN      NS      ns2.workingtitle.pro.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here we’re defining our nameserver address so We’re essentially telling anyone who is querying our server that the nameservers for domain “workingtitle.pro” are “ns1.workingtitle.pro” and “ns2.workingtitle.pro”. You should obviously change this to your own domain name.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;;IP address of Name Server
ns1	IN	A       8.8.8.8	;replace with your server public IP address
ns2	IN	A	8.8.8.8	;replace with your server public IP address
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now we need to define the IP address of our nameservers. Here you can see that the IP for “ns1” and “ns2” has been defined. Replace 8.8.8.8 with your server public IP.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Important note&lt;/b&gt;: It is strongly advised to use at least two different servers with different IPs for your nameservers. This is to provide redundancy in case one of the nameservers fails, but in the spirit of keeping things simple, I’ve only used one server with a single IP.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;;A - Record HostName To IP Address
workingtitle.pro.     IN  A       8.8.8.8 ;replace with your server public IP address
www	IN	A	8.8.8.8	;replace with your server public IP address
mail	IN	A	8.8.8.8	;replace with your server public IP address

;CNAME record
ftp     IN	CNAME        workingtitle.pro.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this section we can set the main DNS records such as the A record for the domain, the IP of the mail server and other services.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;firewall-configuration&#34;&gt;Firewall configuration&lt;/h2&gt;
&lt;p&gt;We need to open port 53 on the server firewall in order to allow DNS traffic through. You can use the following command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;firewall-cmd --zone=external --permanent --add-port=53/udp
firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you’re not using &lt;code&gt;firewalld&lt;/code&gt;, you can use the following command to open the port with &lt;code&gt;iptables&lt;/code&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;iptables -A INPUT -p udp --dport 53 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;
&lt;h2 id=&#34;start-the-dns-service&#34;&gt;Start the DNS service&lt;/h2&gt;
&lt;p&gt;The configuration is done! We can go ahead and start the DNS service:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;systemctl enable named
systemctl start named
&lt;/code&gt;&lt;/pre&gt;</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/setting-up-a-dns-server-on-centos-from-scratch/</guid>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Install SSL on Adobe Connect</title>
      <link>https://workingtitle.pro/posts/install-ssl-on-adobe-connect/</link>
      <description>&lt;p&gt;Adobe Connect is a software primarily used for online classes and web conferencing. Installing SSL on the Connect services such as application and meeting are essential so we’re going to look at how to install SSL on Adobe Connect.&lt;/p&gt;
&lt;p&gt;I’m going to assume you already have Adobe Connect installed on your server. If not, you can you &lt;a href=&#34;https://helpx.adobe.com/adobe-connect/installconfigure/install-connect-using-installer.html&#34;&gt;this guide&lt;/a&gt; to install it.&lt;/p&gt;
&lt;p&gt;Now let’s start. Please note that this guide is for Adobe Connect version 9 or higher.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Before you start:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Make sure your SSL keys are ready. They should be in this format:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your main certificate key in .pem format. You might have received two separate files from your CA but you can combine these two to make a single file.&lt;/li&gt;
&lt;li&gt;Private key in .pem format.&lt;/li&gt;
&lt;li&gt;Remember not to use passphrase on your SSL keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-stunnel&#34;&gt;Install Stunnel&lt;/h2&gt;
&lt;p&gt;Now you need to install stunnel. It’s a program that adds SSL/TLS functionality to your website. Firstly, install it from here: &lt;a href=&#34;https://www.stunnel.org/downloads.html&#34;&gt;https://www.stunnel.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Secondly, open stunnel.exe and follow the installation process. It’s better to install in the C:\Connect\stunnel directory so it will be in the same directory as Adobe Connect and easier to troubleshoot.&lt;/p&gt;
&lt;h2 id=&#34;stunnel-configuration&#34;&gt;Stunnel configuration&lt;/h2&gt;
&lt;p&gt;Copy the code below to the stunnel.conf file located at C:\Connect\Stunnel\conf\ and remove all the previous code found in the file so we can add the new configuration. but before making any changes, make sure you make a copy of the stunnel.conf file.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;; Protocol version (all, SSLv2, SSLv3, TLSv1)
sslVersion = all
options = NO_SSLv2
options = NO_SSLv3
options = DONT_INSERT_EMPTY_FRAGMENTS
options = CIPHER_SERVER_PREFERENCE
renegotiation=no
fips = no
;Some performance tunings
socket = l:TCP_NODELAY=1
socket = r:TCP_NODELAY=1
TIMEOUTclose=0
; application server SSL / HTTPS
3[https-vip]
accept = 10.1.1.1:443
connect = 127.0.0.1:8443
cert = C:\Connect\stunnel\certs\public_certificate_app-server.pem
key = C:\Connect\stunnel\certs\private_key_app-server.key
;configure ciphers as per your requirement and client support.
;this should work for most:
ciphers = TLSv1+HIGH:!SSLv2:!aNULL:!eNULL:!3DES
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Remember to replace 10.1.1.1 with your server IP and In the “cert” and “key” directories you have to specifies the location of the certificate file and the private key respectively. These are the same keys with the .pem format I mentioned at the start.&lt;/p&gt;
&lt;p&gt;Now, we can check to see if the certificate is working correctly with our configurations before taking it live. Follow these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open Stunnel.exe located at /bin/ folder. After doing that an icon appears in the notification area.&lt;/li&gt;
&lt;li&gt;Right-click the icon and select “check configuration”&lt;/li&gt;
&lt;li&gt;If everything is working correctly you should see a message like this:
“2020.07.04 11:40:18 LOG5[main]: Configuration successful”&lt;/li&gt; 
&lt;/ul&gt;
&lt;p&gt;As the result of configurations being correct, we can go ahead and set up Stunnel as a service so you don’t need to manually start it every time you reboot the server.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate to the /bin/ directory via Windows Command Line (CMD)&lt;/li&gt;
&lt;li&gt;Type the following command: stunnel.exe -install&lt;/li&gt;
&lt;li&gt;In the Windows Services menu a new services will be created named Stunnel TLS Wrapper. Make sure that it’s set to automatic.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;customini&#34;&gt;Custom.ini&lt;/h2&gt;
&lt;p&gt;After making the changes to the configuration file, open the custom.ini file located at c:\Connect\9.x\&lt;/p&gt;
&lt;p&gt;Add the following lines to the end of custom.ini:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ADMIN_PROTOCOL=https://
SSL_ONLY=yes
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;serverxml&#34;&gt;Server.xml&lt;/h2&gt;
&lt;p&gt;Find this file and open it: &lt;i&gt;C:\Connect\9.x\appserv\conf\server.xml&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;You need to make two changes and &lt;b&gt;uncomment&lt;/b&gt; certain sections. First uncomment this part:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;Connector port=&amp;#34;8443&amp;#34; protocol=&amp;#34;HTTP/1.1&amp;#34;
    executor=&amp;#34;httpsThreadPool&amp;#34;
        enableLookups=&amp;#34;false&amp;#34;
        acceptCount=&amp;#34;250&amp;#34;
        connectionTimeout=&amp;#34;20000&amp;#34;
        SSLEnabled=&amp;#34;false&amp;#34;
        scheme=&amp;#34;https&amp;#34;
        secure=&amp;#34;true&amp;#34;
        proxyPort=&amp;#34;443&amp;#34;
        URIEncoding=&amp;#34;utf-8&amp;#34;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and then this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;Executor name=&amp;#34;httpsThreadPool&amp;#34;
    namePrefix=&amp;#34;https-8443-&amp;#34;
    maxThreads=&amp;#34;350&amp;#34;
    minSpareThreads=&amp;#34;25&amp;#34;/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And you’re done. You have followed all the steps to install ssl on Adobe Connect.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, Restart all related services, Adobe Connect, Adobe Media and Stunnel wrapper&lt;/li&gt;
&lt;li&gt;Make sure that port 443 is open on your firewall. If there is a need, write an additional rule to let the traffic through.&lt;/li&gt;
&lt;li&gt;If after restarting the services Adobe Connect doesn’t start and you see a “Not Ready” error, try restarting your MySQL server. That helped me.&lt;/li&gt;
&lt;li&gt;Note that this is a simple configuration for the Application only. You can find the full details in this &lt;a href=&#34;https://blogs.adobe.com/connectsupport/files/2016/04/Connect-SSL-Guide.pdf&#34;&gt;official guide&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/install-ssl-on-adobe-connect/</guid>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>What&#39;s the difference between SSL and TLS?</title>
      <link>https://workingtitle.pro/posts/whats-the-difference-between-ssl-tls/</link>
      <description>&lt;p&gt;SSL/TLS is a protocol for encrypting the communications between server and client. Netscape developed the first version of SSL in 1995 never released to the public because of major security flaws.&lt;/p&gt;
&lt;p&gt;The first version that was used by the general public was SSL 2.0 which was released in 1995 via RFC 6176. However in the following year, the protocol received a complete redesign and SSL version 3.0 was released in 1996.&lt;/p&gt;
&lt;p&gt;TLS or Transport Layer Security is just an updated version of SSL. So you could think of TLS as SSL 4.0. Because of some convoluted reasons however, a name change for the protocol occurred. Today when we say “SSL” we’re actually refereeing to TLS because the actual SSL protocol has long been deprecated. Hopefully no servers today are using the older versions of SSL.&lt;/p&gt;
&lt;p&gt;The latest version of TLS is 1.3. Version 1.0 and 1.1 are officially deprecated as of 2020, so it’s absolutely necessary for any servers that are using these protocols to immediately update to version 1.2 or 1.3.&lt;/p&gt;
&lt;h2 id=&#34;but-how-does-it-work&#34;&gt;But how does it work?&lt;/h2&gt;
&lt;p&gt;For encrypting the data between the client and server, TLS uses a mechanism called “handshake”. During a TLS handshake a number of actions need to occur in order to establish a secure connection between the server and the client:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pick the version of TLS used by the client and server ( 1.2 , 1.3 etc)&lt;/li&gt;
&lt;li&gt;ecide on which ciphers to use&lt;/li&gt;
&lt;li&gt;Verify the authenticity of the server via the SSL certificate and and public key (we’ll look at this later)&lt;/li&gt;
&lt;li&gt;And finally, create session keys or symmetric encryption after successfully completing the handshake&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During the TLS handshake, the server and client need to exchange variety of different information in order to set up a secure connection.&lt;/p&gt;
&lt;p&gt;RSA and Elliptic Curve are two of the most notable algorithms used for the client and server to exchange keys during the handshake. In this example we’re going to take a look at the most widely used algorithm: RSA&lt;/p&gt;
&lt;h2 id=&#34;how-a-tls-connection-works&#34;&gt;How a TLS connection works&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;b&gt;Client hello message:&lt;/b&gt; The client starts the process by sending a “hello” message to the server. This message usually contains the cipher suites and the TLS version which the client wants to use, and a string of random bytes.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Server hello message:&lt;/b&gt; The server replies to this message by sending the SSL certificate to the client (the public key). Along with this message, the server sends cipher suites and it’s own string of random characters.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Authentication:&lt;/b&gt; The client now has to verify the authenticity of the SSL certificate it received from the server by checking it against the data provided by the Certificate Authority (e.g GoDaddy, Comodo, Digicert, etc). This is done so by using the Certificate Authority root cert that’s usually embedded in most known programs and browsers. This step is essential so we can make sure we’re talking to the correct server and not some malicious server trying to lure us.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Client response:&lt;/b&gt; After authentication is complete, the client generates another set of random characters called the “premaster key”, encrypts it via the public key it received from the server and then sends it to the server. It’s important to note only the server can decrypt this message, because only the server has the required private key.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Session key creation:&lt;/b&gt; The client and server have 3 keys: the client random, the server random and the premaster key. Using these 3 string of bytes, both client and server create a session key. They both create a “finished” message, sign it with the new session key and send it over. Both client and server can now confirm they have the same session key.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;TLS connection established:&lt;/b&gt; We have established a secure connection. The client and server will now continue to communicate and encrypt the data via the session key.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So this is roughly the process that happens when you try to connect to a server or webpage that’s using SSL/TLS. So far that the technology allows, this is the best way we can secure connections between client and server.&lt;/p&gt;
&lt;p&gt;It would take a normal computer 3 trillion years to break a 2048-bit RSA key. So it’s safe to say it’s pretty secure as of now. But with the rise of quantum computing that we’re seeing, maybe in 10-20 years time, this will not be such a safe method of encryption. We’ll have to come up with new ways!&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/whats-the-difference-between-ssl-tls/</guid>
      <pubDate>Tue, 23 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Setting Up Python App on Virtualenv</title>
      <link>https://workingtitle.pro/posts/setting-up-python-app-on-virtualenv/</link>
      <description>&lt;p&gt;&lt;a href=&#34;https://virtualenv.pypa.io/en/latest/&#34;&gt;Virtualenv&lt;/a&gt; is a tool for creating isolated environments for running your python application. Most python programs require a variety of different libraries that are needed for running the applications on any machine. So If you don’t use a virtual environment, you will have to install all the libraries all the time. Overtime this will cause conflicts between different packages. So it’s always a good idea to isolate the environment for your python project. That’s why, most of the applications need to be written in a virtual environment to avoid problems. In this guide we’re going to look at how to set up Python app on virtualenv.&lt;/p&gt;
&lt;p&gt;Here’s what you need to do to create a virtual environment.&lt;/p&gt;
&lt;p&gt;First, make sure that you have Python installed. You can install the latest version of Python from &lt;a href=&#34;https://www.python.org/&#34;&gt;python.org&lt;/a&gt;. I recommend to use Python 3 which is the latest version with long-term support.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-virtual-environment&#34;&gt;Creating a virtual environment&lt;/h2&gt;
&lt;p&gt;Now we need to create a virtual environment path using this command:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;python3 -m venv /path/to/new/virtual/environment
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then we can use the source command to activate the virtual environment as so:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;source bin/activate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now your shell should look something like this:&lt;/p&gt;
&lt;img src=&#34;https://i.ibb.co/61T5MhZ/python01.png&#34;&gt;
&lt;p&gt;&lt;code&gt;new-test&lt;/code&gt; is the directory path we created in the previous step with the &lt;code&gt;python3 -m&lt;/code&gt; command. This shows that we’re currently inside the virtual environment for Python. Any package and library that you will install will only be accessible inside this virtual environment and will not have system-wide effects.&lt;/p&gt;
&lt;p&gt;For example we’re going to install the pygame which is a set of modules for creating games in python. You can install it with the pip tool:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip install pygame
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can connect to this virtual environment at anytime using the source command. Install all the packages and modules within this environment and run your application. I always recommend to run your python app within virtualenv to avoid environment problems in future.&lt;/p&gt;
&lt;p&gt;Official documentation on Python venv can be found &lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/setting-up-python-app-on-virtualenv/</guid>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Install and Configure SSL on Nginx</title>
      <link>https://workingtitle.pro/posts/install-and-configure-ssl-on-nginx/</link>
      <description>&lt;p&gt;NGINX is web server, reverse proxy and caching tool that is relatively new compared to Apache. It’s gaining popularity around the world due to its applications. Because of its load-balancing feature, it is widely used in websites with heavy traffic, the best example of which is YouTube. In this guide we’re going to look at how to install ssl on nginx&lt;/p&gt;
&lt;p&gt;I’ll walk you through installing an SSL certificate on Nginx. Its a fairly easy process even if you have had no prior experience with Nginx.&lt;/p&gt;
&lt;p&gt;I am going to assume you already have a valid SSL certificate, ready for installation. You can purchase a certificate from a variety of different sources such as Digicert or Comodo, but I would personally recommend using &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let’s Encrypt&lt;a/&gt; which is a non-profit SSL CA backed by the Linux Foundation.&lt;/p&gt;
&lt;p&gt;The first step is to log into your sever via ssh:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ssh root@[your-server-IP]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Navigate to the Nginx directory:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd /etc/nginx/sites-available
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;editing-virtual-host-configs&#34;&gt;Editing virtual host configs&lt;/h2&gt;
&lt;p&gt;There should be a virtual host configuration file for your website under &lt;i&gt;sites-available/example.com.&lt;/i&gt; Open the the file via a text editor. It should look like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;server {
        listen 80;
        listen [::]:80;

        server_name your.domain.com;
        access_log /var/log/nginx/nginx.vhost.access.log;
        error_log /var/log/nginx/nginx.vhost.error.log;
        location / {
        root   /home/www/public_html/your.domain.com/public/;
        index  index.html;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As you can see, Nginx is currently listening on port 80 which means your website is only using HTTP. In order to access the website with HTTPS, you need to create another server block. Copy the entire block and paste it below.&lt;/p&gt;
&lt;p&gt;After copying the block, add the following lines to the new block:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;listen   443;

ssl    on;
ssl_certificate    /etc/ssl/your_domain_name.pem; (or bundle.crt)
ssl_certificate_key    /etc/ssl/your_domain_name.key;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This configuration tells Nginx to listen on port 443 (SSL) and it specifies the directories for your SSL keys.&lt;/p&gt;
&lt;p&gt;“ssl_certificate” is the directory for your main SSL key or the bundle.&lt;/p&gt;
&lt;p&gt;“ssl_certificate_key” is the directory for the private key.&lt;/p&gt;
&lt;p&gt;Make sure that the paths specified are correct and your keys are places in the those directories. Of course you can change to any other directory and update the configuration file accordingly.&lt;/p&gt;
&lt;p&gt;In the end, your configuration file should look something like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;server {
        listen 80;
        listen [::]:80;

        server_name your.domain.com;
        access_log /var/log/nginx/nginx.vhost.access.log;
        error_log /var/log/nginx/nginx.vhost.error.log;
        location / {
        root   /home/www/public_html/your.domain.com/public/;
        index  index.html;
        }
}

server {
        listen 443;
        listen [::]:443;

        ssl    on;
        ssl_certificate    /etc/ssl/your_domain_name.pem; (or bundle.crt)
        ssl_certificate_key    /etc/ssl/your_domain_name.key;

        server_name your.domain.com;
        access_log /var/log/nginx/nginx.vhost.access.log;
        error_log /var/log/nginx/nginx.vhost.error.log;
        location / {
        root   /home/www/public_html/your.domain.com/public/;
        index  index.html;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;check-syntax&#34;&gt;Check syntax&lt;/h2&gt;
&lt;p&gt;As you can see, with this configuration, your website is accessible through both port 80 and 443. In case you want your website to be available ONLY through HTTPS , you can remove the block with port 80, but that’s generally not recommended.&lt;/p&gt;
&lt;p&gt;Now make sure that port 443 is open on your firewall. Then run this command to check Nginx syntax and configuration:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nginx -t
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If it’s successful, then go ahead and restart Nginx for the changes to take effect.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;service nginx restart
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And that’s it. you’re pretty much done. We have learned how to install ssl on nginx. The website should be accessible through HTTPS.&lt;/p&gt;
&lt;p&gt;If you would like to learn more about SSL/TLS in general, you can &lt;a href=&#34;https://workingtitle.pro/index.php/2020/06/23/whats-the-difference-between-ssl-and-tls/&#34;&gt;read&lt;/a&gt; this.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/install-and-configure-ssl-on-nginx/</guid>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>IPSEC/L2tp VPN</title>
      <link>https://workingtitle.pro/posts/ipsec-l2tp-vpn/</link>
      <description>&lt;p&gt;For a long time I was looking for a personal VPN solution. I was too lazy to work on setting up a VPN server from scratch so I had to try out many different options online. I tried OpenVPN for a while and used free online servers but that wasn’t good enough. Slow speeds and difficulty in connecting to some servers were the biggest issue.&lt;/p&gt;
&lt;p&gt;I’ve tried using OpenVPN and configuring my own server, but the setup and installation process is a little bit too convoluted and I didn’t find it very user-friendly.&lt;/p&gt;
&lt;h2 id=&#34;ipsecl2tp&#34;&gt;IPsec/L2TP&lt;/h2&gt;
&lt;p&gt;The best solution I’ve found has been &lt;a href=&#34;https://github.com/hwdsl2/setup-ipsec-vpn&#34;&gt;this IPSec/L2TP on github&lt;/a&gt;. Thanks to hwdsl2 he saved me a lot of trouble. The setup processes on the server couldn’t be easier.&lt;/p&gt;
&lt;p&gt;On the client side, you’ll have to do a one-time registry edit but that’s not too bad. The only downside I can think of is that it allows for only 1 connection per IP. So you can’t have multiple devices that share a public IP connect to the server. Other than that, it works like a charm.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/ipsec-l2tp-vpn/</guid>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
    </item>
    
    <item>
      <title>Dual proxy server with Squid</title>
      <link>https://workingtitle.pro/posts/dual-proxy-server-with-squid/</link>
      <description>&lt;p&gt;Due to some internet restriction that I was experiencing in 2019 , I was forced to find a way to circumvent the disruption to the internet connection, so this is what I did using Squid proxy Server&lt;/p&gt;
&lt;p&gt;I had a VPS set up ready to go inside the country, and I had another identical VPS set up in Germany. You’re going to need very little resource for this. A dual core CPU with 2GB of ram on the VPS should be plenty.&lt;/p&gt;
&lt;p&gt;To have reliable internet connection, I had to set up a squid proxy server on the VPS located within the country. I, while using my connection through the ISP had no connection to any IPs outside the country, but the VPS which was located in a data center did. This allowed me to find a way to the outside.&lt;/p&gt;
&lt;p&gt;After this, I set also set up another squid proxy on the server located in Germany. The goal was to connect these two proxy servers together. So I could connect to the proxy server inside the country, where it could forward my request to the other proxy server in Germany.&lt;/p&gt;
&lt;p&gt;If you don’t know about Squid, you can read about it and download it &lt;a href=&#34;http://www.squid-cache.org/&#34;&gt;from their website&lt;/a&gt;. It’s a fairly easy installation process.&lt;/p&gt;
&lt;p&gt;After installing Squid on both servers, here’s what I did with the configuration on the first VPS (within the country).&lt;/p&gt;
&lt;h2 id=&#34;first-server-configuration-for-squid-proxy&#34;&gt;First Server configuration for squid proxy&lt;/h2&gt;
&lt;p&gt;The following is the configuration for the internal squid proxy server. Which is the server within the country that is easily accessible but has internet restrictions.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#Remember to replace the IPs!
#0.0.0.0 is the first sever inside the country and 1.1.1.1 is the 2nd VPS located in Germany

acl local-servers dstdomain 0.0.0.0 
cache_peer 1.1.1.1 parent 1992 0 no-query default 
cache_peer_domain 1.1.1.1 !0.0.0.0
never_direct deny local-servers
never_direct allow all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Please remember to change the IPs mentioned here with your server IP. 0.0.0.0 is the first server (inside country) and 1.1.1.1 is the second server (outside the country).&lt;/p&gt;
&lt;p&gt;The rest are just default configurations for Squid which you can read about in their documentation. You can add the following lines to the main configuration file.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines



acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 2 hours
acl auth_users proxy_auth REQUIRED
http_access allow auth_users

pinger_enable off
half_closed_clients off
quick_abort_min 0 KB
quick_abort_max 0 KB
quick_abort_pct 95
client_persistent_connections off
server_persistent_connections off
################

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on &amp;#34;localhost&amp;#34; is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 8081

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;second-server-configuration&#34;&gt;Second Server configuration&lt;/h2&gt;
&lt;p&gt;The configs for the second proxy server located outside the country is much easier. We just have to tell Squid to accept requests from our other server. You can add the following lines to the squid configuration:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;acl child_proxy src 0.0.0.0/32
http_access allow child_proxy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;0.0.0.0 is the IP of the first server.&lt;/p&gt;
&lt;p&gt;After doing this you should be set. Check your connections and make sure all needed ports are open on your firewalls on both servers.&lt;/p&gt;
</description>
      <author>trandcanh@gmail.com (Calvin Tran)</author>
      <guid>https://workingtitle.pro/posts/dual-proxy-server-with-squid/</guid>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
    </item>
    
  </channel>
</rss>